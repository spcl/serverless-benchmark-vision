"Authors","Author full names","Author(s) ID","Title","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","Cited by","DOI","Link","Abstract","Author Keywords","Index Keywords","Publisher","Sponsors","Conference name","Conference date","Conference location","Conference code","Document Type","Publication Stage","Open Access","Source","EID"
"Sharma P.; Fuerst A.","Sharma, Prateek (58273575200); Fuerst, Alexander (57218225304)","58273575200; 57218225304","Accountable Carbon Footprints and Energy Profiling For Serverless Functions","2024","SoCC 2024 - Proceedings of the 2024 ACM Symposium on Cloud Computing","","","","522","541","19","1","10.1145/3698038.3698531","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215515480&doi=10.1145%2f3698038.3698531&partnerID=40&md5=daad99897729c2c4d3253aa51b56190f","Cloud computing is a significant and growing cause of carbon emissions. Understanding the energy consumption and carbon footprints of cloud applications is a fundamental prerequisite to raising awareness, designing sustainability metrics, and creating targeted system optimizations. In this paper, we address the challenges of providing accurate and full-system (not just CPU) carbon footprints for serverless (FaaS) functions. To the best of our knowledge, this is the first work which develops an energy and carbon metrology framework for FaaS. Carbon footprints require a new approach to energy profiling. We use FaaS workload properties such as locality to develop a simple and practical online statistical disaggregation approach. Our fine-grained per-invocation carbon footprints also include shared hardware and software emissions, and use insights from Shapley values to fairly account for both operational and embodied emissions. Owing to the growing importance of carbon measurement, we develop a new rigorous marginal energy based validation methodology which results in accountable, complete, and fair footprints. Over a wide range of FaaS workloads and hardware platforms, our energy footprints have an accuracy of > 99%. © 2024 ACM.","Carbon footprint; Cloud computing; Energy measurement; Functions as a Service; Sustainable computing","Cloud platforms; Carbon emissions; Cloud applications; Cloud-computing; Energy; Energy profiling; Energy-consumption; Function as a service; Sustainability metrics; Sustainable computing; System optimizations; Carbon sequestration","Association for Computing Machinery, Inc","ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Groups on Management of Data (SIGMOD); Google; IBM; Microsoft","15th Annual ACM Symposium on Cloud Computing, SoCC 2024","20 November 2024 through 22 November 2024","Redmond","205525","Conference paper","Final","","Scopus","2-s2.0-85215515480"
"Thurimella V.; Raith P.; Hong Enriquez R.P.; Andrei Da Silva A.; Rattihalli G.; Gavrilovska A.; Milojicic D.","Thurimella, Vijay (57221157850); Raith, Philipp (57212228238); Hong Enriquez, Rolando P. (55099055200); Andrei Da Silva, Anderson (57219876361); Rattihalli, Gourav (57193494996); Gavrilovska, Ada (6507056290); Milojicic, Dejan (6603838893)","57221157850; 57212228238; 55099055200; 57219876361; 57193494996; 6507056290; 6603838893","Serverless Computing for Dynamic HPC Workflows","2024","Proceedings of SC 2024-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis","","","","2096","2103","7","0","10.1109/SCW63240.2024.00262","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217155017&doi=10.1109%2fSCW63240.2024.00262&partnerID=40&md5=fceee58c7cf947a9ade24b0389e63f63","Containers have become an important component for scientific workflows, enhancing reproducibility, portability, and isolation when coupled with workflow management systems. However, integrating containers with these systems can be complex, potentially hindering wider adoption. Serverless platforms offer a solution by providing a layer of abstraction over container orchestrators, simplifying management while introducing event-driven capabilities. This paper presents a novel integration of serverless with workflow management systems to optimize scientific workflow execution. Our approach leverages serverless functions to dynamically provision containers for workflow tasks, resulting in up to 30% faster execution. We found that performance can be further improved by reusing containers between multiple different tasks that were provisioned by the serverless platform. These findings demonstrate the utility of combining specialized container orchestration with established workflow management to streamline scientific computing, improve resource utilization, and accelerate time-to-results. Serverless' event-driven architecture enables efficient resource scaling, aligning with the dynamic nature of scientific workloads.  © 2024 IEEE.","","","Institute of Electrical and Electronics Engineers Inc.","ACM; ACM�s Special Interest Group on High Performance Computing (SIGHPC); IEEE Technical Committee on High Performance Computing (TCHPC); Institute of Electrical and Electronics Engineers (IEEE) Computer Society","2024 Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC Workshops 2024","17 November 2024 through 22 November 2024","Atlanta","205946","Conference paper","Final","","Scopus","2-s2.0-85217155017"
"Liu Q.; Cheng Y.; Shen H.; Wang A.; Balaji B.","Liu, Qichang (57392904000); Cheng, Yue (56022559100); Shen, Haiying (59565715100); Wang, Ao (57215285001); Balaji, Bharathan (58224756000)","57392904000; 56022559100; 59565715100; 57215285001; 58224756000","Concurrency-Informed Orchestration for Serverless Functions","2025","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","2","","","147","161","14","0","10.1145/3676641.3716253","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002556739&doi=10.1145%2f3676641.3716253&partnerID=40&md5=ce683540024e3600eee142ffd45dc47a","Cold start delays are a main pain point for today's FaaS (Function-as-a-Service) platforms. A widely used mitigation strategy is keeping recently invoked function containers alive in memory to enable warm starts with minimal overhead. This paper identifies new challenges that state-of-the-art FaaS keep-alive policies neglect. These challenges are caused by concurrent function invocations, a common FaaS workload behavior. First, concurrent requests present a tradeoff between reusing busy containers (delayed warm starts) versus cold-starting containers. Second, concurrent requests cause imbalanced evictions of containers that will be reused shortly thereafter. To tackle the challenges, we propose a novel serverless function container orchestration algorithm called CIDRE. CIDRE makes informed decisions to speculatively choose between a delayed warm start and a cold start under concurrency-driven function scaling. CIDRE uses both fine-grained container-level and coarse-grained concurrency information to make balanced eviction decisions. We evaluate CIDRE extensively using two production FaaS workloads. Results show that CIDRE reduces the cold start ratio and the average invocation overhead by up to 75.1% and 39.3% compared to state-of-the-art function keep-alive policies. © 2025 ACM.","autoscaling; caching; cloud computing; container orchestration; function-as-a-service; serverless computing","Artificial intelligence; Peer to peer networks; Art functions; Autoscaling; Caching; Cloud-computing; Cold-start; Container orchestration; Function-as-a-service; Serverless computing; State of the art; Warm start; Platform as a Service (PaaS)","Association for Computing Machinery","ACM SIGARCH; ACM SIGOPS; ACM SIGPLAN","30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2025","30 March 2025 through 3 April 2025","Rotterdam","207994","Conference paper","Final","","Scopus","2-s2.0-105002556739"
"Kelantonakis G.; Kourou F.; Magoutis K.","Kelantonakis, George (58244146200); Kourou, Fallia (59929656000); Magoutis, Kostas (22035826800)","58244146200; 59929656000; 22035826800","Proportional Fairness and Isolation for Serverless Applications over FaaS Platforms","2025","ICPE 2025 - Proceedings of the 16th ACM/SPEC International Conference on Performance","","","","58","68","10","0","10.1145/3676151.3719363","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007287875&doi=10.1145%2f3676151.3719363&partnerID=40&md5=36293e4f5ff5544951386d8fce3cbb96","Effectively supporting multi-tenant application deployments in the emerging Function-as-a-Service (FaaS) (or serverless computing) model requires extending it with fairness and isolation mechanisms. Quality-of-service (QoS) concepts developed over time in the networking, storage, and virtualized infrastructure domains are currently being investigated in the space of serverless platforms. In this paper, we propose a two-level serverless QoS architecture that combines state-of-the-art scheduling algorithms and mechanisms with the unique characteristics of distributed serverless platforms, resulting into a system that provides proportional fairness for serverless applications with shared access to distributed and load-balanced FaaS platforms. The primary advantage of our approach is the use of higher-level scheduling mechanisms only, avoiding the need to manage low-level resources within underlying FaaS platforms (thus not requiring changes to them) for achieving fairness. We demonstrate the concrete benefits of our architecture using state-of-the-art benchmarks in experiments over AWS EC2. © 2025 Copyright held by the owner/author(s).","differentiated services; fairness; isolation; quality of service; serverless computing","Platform as a Service (PaaS); Resource allocation; Application deployment; Differentiated service; Fairness; Isolation; Multi tenants; Proportional fairness; Quality-of-service; Serverless computing; Service platforms; State of the art; Network function virtualization","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC","16th ACM/SPEC International Conference on Performance, ICPE 2025","5 May 2025 through 9 May 2025","Toronto","208715","Conference paper","Final","","Scopus","2-s2.0-105007287875"
"Copik M.; Calotoiu A.; Rethy G.; Böhringer R.; Bruno R.; Hoefler T.","Copik, Marcin (57194605130); Calotoiu, Alexandru (55415831500); Rethy, Gyorgy (6504316095); Böhringer, Roman (57226183431); Bruno, Rodrigo (56146992200); Hoefler, Torsten (14018121700)","57194605130; 55415831500; 6504316095; 57226183431; 56146992200; 14018121700","Process-as-a-Service: Unifying Elastic and Stateful Clouds with Serverless Processes","2024","SoCC 2024 - Proceedings of the 2024 ACM Symposium on Cloud Computing","","","","223","242","19","0","10.1145/3698038.3698567","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215529249&doi=10.1145%2f3698038.3698567&partnerID=40&md5=532e980176158d3b498aefea98386da9","Fine-grained serverless functions power many new applications that benefit from elastic scaling and pay-as-you-use billing model with minimal infrastructure management overhead. To achieve these properties, Function-as-a-Service (FaaS) platforms disaggregate compute and state and, consequently, introduce non-trivial costs due to the loss of data locality when accessing state, complex control plane interactions, and expensive inter-function communication. We revisit the foundations of FaaS and propose a new cloud abstraction, the cloud process, that retains all the benefits of FaaS while significantly reducing the overheads that result from disaggregation. We show how established operating system abstractions can be adapted to provide powerful granular computing on dynamically provisioned cloud resources while building our Process as a Service (PraaS) platform. PraaS improves current FaaS by offering data locality, fast invocations, and efficient communication. PraaS delivers remote invocations up to 17× faster and reduces communication overhead by up to 99%. © 2024 ACM.","Function-as-a-Service; Operating Systems; Serverless","Infrastructure as a service (IaaS); Platform as a Service (PaaS); Program debugging; Data locality; Fine grained; Function-as-a-service; New applications; Operating system; Power; Process as a services; Scalings; Serverless; Service platforms; Cloud platforms","Association for Computing Machinery, Inc","ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Groups on Management of Data (SIGMOD); Google; IBM; Microsoft","15th Annual ACM Symposium on Cloud Computing, SoCC 2024","20 November 2024 through 22 November 2024","Redmond","205525","Conference paper","Final","","Scopus","2-s2.0-85215529249"
"Li T.; Chen Y.; Yu D.; Zhang Y.; Lagaisse B.","Li, Tianyu (59521078700); Chen, Yingpeng (59261142700); Yu, Donghui (57879527700); Zhang, Yuanyuan (56128011400); Lagaisse, Bert (8933999000)","59521078700; 59261142700; 57879527700; 56128011400; 8933999000","Enhancing Effective Bidirectional Isolation for Function Fusion in Serverless Architectures","2024","Middleware 2024 - Proceedings of the 25th ACM International Middleware Conference","","","","1","7","6","0","10.1145/3652892.3654778","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215526655&doi=10.1145%2f3652892.3654778&partnerID=40&md5=19b2d9541e7bb4b1d486924ed9b639b1","Serverless computing has emerged as a popular paradigm in modern cloud environments, offering flexibility and scalability to tenants. A serverless function might handle sensitive tenant data. Employing Trusted Execution Environment (TEE) techniques to protect such a function from untrusted cloud service providers is attractive for tenant privacy. However, this introduces response latency, thereby impacting the performance of function execution. This paper introduces Fundue, a serverless architecture with bidirectional isolation between tenant and cloud provider that achieves light-weight isolation of functions and reduces cold start latency by fusing functions. Fundue enables multiple functions uploaded by the same tenant to share a single execution environment embedded into the enclave. Fundue allocates separate memory for each serverless function within the execution environment and establishes robust isolation between functions through bounds checking mechanisms. We extensively evaluate Fundue with diverse workloads and representative serverless functions. Our results demonstrate a significant reduction in response latency of serverless function execution, ranging from 17.8% to 88.7% compared to AccTEE, an open-source two-way sandbox serverless framework. Additionally, Fundue mitigates vulnerabilities in existing execution environments, such as stack-based buffer overflows. © 2024 Copyright held by the owner/author(s).","function fusion; Function-as-a-service; Intel SGX; Serverless Computing","Cloud environments; Cloud service providers; Execution environments; Function fusion; Function-as-a-service; Intel SGX; Performance; Serverless architecture; Serverless computing; Trusted execution environments; Memory architecture","Association for Computing Machinery, Inc","ACM","25th ACM International Middleware Conference, Middleware 2024","2 December 2024 through 6 December 2024","Hong Kong","205541","Conference paper","Final","","Scopus","2-s2.0-85215526655"
"Wen Y.; Xu G.; Wang J.; Hao W.","Wen, Yanyan (59479312200); Xu, Guangping (7404264179); Wang, Jianshe (59407851400); Hao, Wei (59479224700)","59479312200; 7404264179; 59407851400; 59479224700","Low-Latency State Management for Real-Time Tasks in Edge Serverless","2024","Proceedings - 2024 IEEE International Symposium on Parallel and Distributed Processing with Applications, ISPA 2024","","","","10","17","7","0","10.1109/ISPA63168.2024.00010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000141034&doi=10.1109%2fISPA63168.2024.00010&partnerID=40&md5=fe302389224177d89af2045fc67ed7b0","Stateful serverless systems commonly adopt an architectural paradigm characterized by compute and storage separation within cloud data centers. Nevertheless, guaranteeing prompt response for real-time tasks at the edge becomes challenging due to network overheads. This paper introduces a Low-Latency state management framework for real-time tasks in edge serverless systems called LoLa, which adaptively places states proximate to functions, thereby mitigating delays in accessing states within edge serverless systems. Our approach aims at mitigating network latency and optimizing resource utilization by co-locating functions and states, thereby enhancing the system's overall efficiency. We introduce an adaptive strategy to coordinate the migration of states. It dynamically adjusts the positions of states based on historical data and real-time feedback. Additionally, we designed an in-memory state storage mechanism to facilitate low-latency access and implement a lightweight and fine-grained state management to ensure stored state consistency. Evaluation results showcase the efficacy of LoLa in reducing state read and write latency within edge serverless systems. Specifically, the average response latency is observed to decrease by 65.2% and 38.1% in the best and worst-case scenarios, respectively.  © 2024 IEEE.","Edge computing; Stateful Serverless","Cloud data centers; Edge computing; Low latency; Management frameworks; Network overhead; Place-state; Real-time tasks; Serverless systems; State management; Stateful serverless","Institute of Electrical and Electronics Engineers Inc.","et al.; IEEE; IEEE Computer Society; IEEE SC Technical Committee on Hyper-Intelligence (HI-TC); IEEE Technical Committee on Scalable Computing (TCSC); IEEE Technical Committee on Smart World","22nd IEEE International Symposium on Parallel and Distributed Processing with Applications, ISPA 2024","30 October 2024 through 2 November 2024","Kaifeng","207145","Conference paper","Final","","Scopus","2-s2.0-105000141034"
"Rotchford D.; Evans S.; Filgueira R.","Rotchford, Daniel (59546374100); Evans, Samuel (59546829200); Filgueira, Rosa (9733528300)","59546374100; 59546829200; 9733528300","Laminar 2.0: Serverless Stream Processing with Enhanced Code Search and Recommendations","2024","Proceedings of SC 2024-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis","","","","2088","2095","7","0","10.1109/SCW63240.2024.00261","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217186391&doi=10.1109%2fSCW63240.2024.00261&partnerID=40&md5=2eaa02a961aa7661ee9640c0d9077be4","This paper presents Laminar 2.0, an enhanced serverless framework for running dispel4py streaming work-flows. Building on Laminar 1.0, this version introduces improved dependency management, client-server functionality, and advanced deep learning models for semantic search. Key innovations include a structural code-to-code search using simplified parse syntax trees (SPTs) for detecting similar Processing Elements (PEs) or workflows, even from incomplete code. Additionally, Laminar 2.0 optimizes text-to-code search through better preprocessing of PEs. Our evaluation shows significant performance improvements over the previous version.  © 2024 IEEE.","dispel4py; Laminar; semantic code search; Serverless computing; streaming workflows","","Institute of Electrical and Electronics Engineers Inc.","ACM; ACM�s Special Interest Group on High Performance Computing (SIGHPC); IEEE Technical Committee on High Performance Computing (TCHPC); Institute of Electrical and Electronics Engineers (IEEE) Computer Society","2024 Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC Workshops 2024","17 November 2024 through 22 November 2024","Atlanta","205946","Conference paper","Final","","Scopus","2-s2.0-85217186391"
"Zhao Z.; Wu M.; Chen H.; Zang B.","Zhao, Ziming (57201619659); Wu, Mingyu (57193513326); Chen, Haibo (55743141500); Zang, Binyu (6701320221)","57201619659; 57193513326; 55743141500; 6701320221","Characterization and Reclamation of Frozen Garbage in Managed FaaS Workloads","2024","EuroSys 2024 - Proceedings of the 2024 European Conference on Computer Systems","","","","281","297","16","1","10.1145/3627703.3629579","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191992131&doi=10.1145%2f3627703.3629579&partnerID=40&md5=557ae0db9f862797fbebaa2b1b1dd6ec","FaaS (function-as-a-service) is becoming a popular workload in cloud environments due to its virtues such as auto-scaling and pay-as-you-go. High-level languages like JavaScript and Java are commonly used in FaaS for programmability, but their managed runtimes complicate memory management in the cloud. This paper first observes the issue of frozen garbage, which is caused by freezing cached function instances where their threads have been paused but the unused memory (e.g., garbage) is not reclaimed due to the semantic gap between FaaS and the managed runtime. This paper presents the first characterization of the negative effects induced by frozen garbage with various functions, which uncovers that it can occupy more than half of FaaS instances' memory resources on average. To this end, this paper proposes Desiccant, a freeze-aware memory manager for managed workloads in FaaS, which reclaims idle memory resources consumed by frozen garbage from managed runtime instances and thus notably improves memory efficiency. The evaluation on various FaaS workloads shows that Desiccant can reduce FaaS functions' peak memory consumption by up to 6.72×. Such saved memory consumption allows caching more FaaS instances to reduce the frequency of cold boots (creating instances before function execution) and p99 latency by up to 4.49× and 37.5%, respectively.  © 2024 ACM.","Function-as-a-Service; Garbage Collection; Language Runtime","Driers (materials); High level languages; Cloud environments; Function-as-a-service; Garbage collection; Language runtimes; Memory resources; Pay as you go; Runtimes; Scalings; Service instances; Various functions; Semantics","Association for Computing Machinery, Inc","ACM SIGOPS; Ant Group Research; Google; Huawei; KAUST; Red Hat","19th European Conference on Computer Systems, EuroSys 2024","22 April 2024 through 25 April 2024","Athens","199050","Conference paper","Final","","Scopus","2-s2.0-85191992131"
"Lu F.; Wei X.; Huang Z.; Chen R.; Wu M.; Chen H.","Lu, Fangming (57440649500); Wei, Xingda (57112888500); Huang, Zhuobin (57553884400); Chen, Rong (56428216800); Wu, Minyu (57193513326); Chen, Haibo (55743141500)","57440649500; 57112888500; 57553884400; 56428216800; 57193513326; 55743141500","Serialization/Deserialization-free State Transfer in Serverless Workflows","2024","EuroSys 2024 - Proceedings of the 2024 European Conference on Computer Systems","","","","132","147","15","5","10.1145/3627703.3629568","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192003448&doi=10.1145%2f3627703.3629568&partnerID=40&md5=9857af0de38e14a50df51f9da3726399","Serialization and deserialization play a dominant role in the state transfer time of serverless workflows, leading to substantial performance penalties during workflow execution. We identify the key reason as a lack of ability to efficiently access the (remote) memory of another function. We propose RMMap, an OS primitive for remote memory map. It allows a serverless function to directly access the memory of another function, even if it is located remotely. RMMap is the first to completely eliminates serialization and deserialization when transferring states between any pairs of functions in (unmodified) serverless workflows. To make remote memory map efficient and feasible, we co-design it with fast networking (RDMA), OS, language runtime, and serverless platform. Evaluations using real-world serverless workloads show that integrating RMMap with Knative reduces the serverless workflow execution time on Knative by up to 2.6 × and improves resource utilizations by 86.3%.  © 2024 ACM.","","Co-designs; Free state; Language runtimes; Memory map; Performance penalties; Remote memory; State transfer; Transfer time; Work-flows; Workflow execution","Association for Computing Machinery, Inc","ACM SIGOPS; Ant Group Research; Google; Huawei; KAUST; Red Hat","19th European Conference on Computer Systems, EuroSys 2024","22 April 2024 through 25 April 2024","Athens","199050","Conference paper","Final","","Scopus","2-s2.0-85192003448"
"Lv C.; Shi X.; Lei Z.; Huang J.; Tan W.; Zheng X.; Zhao X.","Lv, Cunchi (58024239300); Shi, Xiao (57205328361); Lei, Zhengyu (58025038600); Huang, Jinyue (59705965600); Tan, Wenting (58024239200); Zheng, Xiaohui (57020284600); Zhao, Xiaofang (56022060800)","58024239300; 57205328361; 58025038600; 59705965600; 58024239200; 57020284600; 56022060800","Dilu: Enabling GPU Resourcing-on-Demand for Serverless DL Serving via Introspective Elasticity","2025","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","1","","","311","325","14","0","10.1145/3669940.3707251","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002373103&doi=10.1145%2f3669940.3707251&partnerID=40&md5=e15ad0c1642a0db34cb380a09f7c8364","Serverless computing, with its ease of management, auto-scaling, and cost-effectiveness, is widely adopted by deep learning (DL) applications. DL workloads, especially with large language models, require substantial GPU resources to ensure QoS. However, it is prone to produce GPU fragments (e.g., 15%-94%) in serverless DL systems due to the dynamicity of workloads and coarse-grained static GPU allocation mechanisms, gradually eroding the profits offered by serverless elasticity. Different from classical serverless systems that only scale horizontally, we present introspective elasticity (IE), a fine-grained and adaptive two-dimensional co-scaling mechanism to support GPU resourcing-on-demand for serverless DL tasks. Based on this insight, we build Dilu, a cross-layer and GPU-based serverless DL system with IE support. First, Dilu provides multi-factor profiling for DL tasks with efficient pruning search methods. Second, Dilu adheres to the resourcing-complementary principles in scheduling to improve GPU utilization with QoS guarantees. Third, Dilu adopts an adaptive 2D co-scaling method to enhance the elasticity of GPU provisioning in real time. Evaluations show that it can dynamically adjust the resourcing of various DL functions with low GPU fragmentation (10%-46% GPU defragmentation), high throughput (up to 1.8× inference and 1.1× training throughput increment) and QoS guarantees (11%-71% violation rate reduction), compared to the SOTA baselines.  © 2025 ACM.","co-scaling; gpu resourcing-on-demand; introspective elasticity; serverless deep learning","Computer graphics equipment; Problem oriented languages; Resource allocation; Co-scaling; Gpu resourcing-on-demand; Introspective elasticity; Language model; Learning tasks; On demands; QoS guarantee; Resourcing; Scalings; Serverless deep learning; Graphics processing unit","Association for Computing Machinery","ACM SIGARCH; ACM SIGOPS; ACM SIGPLAN","30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2025","30 March 2025 through 3 April 2025","Rotterdam","207994","Conference paper","Final","","Scopus","2-s2.0-105002373103"
"Yu H.; Basu Roy R.; Fontenot C.; Tiwari D.; Li J.; Zhang H.; Wang H.; Park S.-J.","Yu, Hanfei (57220804031); Basu Roy, Rohan (57219249946); Fontenot, Christian (58559623600); Tiwari, Devesh (23467777300); Li, Jian (59080155500); Zhang, Hong (57188929033); Wang, Hao (57170260400); Park, Seung-Jong (7501830301)","57220804031; 57219249946; 58559623600; 23467777300; 59080155500; 57188929033; 57170260400; 7501830301","RainbowCake: Mitigating Cold-starts in Serverless with Layer-wise Container Caching and Sharing","2024","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","1","","","335","350","15","18","10.1145/3617232.3624871","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191460215&doi=10.1145%2f3617232.3624871&partnerID=40&md5=9c550c4c80c89de376b22ad2dfed9d34","Serverless computing has grown rapidly as a new cloud computing paradigm that promises ease-of-management, cost-efficiency, and auto-scaling by shipping functions via self-contained virtualized containers. Unfortunately, serverless computing suffers from severe cold-start problems - -starting containers incurs non-trivial latency. Full container caching is widely applied to mitigate cold-starts, yet has recently been outperformed by two lines of research: partial container caching and container sharing. However, either partial container caching or container sharing techniques exhibit their drawbacks. Partial container caching effectively deals with burstiness while leaving cold-start mitigation halfway; container sharing reduces cold-starts by enabling containers to serve multiple functions while suffering from excessive memory waste due to over-packed containers.This paper proposes RainbowCake, a layer-wise container pre-warming and keep-alive technique that effectively mitigates cold-starts with sharing awareness at minimal waste of memory. With structured container layers and sharing-aware modeling, RainbowCake is robust and tolerant to invocation bursts. We seize the opportunity of container sharing behind the startup process of standard container techniques. RainbowCake breaks the container startup process of a container into three stages and manages different container layers individually. We develop a sharing-aware algorithm that makes event-driven layer-wise caching decisions in real-time. Experiments on OpenWhisk clusters with real-world workloads show that RainbowCake reduces 68% function startup latency and 77% memory waste compared to state-of-the-art solutions. © 2024 Copyright is held by the owner/author(s). Publication rights licensed to ACM.","cold-start; container caching; container sharing; serverless computing","Cloud-computing; Cold-start; Computing paradigm; Container caching; Container sharing; Cost-efficiency; Layer-wise; Management costs; Serverless computing; Start-up process; Containers","Association for Computing Machinery","ACM Special Interest Group on Computer Architecture (SIGARCH); ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Group on Programming Languages (SIGPLAN)","29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2024","27 April 2024 through 1 May 2024","San Diego","198945","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85191460215"
"","","","EuroSys 2024 - Proceedings of the 2024 European Conference on Computer Systems","2024","EuroSys 2024 - Proceedings of the 2024 European Conference on Computer Systems","","","","","","1224","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192000216&partnerID=40&md5=3bc5951a6788d23335906a8283cfad8a","The proceedings contain 71 papers. The topics discussed include: WiseGraph: optimizing GNN with joint workload partition of graph and operations; core graph: exploiting edge centrality to speedup the evaluation of iterative graph queries; Contigra: graph mining with containment constraints; Halflife: an adaptive Flowlet-based load balancer with fading timeout in DCNs; HODA: a high-performance open vSwitch dataplane with multiple specialized data paths; Astraea: towards fair and efficient learning-based congestion control; unison: a parallel-efficient and user-transparent network simulation kernel; serialization/deserialization-free state transfer in serverless workflows; OCCAM: a programming system for reliable network management; Aceso: efficient parallel DNN training through iterative bottleneck alleviation; Totoro: a scalable federated learning engine for the edge; and DeTA: minimizing data leaks in federated learning via decentralized and trustworthy aggregation.","","","Association for Computing Machinery, Inc","ACM SIGOPS; Ant Group Research; Google; Huawei; KAUST; Red Hat","19th European Conference on Computer Systems, EuroSys 2024","22 April 2024 through 25 April 2024","Athens","199050","Conference review","Final","","Scopus","2-s2.0-85192000216"
"Sankaranarayanan K.; Roy R.B.; Tiwari D.","Sankaranarayanan, Kausalya (59546539400); Roy, Rohan Basu (57219249946); Tiwari, Devesh (23467777300)","59546539400; 57219249946; 23467777300","PULSE: Using Mixed-Quality Models for Reducing Serverless Keep-Alive Cost","2024","Proceedings of SC 2024-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis","","","","99","109","10","0","10.1109/SCW63240.2024.00021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217167511&doi=10.1109%2fSCW63240.2024.00021&partnerID=40&md5=dc77f475b5d10c639f0d6b553a3f3a4c","This paper addresses a key challenge with using serverless computing for machine learning (ML) inference which is cold starts that occur during initial invocations and container inactivity. Fixed keep-alive policies, like the commonly adopted 10-minute strategy, have been implemented by cloud providers to alleviate cold start issues. However, the substantial size of ML models poses a significant hurdle, leading to elevated keep-alive costs and potential strain on system resources. In response to these challenges, we introduce PULSE, a dynamic 10-minute keep-alive mechanism that employs ML model variants to optimize the balance between keep-alive costs, accuracy, and service time while avoiding peaks in keep-alive memory consumption. Our evaluation, using real-world serverless workloads and commonly used machine learning models, demonstrates reduced keep-alive costs compared to the fixed policy. Additionally, we observe that integrating PULSE improves the performance of existing state-of-the-art serverless function warm-up strategies.  © 2024 IEEE.","","","Institute of Electrical and Electronics Engineers Inc.","ACM; ACM�s Special Interest Group on High Performance Computing (SIGHPC); IEEE Technical Committee on High Performance Computing (TCHPC); Institute of Electrical and Electronics Engineers (IEEE) Computer Society","2024 Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC Workshops 2024","17 November 2024 through 22 November 2024","Atlanta","205946","Conference paper","Final","","Scopus","2-s2.0-85217167511"
"","","","Mid4CC 2023 - Proceedings of the 2023 1st International Workshop on Middleware for the Computing Continuum, Part of: Middleware 2023","2023","Mid4CC 2023 - Proceedings of the 2023 1st International Workshop on Middleware for the Computing Continuum, Part of: Middleware 2023","","","","","","36","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182005747&partnerID=40&md5=4cfcde6aa50009345beed8af7df23fc5","The proceedings contain 6 papers. The topics discussed include: secure and lightweight access control for highly decentralized and distributed file systems; an analysis about federated learning in low-powerful devices; orchestrating serverless applications in the cloud-to-edge continuum; an optimized blockchain-based data transmission system for telemedicine; benchmarks for job scheduling in ultra-distributed systems; and BlazeFlow: a multi-layer communication middleware for real-time distributed IoT applications.","","","Association for Computing Machinery, Inc","ACM","1st International Workshop on Middleware for the Computing Continuum, Mid4CC 2023","11 December 2023 through 15 December 2023","Bologna","195663","Conference review","Final","","Scopus","2-s2.0-85182005747"
"Liu G.; Zhao L.; Li Y.; Duan Z.; Chen S.; Hu Y.; Su Z.; Qu W.","Liu, Guowei (59033601000); Zhao, Laiping (35243865000); Li, Yiming (57212459721); Duan, Zhaolin (59030889100); Chen, Sheng (57203904413); Hu, Yitao (57188994639); Su, Zhiyuan (57141447800); Qu, Wenyu (8917789900)","59033601000; 35243865000; 57212459721; 59030889100; 57203904413; 57188994639; 57141447800; 8917789900","Fuyao: DPU-enabled Direct Data Transfer for Serverless Computing","2024","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","3","","","431","447","16","7","10.1145/3620666.3651327","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192145265&doi=10.1145%2f3620666.3651327&partnerID=40&md5=a141a83787885eb9ff1f832bb2b44248","Serverless computing typically relies on the third-party forwarding method to transmit data between functions. This method couples control flow and data flow together, resulting in significantly slow data transmission speeds. This challenge makes it difficult for the serverless computing paradigm to meet the low-latency requirements of web services. To solve this problem, we propose decoupling the control flow from the data flow, enabling direct data transfer between functions. We introduce Fuyao, the first intermediate data transfer solution capable of reducing data transfer latency to the sub-millisecond level. Fuyao provides four different data transfer methods to cater to diverse data transfer requirements within or between nodes. For function pairs that communicate frequently, Fuyao builds a stateful direct connection between them, enabling rapid inter-function data exchange. We evaluate Fuyao using real-world representative benchmarks. Experimental results show that Fuyao outperforms state-of-the-art systems by up to 57× on latency. © 2024 Copyright held by the owner/author(s).","Data Transfer; DPU; Serverless Computing","Electronic data interchange; Web services; Computing paradigm; Control data; Control-flow; Data-transmission speed; Dataflow; DPU; Low latency; Serverless computing; Third parties; Transmit data; Data transfer","Association for Computing Machinery","ACM Special Interest Group on Computer Architecture (SIGARCH); ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Group on Programming Languages (SIGPLAN)","29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2024","27 April 2024 through 1 May 2024","San Diego","198945","Conference paper","Final","","Scopus","2-s2.0-85192145265"
"You J.; Chen K.; Zhao L.; Li Y.; Chen Y.; Du Y.; Wang Y.; Wen L.; Hu K.; Li K.","You, Jianing (59479402200); Chen, Kang (55515774700); Zhao, Laiping (35243865000); Li, Yiming (57212459721); Chen, Yichi (58793325700); Du, Yuxuan (59294816400); Wang, Yanjie (59729525500); Wen, Luhang (59729366100); Hu, Keyang (58036817000); Li, Keqiu (57204189178)","59479402200; 55515774700; 35243865000; 57212459721; 58793325700; 59294816400; 59729525500; 59729366100; 58036817000; 57204189178","AlloyStack: A Library Operating System for Serverless Workflow Applications","2025","EuroSys 2025 - Proceedings of the 2025 20th European Conference on Computer Systems","","","","921","937","16","0","10.1145/3689031.3717490","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002252110&doi=10.1145%2f3689031.3717490&partnerID=40&md5=3aa5391f5c8b5b645326275c94c0ef5c","Serverless workflow applications, composed of multiple serverless functions, are increasingly popular in production. However, inter-function communication and cold start latency remain key performance bottlenecks. This paper introduces AlloyStack, a library operating system (LibOS) tailored for serverless workflows. AlloyStack addresses two major challenges: (1) reducing cold start latency through on-demand OS component loading and (2) minimizing data transfer overhead by enabling functions within the same workflow to share a single address space, eliminating unnecessary data copying. To ensure secure isolation, AlloyStack uses Memory Protection Keys (MPK) to separate user functions from the LibOS while maintaining efficient data sharing. Our evaluation shows that AlloyStack reduces cold start times by 98.5% to just 1.3ms. Compared to SOTA systems, AlloyStack achieves a 7.3× to 38.7× speedup in Rust end-to-end latency and a 4.8× to 78.3× speedup in other languages for intermediate data-intensive workflows. © 2025 Copyright held by the owner/author(s).","Cold Start; Data Transfer; Serverless Computing","Osmium alloys; Address space; Cold-start; Data copying; Data Sharing; Memory protection; On demands; Performance bottlenecks; Serverless computing; Work-flows; Workflow applications; Lithium alloys","Association for Computing Machinery, Inc","ACM SIGOPS; Amazon Web Services; AMD; et al.; Huawei; Microsoft","20th European Conference on Computer Systems, EuroSys 2025, co-located 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2025","30 March 2025 through 3 April 2025","Rotterdam","207851","Conference paper","Final","","Scopus","2-s2.0-105002252110"
"Pei Q.; Wang Y.; Shin S.","Pei, Qi (57223270279); Wang, Yipeng (57195932291); Shin, Seunghee (57191963268)","57223270279; 57195932291; 57191963268","Litmus: Fair Pricing for Serverless Computing","2025","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","4","","","155","169","14","0","10.1145/3622781.3674181","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007015177&doi=10.1145%2f3622781.3674181&partnerID=40&md5=dedd3d35fd7bc0342937702d72c0b0ae","Serverless computing has emerged as a market-dominant paradigm in modern cloud computing, benefiting both cloud providers and tenants. While service providers can optimize their machine utilization, tenants only need to pay for the resources they use. To maximize resource utilization, these serverless systems co-run numerous short-lived functions, bearing frequent system condition shifts. When the system gets overcrowded, a tenant's function may suffer from disturbing slowdowns. Ironically, tenants also incur higher costs during these slowdowns, as commercial serverless platforms determine costs proportional to their execution times.This paper argues that cloud providers should compensate tenants for losses incurred when the server is over-provisioned. However, estimating tenants' losses is challenging without pre-profiled information about their functions. Prior studies have indicated that assessing tenant losses leads to heavy overheads. As a solution, this paper introduces a new pricing model that offers discounts based on the machine's state while presuming the tenant's loss under that state. To monitor the machine state accurately, Litmus pricing frequently conducts Litmus tests, an effective and lightweight solution for measuring system congestion. Our experiments show that Litmus pricing can accurately gauge the impact of system congestion and offer nearly ideal prices, with only a 0.2% price difference on average, in a heavily congested system. © 2025 Association for Computing Machinery. All rights reserved.","congestion estimation; online pricing; resource sharing; serverless computing","Cloud providers; Cloud-computing; Congestion estimation; Fair pricing; Machine state; Online pricing; Resources sharing; Serverless computing; Service provider; Systems congestion; Resource allocation","Association for Computing Machinery","SIGARCH; SIGOPS; SIGPLAN","29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2024","27 April 2024 through 1 May 2024","San Diego","198945","Conference paper","Final","","Scopus","2-s2.0-105007015177"
"Nestorov A.M.; Marrón D.; Gutierrez-Torre A.; Wang C.; Misale C.; Youssef A.; Carrera D.; Berral J.L.","Nestorov, Anna Maria (57194447320); Marrón, Diego (56522922000); Gutierrez-Torre, Alberto (57202156916); Wang, Chen (57157809200); Misale, Claudia (56132082000); Youssef, Alaa (57214412197); Carrera, David (9736961100); Berral, Josep Lluís (23090036200)","57194447320; 56522922000; 57202156916; 57157809200; 56132082000; 57214412197; 9736961100; 23090036200","Dexter: A Performance-Cost Efficient Resource Allocation Manager for Serverless Data Analytics","2024","Middleware 2024 - Proceedings of the 25th ACM International Middleware Conference","","","","117","130","13","0","10.1145/3652892.3700753","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215537428&doi=10.1145%2f3652892.3700753&partnerID=40&md5=7fb64d9b5d07154bd32375a8a9262458","Leveraging serverless platforms for the efficient execution of distributed data analytics frameworks, such as Apache Spark [3], has gained substantial interest since early 2022. The elasticity, free-of-management, and on-demand scalability of serverless have motivated the effort in deploying distributed data analytics applications to serverless platforms. However, effectively auto-scaling resources for such complex workloads so that we can fully benefit from the resource elasticity of serverless remains challenging. Mis-configuration can result in severe performance and cost issues arising from resource under- and over-provisioning. In this paper, we present Dexter, a robust resource allocation manager dynamically allocating resources at a fine-grained level to guarantee performance-cost efficiency (optimizing total runtime cost). Dexter is novel in combining predictive and reactive strategies that fully leverage the elasticity of serverless to enhance the performance-cost efficiency for workflow executions. Unlike black-box ML models, Dexter quickly reaches a sufficiently good solution, prioritizing simplicity, generality, and ease of understanding. Our experimental evaluation shows that, compared with the default serverless Spark resource allocation that dynamically requests exponentially more executors to accommodate pending tasks, our solution achieves a cost reduction of up to 4.65×, while improving performance-cost efficiency up to 3.50×. Dexter also enables a substantial resource saving, demanding up to 5.75× fewer resources. © 2024 Copyright held by the owner/author(s).","Data Analytics; Resource Allocation; Serverless; Spark; Stage","Cost-efficiency; Cost-efficient; Data analytics; Distributed data analytics; Efficient resource allocation; On demands; Performance costs; Resources allocation; Scalings; Serverless; Costs","Association for Computing Machinery, Inc","ACM","25th ACM International Middleware Conference, Middleware 2024","2 December 2024 through 6 December 2024","Hong Kong","205541","Conference paper","Final","","Scopus","2-s2.0-85215537428"
"Brunnert A.","Brunnert, Andreas (8561765400)","8561765400","Green Software Metrics","2024","ICPE 2024 - Companion of the 15th ACM/SPEC International Conference on Performance Engineering","","","","287","288","1","0","10.1145/3629527.3652883","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193945272&doi=10.1145%2f3629527.3652883&partnerID=40&md5=0da98798454864f2d6105599f3dfd6f3","Efficiency has always been at the core of software performance engineering research. Many aspects that have been addressed in performance engineering for decades are gaining popularity under the umbrella of Green IT and Green Software Engineering. Engineers and marketers in the industry are looking for ways to measure how green (in terms of carbon dioxide emissions) their software products are. Proxy measures are proposed, such as hosting cost or the power consumption of the hardware environment on which the software is running. In environments where a software system runs on a dedicated server instance, this may make sense, but in virtualised, containerised or serverless environments, it is necessary to find ways of allocating the energy consumption of the entire server to software components that share the same infrastructure. This paper proposes the use of resource demand measurements as a basis for measuring how green a given software actually is.  © 2024 Copyright held by the owner/author(s).","green it; green software engineering; resource demand","Carbon dioxide; Energy utilization; Global warming; Software engineering; Carbon dioxide emissions; Green software engineering; Green-IT; Hardware environment; Performance engineering; Proxy measure; Resource demands; Software metrics; Software performance engineerings; Software products; Green computing","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC Reaserch","15th ACM/SPEC International Conference on Performance Engineering, ICPE 2024","7 May 2024 through 11 May 2024","London","199384","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85193945272"
"Schmid L.; Copik M.; Calotoiu A.; Brandner L.; Koziolek A.; Hoefler T.","Schmid, Larissa (57764130600); Copik, Marcin (57194605130); Calotoiu, Alexandru (55415831500); Brandner, Laurin (57209408870); Koziolek, Anne (55094731500); Hoefler, Torsten (14018121700)","57764130600; 57194605130; 55415831500; 57209408870; 55094731500; 14018121700","SeBS-Flow: Benchmarking Serverless Cloud Function Workflows","2025","EuroSys 2025 - Proceedings of the 2025 20th European Conference on Computer Systems","","","","902","920","18","1","10.1145/3689031.3717465","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002222395&doi=10.1145%2f3689031.3717465&partnerID=40&md5=c789c237890f399c160bf53c4d226fc5","Serverless computing has emerged as a prominent paradigm, with a significant adoption rate among cloud customers. While this model offers advantages such as abstraction from the deployment and resource scheduling, it also poses limitations in handling complex use cases due to the restricted nature of individual functions. Serverless workflows address this limitation by orchestrating multiple functions into a cohesive application. However, existing serverless workflow platforms exhibit significant differences in their programming models and infrastructure, making fair and consistent performance evaluations difficult in practice. To address this gap, we propose the first serverless workflow benchmarking suite SeBS-Flow, providing a platform-agnostic workflow model that enables consistent benchmarking across various platforms. SeBS-Flow includes six real-world application benchmarks and four microbenchmarks representing different computational patterns. We conduct comprehensive evaluations on three major cloud platforms, assessing performance, cost, scalability, and runtime deviations. We make our benchmark suite open-source, enabling rigorous and comparable evaluations of serverless workflows over time. © 2025 Copyright held by the owner/author(s).","benchmark; faas; function-as-a-service; orchestration; serverless; serverless DAG; workflow","Benchmarking; Data as a service (DaaS); Open source software; Program debugging; Benchmark; Faas; Function-as-a-service; Multiple function; Orchestration; Programming models; Resource-scheduling; Serverless; Serverless DAG; Work-flows; Cloud platforms","Association for Computing Machinery, Inc","ACM SIGOPS; Amazon Web Services; AMD; et al.; Huawei; Microsoft","20th European Conference on Computer Systems, EuroSys 2025, co-located 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2025","30 March 2025 through 3 April 2025","Rotterdam","207851","Conference paper","Final","","Scopus","2-s2.0-105002222395"
"Phalak C.; Chahal D.; Ramesh M.; Singhal R.","Phalak, Chetan (39861982700); Chahal, Dheeraj (57147492400); Ramesh, Manju (57221699821); Singhal, Rekha (36069730400)","39861982700; 57147492400; 57221699821; 36069730400","Towards Geo-Distributed Training of ML Models in a Multi-Cloud Environment","2024","ICPE 2024 - Companion of the 15th ACM/SPEC International Conference on Performance Engineering","","","","211","217","6","1","10.1145/3629527.3651422","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193954786&doi=10.1145%2f3629527.3651422&partnerID=40&md5=6ac4521e6f51c6f6feebab2faf34e483","Geo-distributed (GD) training is a machine-learning technique that uses geographically distributed data for model training. Like Federated Learning, geo-distributed machine learning can provide data privacy and also benefit from the cloud infrastructure provided by many vendors in multiple geographies. However, GD training suffers from multiple challenges such as performance degradation due to cross-geography low network bandwidth and high cost of deployment. Additionally, all major cloud vendors such as Amazon AWS, Microsoft Azure, and Google Cloud Platform provide services in several geographies. Hence, finding a high-performance as well as cost-effective cloud service provider and service for GD training is a challenge. In this paper, we present our evaluation of the performance and cost associated with training models in multi-cloud and multi-geography. We evaluate multiple deployment architectures using computing and storage services from multiple cloud vendors. The use of serverless instances in conjunction with virtual machines for model training is evaluated in this study. Additionally, we build and evaluate cost models for estimating the cost of distributed training of models in a multi-cloud environment. Our study shows that the judicious selection of cloud services and architecture might result in cost and performance gains.  © 2024 Copyright held by the owner/author(s).","cost model; geo-distributed training; multi-cloud","Data privacy; Digital storage; Distributed database systems; Machine learning; Network architecture; Windows operating system; Cloud environments; Cloud Vendor; Cost models; Distributed data; Geo-distributed training; High costs; Machine learning techniques; Model training; Multi-clouds; Performance; Cost effectiveness","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC Reaserch","15th ACM/SPEC International Conference on Performance Engineering, ICPE 2024","7 May 2024 through 11 May 2024","London","199384","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85193954786"
"Li Z.; Yu R.; Liu J.","Li, Zhiqi (59929977800); Yu, Ruiqi (59929234000); Liu, Jianshu (57211692761)","59929977800; 59929234000; 57211692761","IrisBench: An Open-Source Benchmark Suite for Video Processing Systems in Cloud","2025","ICPE Companion 2025 - Companion of the 16th ACM/SPEC International Conference on Performance Engineering","","","","167","173","6","0","10.1145/3680256.3721317","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007287406&doi=10.1145%2f3680256.3721317&partnerID=40&md5=d93668108000a59af2268efee005088c","Recent advances in generative text-to-video AI models (e.g., VideoPoet and Sora) have spurred a surge in video production, leading to an increased demand for video processing pipelines among various video service providers such as YouTube and TikTok. With the improvement of cloud computing, video processing systems are frequently updated and present both opportunities and challenges while optimizing the quality of service (QoS) and cloud resource utilization. However, research on evaluating the performance of video processing systems is limited. Besides the availability of video datasets and realistic workloads, the lack of an open-source benchmark system reflecting the characteristics of industrial video processing systems is a significant gap. To fill this gap, we develop IrisBench, an open-source benchmark suite for cloud video processing systems to facilitate research on performance analysis. Our benchmark suite includes three video services: video transcoding, video partitioning, and video object detection services. Our future work relies on using IrisBench to study the architectural implications of various cloud video processing systems in the cloud. © 2025 Owner/Author.","benchmark; cloud computing; serverless; stream processing","Interoperability; Object detection; Open source software; Service oriented architecture (SOA); Special effects; Benchmark; Benchmark suites; Cloud-computing; Open-source; Processing systems; Serverless; Stream processing; Video processing; Video production; Video service providers; Video streaming","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC","16th ACM/SPEC International Conference on Performance Engineering, ICPE Companion 2025","5 May 2025 through 9 May 2025","Toronto","208717","Conference paper","Final","","Scopus","2-s2.0-105007287406"
"Kohli S.; Kharbanda S.; Bruno R.; Carreira J.; Fonseca P.","Kohli, Sumer (57224535205); Kharbanda, Shreyas (59011608800); Bruno, Rodrigo (56146992200); Carreira, Joao (59112718200); Fonseca, Pedro (56343089800)","57224535205; 59011608800; 56146992200; 59112718200; 56343089800","Pronghorn: Effective Checkpoint Orchestration for Serverless Hot-Starts","2024","EuroSys 2024 - Proceedings of the 2024 European Conference on Computer Systems","","","","298","316","18","6","10.1145/3627703.3629556","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191970904&doi=10.1145%2f3627703.3629556&partnerID=40&md5=ce3dbda07e7e149810ecdb1e6cfdc3ac","Serverless computing allows developers to deploy and scale stateless functions in ephemeral workers easily. As a result, serverless computing has been widely used for many applications, such as computer vision, video processing, and HTML generation. However, we find that the stateless nature of serverless computing wastes many of the important benefits modern language runtimes have to offer. A notable example is the extensive profiling and Just-in-Time (JIT) compilation effort that runtimes implement to achieve acceptable performance of popular high-level languages, such as Java, JavaScript, and Python. Unfortunately, when modern language runtimes are naively adopted in serverless computing, all of these efforts are lost upon worker eviction. Checkpoint-restore methods alleviate the problem by resuming workers from snapshots taken after initialization. However, production-grade language runtimes can take up to thousands of invocations to fully optimize a single function, thus rendering naive checkpoint-restore policies ineffective. This paper proposes Pronghorn, a snapshot serverless orchestrator that automatically monitors the function performance and decides (1) when it is the right moment to take a snapshot and (2) which snapshot to use for new workers. Pronghorn is agnostic to the underlying platform and JIT runtime, thus easing its integration into existing runtimes and worker deployment environments (container, virtual machine, etc.). On a set of representative serverless benchmarks, Pronghorn provides end-to-end median latency improvements of 37.2% across 9 out of 13 benchmarks (20-58% latency reduction) when compared to state-of-art checkpointing policies.  © 2024 Owner/Author.","","Just in time production; Python; Restoration; Video signal processing; Acceptable performance; High-level language; Higher-level languages; Javascript; Just-in-time compilation; Language runtimes; Modern languages; Video processing; Workers'; High level languages","Association for Computing Machinery, Inc","ACM SIGOPS; Ant Group Research; Google; Huawei; KAUST; Red Hat","19th European Conference on Computer Systems, EuroSys 2024","22 April 2024 through 25 April 2024","Athens","199050","Conference paper","Final","","Scopus","2-s2.0-85191970904"
"Yu H.; Wang H.; Tiwari D.; Li J.; Park S.-J.","Yu, Hanfei (57220804031); Wang, Hao (57170260400); Tiwari, Devesh (23467777300); Li, Jian (59080155500); Park, Seung-Jong (7501830301)","57220804031; 57170260400; 23467777300; 59080155500; 7501830301","Stellaris: Staleness-Aware Distributed Reinforcement Learning with Serverless Computing","2024","International Conference for High Performance Computing, Networking, Storage and Analysis, SC","","","","","","","1","10.1109/SC41406.2024.00045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213889656&doi=10.1109%2fSC41406.2024.00045&partnerID=40&md5=0adb7415dd5d2f01eebf1fd83a3f03d2","Deep reinforcement learning (DRL) has achieved remarkable success in diverse areas, including gaming AI, scientific simulations, and large-scale (HPC) system scheduling. DRL training, which involves a trial-and-error process, demands considerable time and computational resources. To overcome this challenge, distributed DRL algorithms and frameworks have been developed to expedite training by leveraging large-scale resources. However, existing distributed DRL solutions rely on synchronous learning with serverful infrastructures, suffering from low training efficiency and overwhelming training costs. This paper proposes Stellaris, the first to introduce a generic asynchronous learning paradigm for distributed DRL training with serverless computing. We devise an importance sampling truncation technique to stabilize DRL training and develop a staleness-aware gradient aggregation method tailored to the dynamic staleness in asynchronous serverless DRL training. Experiments on AWS EC2 regular testbeds and HPC clusters show that Stellaris outperforms existing state-of-the-art DRL baselines by achieving 2.2 × higher rewards (i.e., training quality) and reducing 41% training costs. © 2024 IEEE.","","Contrastive Learning; Reinforcement learning; Computational resources; Large-scales; Learning frameworks; Reinforcement learning algorithms; Reinforcement learning solution; Reinforcement learnings; Scientific simulations; System scheduling; Training costs; Trial-and-error process; Deep reinforcement learning","IEEE Computer Society","ACM; ACM�s Special Interest Group on High Performance Computing (SIGHPC); IEEE Technical Committee on High Performance Computing (TCHPC); Institute of Electrical and Electronics Engineers (IEEE) Computer Society","2024 International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2024","17 November 2024 through 22 November 2024","Atlanta","205531","Conference paper","Final","","Scopus","2-s2.0-85213889656"
"Marchese A.; Tomarchio O.","Marchese, Angelo (57670408700); Tomarchio, Orazio (6602163319)","57670408700; 6602163319","Orchestrating serverless applications in the Cloud-to-Edge continuum","2023","Mid4CC 2023 - Proceedings of the 2023 1st International Workshop on Middleware for the Computing Continuum, Part of: Middleware 2023","","","","12","17","5","3","10.1145/3631309.3632834","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181847845&doi=10.1145%2f3631309.3632834&partnerID=40&md5=1bc941d32508694a59d35c9cbef09dbb","The orchestration of serverless applications presents different challenges due to the increasing request load they need to manage and the time-sensitive requirements. The combination of Cloud and Edge computing paradigms attempts to avoid their pitfalls while taking the best of both worlds: Cloud scalability and computing closer to the Edge where data is typically generated. However, placing microservices in such heterogeneous environments while meeting QoS constraints is a challenging task due to the geo-distribution of nodes and varying computational resources. In this paper we propose to extend Knative, an example of a serverless computing framework based on the Kubernetes platform, to enable dynamic serverless application orchestration, taking into account both infrastructure and application states. Our approach aims to reduce QoS violations and improve application response time in Cloud-to-Edge continuum scenarios. © 2023 Owner/Author(s).","containers technology; knative; knative scheduler; orchestration; serverless computing","Cloud-computing; Computing paradigm; Container technology; Edge computing; Heterogeneous environments; Knative; Knative scheduler; Orchestration; QoS constraints; Serverless computing","Association for Computing Machinery, Inc","ACM","1st International Workshop on Middleware for the Computing Continuum, Mid4CC 2023","11 December 2023 through 15 December 2023","Bologna","195663","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85181847845"
"","","","MiddleWEdge 2023 - Proceedings of the 2nd International Workshop on Middleware for the Edge, Part of: ACM/IFIP Middleware 2023","2023","MiddleWEdge 2023 - Proceedings of the 2nd International Workshop on Middleware for the Edge, Part of: ACM/IFIP Middleware 2023","","","","","","28","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180326463&partnerID=40&md5=c0e3c505b71969c36b9204fbaee055dc","The proceedings contain 4 papers. The topics discussed include: offloading real-time tasks in IIoT environments under consideration of networking uncertainties; on the adversarial robustness of full integer quantized TinyML models at the edge; performance characterization of MQTT brokers in a device-local edge deployment; and Enoki: stateful distributed FaaS from edge to cloud.","","","Association for Computing Machinery, Inc","ACM","2nd International Workshop on Middleware for the Edge, MiddleWEdge 2023, co-located with ACM Middleware 2023","11 December 2023","Bologna","194762","Conference review","Final","","Scopus","2-s2.0-85180326463"
"Copik M.; Calotoiu A.; Zhou P.; Taranov K.; Hoefler T.","Copik, Marcin (57194605130); Calotoiu, Alexandru (55415831500); Zhou, Pengyu (59189003400); Taranov, Konstantin (57200202367); Hoefler, Torsten (14018121700)","57194605130; 55415831500; 59189003400; 57200202367; 14018121700","FaaSKeeper: Learning from Building Serverless Services with ZooKeeper as an Example","2024","HPDC 2024 - Proceedings of the 33rd International Symposium on High-Performance Parallel and Distributed Computing","","","","94","108","14","2","10.1145/3625549.3658661","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204935061&doi=10.1145%2f3625549.3658661&partnerID=40&md5=87f655732591b651bc3f1c6d0d37f287","FaaS (Function-as-a-Service) revolutionized cloud computing by replacing persistent virtual machines with dynamically allocated resources. This shift trades locality and statefulness for a pay-as-you-go model more suited to variable and infrequent workloads. However, the main challenge is to adapt services to the serverless paradigm while meeting functional, performance, and consistency requirements. In this work, we push the boundaries of FaaS computing by designing a serverless variant of ZooKeeper, a centralized coordination service with a safe and wait-free consensus mechanism. We define synchronization primitives to extend the capabilities of scalable cloud storage and outline a set of requirements for efficient computing with serverless. In FaaSKeeper, the first coordination service built on serverless functions and cloud-native services, we explore the limitations of serverless offerings and propose improvements essential for complex and latency-sensitive applications. We share serverless design lessons based on our experiences of implementing a ZooKeeper model deployable to clouds today. FaaSKeeper maintains the same consistency guarantees and interface as ZooKeeper, with a serverless price model that lowers costs up to 110 - 719x on infrequent workloads. © 2024 held by the owner/author(s).","cloud computing; faas; function-as-a-service; serverless; zookeeper","Cloud-computing; Faas; Function-as-a-service; Functional performance; Functional requirement; Go model; Pay as you go; Serverless; Statefulness; Zookeeper; Cloud storage","Association for Computing Machinery, Inc","ACM SIGARCH; CORNELIS NETWORKS; E4 COMPUTER ENGINEERING; NEC Laboratories America; VAST","33rd International Symposium on High-Performance Parallel and Distributed Computing, HPDC 2024","3 June 2024 through 7 June 2024","Pisa","202382","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85204935061"
"Joosen A.; Hassan A.; Asenov M.; Singh R.; Darlow L.; Wang J.; Deng Q.; Barker A.","Joosen, Artjom (58743605000); Hassan, Ahmed (58743078800); Asenov, Martin (57208301443); Singh, Rajkarn (56297011800); Darlow, Luke (56543450800); Wang, Jianfeng (58855387700); Deng, Qiwen (58759238900); Barker, Adam (56066309500)","58743605000; 58743078800; 57208301443; 56297011800; 56543450800; 58855387700; 58759238900; 56066309500","Serverless Cold Starts and Where to Find Them","2025","EuroSys 2025 - Proceedings of the 2025 20th European Conference on Computer Systems","","","","938","953","15","0","10.1145/3689031.3696073","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002239381&doi=10.1145%2f3689031.3696073&partnerID=40&md5=5bcdcab14d28e97db35dc624c60c8308","This paper analyzes a month-long trace of 85 billion user requests and 11.9 million cold starts from Huawei’s serverless cloud platform. Our analysis spans workloads from five data centers. We focus on cold starts and provide a comprehensive examination of the underlying factors influencing the number and duration of cold starts. These factors include trigger types, request synchronicity, runtime languages, and function resource allocations. We investigate components of cold starts, including pod allocation time, code and dependency deployment time, and scheduling delays, and examine their relationships with runtime languages, trigger types, and resource allocation. We introduce pod utility ratio to measure the pod’s useful lifetime relative to its cold start time, giving a more complete picture of cold starts, and see that some pods with long cold start times have longer useful lifetimes. Our findings reveal the complexity and multifaceted origins of the number, duration, and characteristics of cold starts, driven by differences in trigger types, runtime languages, and function resource allocations. For example, cold starts in Region 1 take up to 7 seconds, dominated by dependency deployment time and scheduling. In Region 2, cold starts take up to 3 seconds and are dominated by pod allocation time. Based on this, we identify opportunities to reduce the number and duration of cold starts using strategies for multi-region scheduling. Finally, we suggest directions for future research to address these challenges and enhance the performance of serverless cloud platforms. © 2025 Copyright held by the owner/author(s).","cloud; cold starts; datasets; serverless; time series","Resource allocation; Cloud platforms; Cold-start; Dataset; Deployment time; Paper analysis; Resources allocation; Runtimes; Serverless; Times series; Useful lifetime; Cloud platforms","Association for Computing Machinery, Inc","ACM SIGOPS; Amazon Web Services; AMD; et al.; Huawei; Microsoft","20th European Conference on Computer Systems, EuroSys 2025, co-located 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2025","30 March 2025 through 3 April 2025","Rotterdam","207851","Conference paper","Final","","Scopus","2-s2.0-105002239381"
"Roy R.B.; Kanakagiri R.; Jiang Y.; Tiwari D.","Roy, Rohan Basu (57219249946); Kanakagiri, Raghavendra (55187931700); Jiang, Yankai (58960464100); Tiwari, Devesh (23467777300)","57219249946; 55187931700; 58960464100; 23467777300","The Hidden Carbon Footprint of Serverless Computing","2024","SoCC 2024 - Proceedings of the 2024 ACM Symposium on Cloud Computing","","","","570","579","9","3","10.1145/3698038.3698546","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215516558&doi=10.1145%2f3698038.3698546&partnerID=40&md5=e6091f2e050cd6f2a18f796afd7c6c61","Due to the unique aspects of serverless computing like keep-alive and co-location of functions, it is challenging to account for its carbon footprint. This is the first work to introduce the need for systematic methodologies for carbon accounting in the serverless environment, propose new methodologies and in-depth analysis, and highlight how the carbon footprint estimation can vary based on the chosen methodology. It discusses how serverless-specific scheduling choices can impact the tradeoffs between performance and carbon footprint, with an aim toward standardizing methodological choices and identifying opportunities for future improvements. © 2024 Owner/Author.","Carbon Footprint; Serverless Computing; Sustainability","Carbon capture and storage; Carbon sequestration; Carbon accounting; Colocations; Future improvements; In-depth analysis; Keep-alive; Performance; Serverless computing; Systematic methodology; Carbon capture and utilization","Association for Computing Machinery, Inc","ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Groups on Management of Data (SIGMOD); Google; IBM; Microsoft","15th Annual ACM Symposium on Cloud Computing, SoCC 2024","20 November 2024 through 22 November 2024","Redmond","205525","Conference paper","Final","","Scopus","2-s2.0-85215516558"
"Cordingly R.","Cordingly, Robert (57220806485)","57220806485","Sky Computing for Serverless","2023","Middleware Demos, Posters and Doctoral Symposium 2023: Proceedings of the 24th International Middleware Conference Demos, Posters and Doctoral Symposium, Part of: Middleware 2023","","","","1","2","1","0","10.1145/3626564.3629087","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180125508&doi=10.1145%2f3626564.3629087&partnerID=40&md5=22ee329fe8087784b078a933c4a8fdb2","Sky Computing creates an all-in-one computing platform using resources from various cloud providers. A Serverless Sky architecture can aggregate serverless resources to achieve low costs, high performance, and fault tolerance. This research will create a Serverless Sky platform to then: evaluate performance implications, design and evaluate different architectures, and investigate autonomous resource aggregation for smart self-management. © 2023 Owner/Author.","","Cloud providers; Computing platform; Low-costs; Performance; Resource aggregation; Self management; Fault tolerance","Association for Computing Machinery, Inc","ACM","24th International Middleware Conference Demos, Posters and Doctoral Symposium, Middleware 2023","11 December 2023 through 15 December 2023","Bologna","194866","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85180125508"
"Szekely A.; Belay A.; Morris R.; Kaashoek M.F.","Szekely, Ariel (57219246701); Belay, Adam (36185141400); Morris, Robert (7404059170); Kaashoek, M. Frans (57207515364)","57219246701; 36185141400; 7404059170; 57207515364","Unifying serverless and microservice workloads with SigmaOS","2024","SOSP 2024 - Proceedings of the 2024 ACM SIGOPS 30th Symposium on Operating Systems Principles","","","","385","402","17","0","10.1145/3694715.3695947","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215517766&doi=10.1145%2f3694715.3695947&partnerID=40&md5=56228b38c30dd4e4ba3043eee0c86bf1","Many cloud applications use both serverless functions, for bursts of stateless parallel computation, and container orchestration, for long-running microservices and tasks that need to interact. Ideally a single platform would offer the union of these systems' capabilities, but neither is sufficient to act as that single platform: serverless functions are lightweight but cannot act as servers with long-term state, while container orchestration offers general-purpose computation but instance start-up takes too long to support burst parallelism.σOS is a new multi-tenant cloud operating system that combines the best of container orchestration and serverless in one platform with one API. σOS computations, called procs, can be long-running, stateful, and interact with each other, making them a good match for both serverless and microservice tasks. A key aspect of the σOS design is its cloud-centric API, which provides flexible management of computation, a novel abstraction for communication endpoints, σEPs - -which allow procs of a tenant to communicate efficiently but prohibits procs from sending packets to other tenants - -and a flexible naming system to name, for example, σEPs.Quick proc start-up is important for serverless uses. A key enabling observation is that both serverless and microservice applications rely on cloud services for much of the work traditionally done by the local OS (e.g., access to durable storage and additional compute resources). σOS exploits this observation by providing only a small and generic local operating system image to each proc, which can be created much more quickly than a container orchestration instance since σOS need not install application-specific filesystem content or (due to σOS's σEPs) configure an isolated overlay network.Microbenchmarks show that σOS can cold start a proc in 7.7 msec and can create 36,650 procs per second, distributing them over a 24-machine cluster. An evaluation of σOS with two microservice applications from DeathStarBench, a MapReduce application, and an image processing benchmark, shows that the σOS API supports both microservices and lambda-style computations, and provides better performance than corresponding versions on AWS Lambda and Kubernetes. © 2024 Copyright held by the owner/author(s).","","Binary decision diagrams; C (programming language); Cloud computing; Cloud platforms; Cluster computing; Data flow analysis; Graph structures; Hierarchical systems; Interoperability; Message passing; Plastic bottles; Problem oriented languages; Procedure oriented languages; Program debugging; Unified Modeling Language; Cloud applications; Cloud services; Flexible management; General-purpose computations; Lambda's; Multi tenants; Naming systems; Parallel Computation; Single platform; System capabilities; Benchmarking","Association for Computing Machinery, Inc","ACM SIGOPS; Akamai; Amazon; et al.; FUTUREWEI Technologies; NSF","30th ACM Symposium on Operating Systems Principles, SOSP 2024","4 November 2024 through 6 November 2024","Austin","205375","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85215517766"
"","","","Middleware 2023 - Proceedings of the 24th ACM/IFIP International Middleware Conference","2023","Middleware 2023 - Proceedings of the 24th ACM/IFIP International Middleware Conference","","","","","","329","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180091189&partnerID=40&md5=5fd9c57847d3e0170b4032ee9f3f1b51","The proceedings contain 24 papers. The topics discussed include: fast VM replication on heterogeneous hypervisors for robust fault tolerance; sora: a latency sensitive approach for microservice soft resource adaptation; INSANE: a unified middleware for QoS-aware network acceleration in edge cloud computing; an end-to-end performance comparison of seven permissioned blockchain systems; BASALT: a rock-solid byzantine-tolerant peer sampling for very large decentralized networks; OrderlessChain: a CRDT-based BFT coordination-free blockchain without global order of transactions; characterizing distributed machine learning workloads on Apache spark; Pravega: a tiered storage system for data streams; bridging the gap of timing assumptions in byzantine consensus; and kernel-as-a-service: a serverless programming model for heterogeneous hardware accelerators.","","","Association for Computing Machinery, Inc","","24th ACM/IFIP International Middleware Conference, Middleware 2023","11 December 2023 through 15 December 2023","Bologna","194765","Conference review","Final","","Scopus","2-s2.0-85180091189"
"Lertpongrujikorn P.; Nguyen H.D.; Salehi M.A.","Lertpongrujikorn, Pawissanutt (57762938900); Nguyen, Hai Duc (57214666053); Salehi, Mohsen Amini (7006812609)","57762938900; 57214666053; 7006812609","Streamlining Cloud-Native Application Development and Deployment with Robust Encapsulation","2024","SoCC 2024 - Proceedings of the 2024 ACM Symposium on Cloud Computing","","","","847","865","18","0","10.1145/3698038.3698552","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215529028&doi=10.1145%2f3698038.3698552&partnerID=40&md5=e62f99e6746d4b564df015e6ce55c93f","Current Serverless abstractions (e.g., FaaS) poorly support non-functional requirements (e.g., QoS and constraints), are provider-dependent, and are incompatible with other cloud abstractions (e.g., databases). As a result, application developers have to undergo numerous rounds of development and manual deployment refinements to finally achieve their desired quality and efficiency. In this paper, we present Object-as-a-Service (OaaS) - -a novel serverless paradigm that borrows the object-oriented programming concepts to encapsulate business logic, data, and non-functional requirements into a single deployment package, thereby streamlining provider-agnostic cloud-native application development. We also propose a declarative interface for the non-functional requirements of applications that relieves developers from daunting refinements to meet their desired QoS and deployment constraint targets. We realized the OaaS paradigm through a platform called Oparaca and evaluated it against various real-world applications and scenarios. The evaluation results demonstrate that Oparaca can enhance application performance by 60× and improve reliability by 50× through latency, throughput, and availability enforcement - -all with remarkably less development and deployment time and effort. © 2024 ACM.","abstraction; cloud computing; cloud-native programming; function-as-a-service; object-asa-service; serverless","Data as a service (DaaS); Object oriented programming; Abstraction; Application deployment; Application development; Cloud-computing; Cloud-native programming; Function-as-a-service; Native programming; Non-functional requirements; Object-asa-service; Serverless; Cloud platforms","Association for Computing Machinery, Inc","ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Groups on Management of Data (SIGMOD); Google; IBM; Microsoft","15th Annual ACM Symposium on Cloud Computing, SoCC 2024","20 November 2024 through 22 November 2024","Redmond","205525","Conference paper","Final","","Scopus","2-s2.0-85215529028"
"Pfandzelter T.; Dhakal A.; Frachtenberg E.; Chalamalasetti S.R.; Emmot D.; Hogade N.; Enriquez R.P.H.; Rattihalli G.; Bermbach D.; Milojicic D.","Pfandzelter, Tobias (57208737730); Dhakal, Aditya (57200495213); Frachtenberg, Eitan (6507589117); Chalamalasetti, Sai Rahul (26324764600); Emmot, Darel (57215354149); Hogade, Ninad (57215138064); Enriquez, Rolando Pablo Hong (55099055200); Rattihalli, Gourav (57193494996); Bermbach, David (51461094200); Milojicic, Dejan (6603838893)","57208737730; 57200495213; 6507589117; 26324764600; 57215354149; 57215138064; 55099055200; 57193494996; 51461094200; 6603838893","Kernel-as-a-Service: A Serverless Programming Model for Heterogeneous Hardware Accelerators","2023","Middleware 2023 - Proceedings of the 24th ACM/IFIP International Middleware Conference","","","","192","206","14","1","10.1145/3590140.3629115","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179886670&doi=10.1145%2f3590140.3629115&partnerID=40&md5=5c5c7b7ad69fed72b4cf669cc53c12e3","With the slowing of Moore's law and decline of Dennard scaling, computing systems increasingly rely on specialized hardware accelerators in addition to general-purpose compute units. Increased hardware heterogeneity necessitates disaggregating applications into workflows of fine-grained tasks that run on a diverse set of CPUs and accelerators. Current accelerator delivery models cannot support such applications efficiently, as (1) the overhead of managing accelerators erases performance benefits for fine-grained tasks; (2) exclusive accelerator use per task leads to underutilization; and (3) specialization increases complexity for developers. We propose adopting concepts from Function-as-a-Service (FaaS), which has solved these challenges for general-purpose CPUs in cloud computing. Kernel-as-a-Service (KaaS) is a novel serverless programming model for generic compute accelerators that aids heterogeneous workflows by combining the ease-of-use of higher-level abstractions with the performance of low-level hand-tuned code. We evaluate KaaS with a focus on the breadth of the idea and its generality to diverse architectures rather than on an in-depth implementation for a single accelerator. Using proof-of-concept prototypes, we show that this programming model provides performance, performance efficiency, and ease-of-use benefits across a diverse range of compute accelerators. Despite increased levels of abstraction, when compared to a naive accelerator implementation, KaaS reduces completion times for fine-grained tasks by up to 96.0% (GPU), 68.4% (FPGA), 98.6% (TPU), and 34.9% (QPU) in our experiments.  © 2023 ACM.","Accelerators; Heterogeneity; Serverless","Computer hardware; Program processors; Ease-of-use; Fine grained; Hardware accelerators; Heterogeneity; Heterogeneous hardware; Moore Law; Performance; Programming models; Serverless; Work-flows; Abstracting","Association for Computing Machinery, Inc","","24th ACM/IFIP International Middleware Conference, Middleware 2023","11 December 2023 through 15 December 2023","Bologna","194765","Conference paper","Final","","Scopus","2-s2.0-85179886670"
"Zahra Z.; Li Z.; Filgueira R.","Zahra, Zaynab (58621483100); Li, Zihao (58621414800); Filgueira, Rosa (9733528300)","58621483100; 58621414800; 9733528300","Laminar: A New Serverless Stream-based Framework with Semantic Code Search and Code Completion","2023","ACM International Conference Proceeding Series","","","","2009","2020","11","2","10.1145/3624062.3624280","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178128961&doi=10.1145%2f3624062.3624280&partnerID=40&md5=566021dd8ac1829eb1ad05290f433121","This paper introduces Laminar, a novel serverless framework based on dispel4py, a parallel stream-based dataflow library. Laminar efficiently manages streaming workflows and components through a dedicated registry, offering a seamless serverless experience. Leveraging large lenguage models, Laminar enhances the framework with semantic code search, code summarization, and code completion. This contribution enhances serverless computing by simplifying the execution of streaming computations, managing data streams more efficiently, and offering a valuable tool for both researchers and practitioners. © 2023 ACM.","code completion; code summarization; dispel4py; semantic code search; serverless computing; streaming applications; transformers","Computer hardware description languages; Data streams; Semantic Web; Code completions; Code search; Code summarization; Dispel4py; Semantic code search; Semantic codes; Serverless computing; Stream-based; Streaming applications; Transformer; Semantics","Association for Computing Machinery","ACM's Special Interest Group on High Performance Computing (SIGHPC); Association for Computing Machinery; IEEE�s Technical Committee on High Performance Computing (TCHPC); Institute of Electrical and Electronics Engineers (IEEE) Computer Society","2023 International Conference on High Performance Computing, Network, Storage, and Analysis, SC Workshops 2023","12 November 2023 through 17 November 2023","Denver","194341","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85178128961"
"","","","Middleware Demos, Posters and Doctoral Symposium 2023: Proceedings of the 24th International Middleware Conference Demos, Posters and Doctoral Symposium, Part of: Middleware 2023","2023","Middleware Demos, Posters and Doctoral Symposium 2023: Proceedings of the 24th International Middleware Conference Demos, Posters and Doctoral Symposium, Part of: Middleware 2023","","","","","","38","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180353963&partnerID=40&md5=10892b75773bfca5a784d1a8da4bf997","The proceedings contain 16 papers. The topics discussed include: sky computing for serverless; serverless abstractions for edge computing in large low-earth orbit satellite networks; adaptive thresholding for fair and robust biometric authentication; better orchestration for SLO-oriented cross-site microservices in multi-tenant cloud/edge continuum; secure and scalable policy management in cloud native networking; towards effective yet frugal blockchain network emulation; decentralized service composition applications (dXapps): towards a new paradigm for building blockchain applications; towards the automatic adaptation of stateful microservices applications across edge and cloud; towards a user-centric decentralized web: lightweight clients as first-class blockchain citizens; and towards building edge-side common data processing services on the computing continuum.","","","Association for Computing Machinery, Inc","ACM","24th International Middleware Conference Demos, Posters and Doctoral Symposium, Middleware 2023","11 December 2023 through 15 December 2023","Bologna","194866","Conference review","Final","","Scopus","2-s2.0-85180353963"
"Zhang X.; He Q.; Fan H.; Wu S.","Zhang, Xinmin (59416941900); He, Qiang (55217854300); Fan, Hao (57215688890); Wu, Song (56931730600)","59416941900; 55217854300; 57215688890; 56931730600","Faascale: Scaling MicroVM Vertically for Serverless Computing with Memory Elasticity","2024","SoCC 2024 - Proceedings of the 2024 ACM Symposium on Cloud Computing","","","","196","212","16","0","10.1145/3698038.3698512","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215508885&doi=10.1145%2f3698038.3698512&partnerID=40&md5=cf1d722d323a9741074bbde8738bdae2","This paper quantitatively analyses the potential of vertical scaling MicroVMs in serverless computing. Our analysis shows that under real-world serverless workloads, vertical scaling can significantly improve execution performance and resource utilization. However, we also find that the memory scaling of MicroVMs is the bottleneck that hinders vertical scaling from reaching the performance ceiling. We propose Faascale, a novel mechanism that efficiently scales the memory of MicroVMs for serverless applications. Faascale employs a series of techniques to tackle this bottleneck: 1) it sizes up/down the memory for a MicroVM by blocks that bind with a function instance instead of general pages; and 2) it pre-populates physical memory for function instances to reduce the delays introduced by the lazy-population. Compared with existing memory scaling mechanisms, Faascale improves the memory scaling efficiency by 2 to 3 orders of magnitude. We implement Faascale on Amazon Firecracker to evaluate its gains for the serverless platform. The results of experiments conducted on eight serverless benchmark functions demonstrate that compared with horizontal scaling strategies based the state-of-the-art snapshots technique, Faascale reduces time for cold-starting MicroVMs by 89.01% and functions execution time by 23.93% on average. © 2024 ACM.","Cold Start; MicroVMs; Serverless Computing; Vertical Scaling","Cold-start; Execution performance; Microvms; Physical memory; Real-world; Resources utilizations; Scalings; Serverless computing; Vertical scaling; Scaling laws","Association for Computing Machinery, Inc","ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Groups on Management of Data (SIGMOD); Google; IBM; Microsoft","15th Annual ACM Symposium on Cloud Computing, SoCC 2024","20 November 2024 through 22 November 2024","Redmond","205525","Conference paper","Final","","Scopus","2-s2.0-85215508885"
"Mohanty S.; Bhasi V.M.; Son M.; Kandemir M.T.; Das C.","Mohanty, Shruti (57219667472); Bhasi, Vivek M. (57343857600); Son, Myungjun (57196193363); Kandemir, Mahmut Taylan (35549787100); Das, Chita (7201851990)","57219667472; 57343857600; 57196193363; 35549787100; 7201851990","FAAStloop: Optimizing Loop-Based Applications for Serverless Computing","2024","SoCC 2024 - Proceedings of the 2024 ACM Symposium on Cloud Computing","","","","943","960","17","1","10.1145/3698038.3698560","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215507435&doi=10.1145%2f3698038.3698560&partnerID=40&md5=76d6709d8096b305735083bff3e93028","Serverless Computing has garnered significant interest for executing High-Performance Computing (HPC) applications in recent years, attracting attention for its elastic scalability, reduced entry barriers, and pay-per-use pricing model. Specifically, highly parallel HPC apps can be divided and offloaded to multiple Serverless Functions (SFs) that execute their respective tasks concurrently and, finally, their results are stored/aggregated. While state-of-the-art userside serverless frameworks have attempted to fine-tune task division amongst the SFs to optimize for performance and/or cost, they have either used static task division parameters or have only focused on minimizing the number of SFs through task packing. However, these methods treat the HPC code as a black-box and usually require significant manual intervention to find the optimal task division. Since a significant portion of the HPC applications have a loop structure, in this work, we try to answer the following two questions: (i) Can modifying the loop structure in the HPC code, originally optimized for monolithic (non-serverless) frameworks, enhance performance and reduce costs in a serverless architecture?, and (ii) Can we develop a framework that allows for an efficient transition of monolithic code to serverless, with minimum user input? To this end, we propose a novel framework, FAAStloop, which intelligently employs loop-based optimizations (as well as task packing) in SF containers to optimally execute HPC apps across SFs. FAAStloop chooses the relevant optimization parameters using statistical models (constructed via app profiling) that are able to predict the relevant performance/cost metrics as a function of our choice of parameters. Our extensive experimental evaluation of FAAStloop on the AWS Lambda platform reveals that our framework outperforms state-of-the-art works by up to 3.3× and 2.1×, in terms of end-to-end execution latency and cost, respectively. © 2024 ACM.","compiler; HPC; loop optimizations; serverless","Application programming interfaces (API); Black-box testing; C (programming language); Network-on-chip; Problem oriented languages; Program debugging; Structural optimization; Compiler; Computing codes; High-performance computing; High-performance computing applications; Loop optimizations; Performance computing; Performance costs; Serverless; State of the art; Task division; Program compilers","Association for Computing Machinery, Inc","ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Groups on Management of Data (SIGMOD); Google; IBM; Microsoft","15th Annual ACM Symposium on Cloud Computing, SoCC 2024","20 November 2024 through 22 November 2024","Redmond","205525","Conference paper","Final","","Scopus","2-s2.0-85215507435"
"Ghosh A.; Yadwadkar N.J.; Erez M.","Ghosh, Anyesha (59514280000); Yadwadkar, Neeraja J. (56429519900); Erez, Mattan (55332163300)","59514280000; 56429519900; 55332163300","Fast and Efficient Scaling for Microservices with SurgeGuard","2024","International Conference for High Performance Computing, Networking, Storage and Analysis, SC","","","","","","","0","10.1109/SC41406.2024.00103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214989190&doi=10.1109%2fSC41406.2024.00103&partnerID=40&md5=803fdd48a105e7452e3dd39827d8ba5f","The microservice architecture is increasingly popular for flexible, large-scale online applications. However, existing resource management mechanisms incur high latency in detecting Quality of Service (QoS) violations, and hence, fail to allocate resources effectively under commonly-observed varying load conditions. This results in over-allocation coupled with a late response that increase both the total cost of ownership and the magnitude of each QoS violation event. We present SurgeGuard, a decentralized resource controller for microservice applications specifically designed to guard application QoS during surges in load and network latency. SurgeGuard uses the key insight that for rapid detection and effective management of QoS violations, the controller must be aware of any available slack in latency and communication patterns between microservices within a task-graph. Our experiments show that for the workloads in DeathStarBench, SurgeGuard on average reduces the combined violation magnitude and duration by 61.1 % and 93.7 %, respectively, compared to the well-known Parties and Caladan algorithms, and requires 8 % fewer resources than Parties. © 2024 IEEE.","Cloud computing; datacenters; microservices; quality-of-service; resource management; serverless","Information management; Resource allocation; Cloud-computing; Datacenter; Large-scales; Microservice; On-line applications; Quality-of-service; Resource management; Scalings; Serverless; Service violations; Quality of service","IEEE Computer Society","ACM; ACM�s Special Interest Group on High Performance Computing (SIGHPC); IEEE Technical Committee on High Performance Computing (TCHPC); Institute of Electrical and Electronics Engineers (IEEE) Computer Society","2024 International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2024","17 November 2024 through 22 November 2024","Atlanta","205531","Conference paper","Final","","Scopus","2-s2.0-85214989190"
"Hong Z.; Lin J.; Guo S.; Luo S.; Chen W.; Wattenhofer R.; Yu Y.","Hong, Zicong (57208396334); Lin, Jian (57242947400); Guo, Song (58594251100); Luo, Sifu (58480644800); Chen, Wuhui (23092980500); Wattenhofer, Roger (6701529043); Yu, Yue (55566298800)","57208396334; 57242947400; 58594251100; 58480644800; 23092980500; 6701529043; 55566298800","Optimus: Warming Serverless ML Inference via Inter-Function Model Transformation","2024","EuroSys 2024 - Proceedings of the 2024 European Conference on Computer Systems","","","","1039","1053","14","10","10.1145/3627703.3629567","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190253639&doi=10.1145%2f3627703.3629567&partnerID=40&md5=006581feed802aef60d5f6dd094c0fd0","Serverless ML inference is an emerging cloud computing paradigm for low-cost, easy-to-manage inference services. In serverless ML inference, each call is executed in a container; however, the cold start of containers results in long inference delays. Unfortunately, most existing works do not work well because they still need to load models into containers from scratch, which is the bottleneck based on our observations. Therefore, this paper proposes a low-latency serverless ML inference system called Optimus via a new container management scheme. Our key insight is that the loading of a new model can be significantly accelerated when using an existing model with a similar structure in a warm but idle container. We thus develop a novel idea of inter-function model transformation for serverless ML inference, which delves into models within containers at a finer granularity of operations, designs a set of in-container meta-operators for both CNN and transformer model transformation, and develops an efficient scheduling algorithm with linear complexity for a low-cost transformation strategy. Our evaluations on thousands of models show that Optimus reduces inference latency by 24.00% ∼ 47.56% in both simulated and real-world workloads compared to state-of-the-art work.  © 2024 ACM.","cold start; ML inference; Serverless computing","Computational complexity; Costs; Inference engines; Linear transformations; Scheduling algorithms; Cloud-computing; Cold-start; Computing paradigm; Function modelling; Load modeling; Low-costs; ML inference; Model transformation; Optimus; Serverless computing; Containers","Association for Computing Machinery, Inc","ACM SIGOPS; Ant Group Research; Google; Huawei; KAUST; Red Hat","19th European Conference on Computer Systems, EuroSys 2024","22 April 2024 through 25 April 2024","Athens","199050","Conference paper","Final","","Scopus","2-s2.0-85190253639"
"Farahani R.; Loh F.; Roman D.; Prodan R.","Farahani, Reza (57220368259); Loh, Frank (57190303633); Roman, Dumitru (23467687800); Prodan, Radu (8858675500)","57220368259; 57190303633; 23467687800; 8858675500","Serverless Workflow Management on the Computing Continuum: A Mini-Survey","2024","ICPE 2024 - Companion of the 15th ACM/SPEC International Conference on Performance Engineering","","","","146","150","4","3","10.1145/3629527.3652901","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193980736&doi=10.1145%2f3629527.3652901&partnerID=40&md5=4d1c60491c77d280f2b8d2e25a92ddf0","The growing desire among application providers for a cost model based on pay-per-use, combined with the need for a seamlessly integrated platform to manage the complex workflows of their applications, has spurred the emergence of a promising computing paradigm known as serverless computing. Although serverless computing was initially considered for cloud environments, it has recently been extended to other layers of the computing continuum, i.e., edge and fog. This extension emphasizes that the proximity of computational resources to data sources can further reduce costs and improve performance and energy efficiency. However, orchestrating the computing continuum in complex application workflows, including a set of serverless functions, introduces new challenges. This paper investigates the opportunities and challenges introduced by serverless computing for workflow management systems (WMS) on the computing continuum. In addition, the paper provides a taxonomy of state-of-the-art WMSs and reviews their capabilities.  © 2024 Copyright held by the owner/author(s).","edge-cloud continuum; function scheduling; function-as-a-service (faas); serverless computing; service orchestration; sustainability; workflow; workflow management systems (wms)","Energy efficiency; Green computing; Sustainable development; Edge clouds; Edge-cloud continuum; Function scheduling; Function-as-a-service; Serverless computing; Service orchestration; Work-flows; Workflow management system; Workflow management systems; Workflow managements; Workflow management","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC Reaserch","15th ACM/SPEC International Conference on Performance Engineering, ICPE 2024","7 May 2024 through 11 May 2024","London","199384","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85193980736"
"Alverti C.; Psomadakis S.; Ocalan B.; Jaiswal S.; Xu T.; Torrellas J.","Alverti, Chloe (57216301139); Psomadakis, Stratos (55830537900); Ocalan, Burak (58965325800); Jaiswal, Shashwat (57403846800); Xu, Tianyin (24825809900); Torrellas, Josep (7003395953)","57216301139; 55830537900; 58965325800; 57403846800; 24825809900; 7003395953","CXLfork: Fast Remote Fork over CXL Fabrics","2025","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","2","","","210","226","16","0","10.1145/3676641.3715988","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002574133&doi=10.1145%2f3676641.3715988&partnerID=40&md5=cf6cf26ea3856c56c82c61bef7ea42aa","The shared and distributed memory capabilities of the emerging Compute Express Link (CXL) interconnect urge us to rethink the traditional interfaces of system software. In this paper, we explore one such interface: remote fork using CXL-attached shared memory for cluster-wide process cloning. We present CXLfork, a remote fork interface that realizes close to zero-serialization, zero-copy process cloning across nodes over CXL fabrics. CXLfork utilizes globally-shared CXL memory for cluster-wide deduplication of process states. It also enables fine-grained control of state tiering between local and CXL memory. We use CXLfork to develop CXL-porter, an efficient horizontal autoscaler for serverless functions deployed on CXL fabrics. CXLfork minimizes cold-start overhead without sacrificing local memory. CXLfork attains restore latency close to that of a local fork, outperforming state-of-practice by 2.26x on average, and reducing local memory consumption by 87% on average. © 2025 ACM.","checkpoint restore; cold start; cxl; process forking; remote memory; serverless computing","Checkpoint restore; Cold-start; Cxl; Distributed Memory; Local memory; Memory capabilities; Process forking; Remote memory; Serverless computing; Shared memory; Memory architecture","Association for Computing Machinery","ACM SIGARCH; ACM SIGOPS; ACM SIGPLAN","30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2025","30 March 2025 through 3 April 2025","Rotterdam","207994","Conference paper","Final","","Scopus","2-s2.0-105002574133"
"Eizaguirre G.T.; Barcelona-Pons D.; Arjona A.; Vernik G.; García-López P.; Alexandrov T.","Eizaguirre, Germán T. (57221861497); Barcelona-Pons, Daniel (57206657129); Arjona, Aitor (57218478604); Vernik, Gil (27068121500); García-López, Pedro (24479469800); Alexandrov, Theodore (26040516400)","57221861497; 57206657129; 57218478604; 27068121500; 24479469800; 26040516400","Serverful Functions: Leveraging Servers in Complex Serverless Workflows (industry track)","2024","Middleware Industrial Track 2024 - Proceedings of the Middleware Industrial Track, Part of: Middleware 2024","","","","15","21","6","0","10.1145/3700824.3701095","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215507706&doi=10.1145%2f3700824.3701095&partnerID=40&md5=fd9030289a5926597d8732a784bd89a4","The scalability of cloud functions makes them a convenient backend for elastic data analytics pipelines where parallelism changes drastically from one stage to the next. However, cloud functions require intermediate storage systems for communication, which limits the efficiency of stateful operations. Furthermore, cloud functions are expensive, which reduces the cost-effectiveness of pure serverless architectures. We propose a hybrid architecture for data analytics that uses cloud functions for embarrassingly parallel stages and virtual cloud instances for stateful operations under a unified serverless programming framework. Extending Lithops, a serverless programming library, we implement a parallel programming interface that proactively provisions serverless and serverful cloud resources with minimal user intervention. We validate the feasibility of a hybrid architecture, by comparing it to fully serverless and serverful versions of a production-level metabolomics pipeline. We show that mixing cloud functions with virtual instances increases the cost-effectiveness of the execution by up to 188.23% over the serverless implementation, while achieving a speedup of 3.64 compared to the serverful one. © 2024 Copyright held by the owner/author(s).","Cloud computing; function-as-a-service; resource allocation; resource efficiency; serverless computing","Cloud analytics; Cloud platforms; Parallel architectures; Resource allocation; Cloud-computing; Data analytics; Function-as-a-service; Hybrid architectures; Intermediate storage; Resource efficiencies; Resources allocation; Serverless computing; Storage systems; Work-flows; Cloud computing architecture","Association for Computing Machinery, Inc","ACM","2024 Middleware Industrial Track, Middleware Industrial Track 2024","2 December 2024 through 6 December 2024","Hong Kong","205526","Conference paper","Final","","Scopus","2-s2.0-85215507706"
"Iosup A.; Prodan R.; Varbanescu A.-L.","Iosup, Alexandru (23392350500); Prodan, Radu (8858675500); Varbanescu, Ana-Lucia (15020089900)","23392350500; 8858675500; 15020089900","GraphSys-2024: 2nd Workshop on Serverless, Extreme-Scale, and Sustainable Graph Processing Systems","2024","ICPE 2024 - Companion of the 15th ACM/SPEC International Conference on Performance Engineering","","","","95","96","1","0","10.1145/3629527.3651435","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194002934&doi=10.1145%2f3629527.3651435&partnerID=40&md5=2d6a3f04e70effbe1d996fc32c63ed51","[No abstract available]","","","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC Reaserch","15th ACM/SPEC International Conference on Performance Engineering, ICPE 2024","7 May 2024 through 11 May 2024","London","199384","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85194002934"
"Zhao Z.N.; Morrison A.; Fletcher C.W.; Torrellas J.","Zhao, Zirui Neil (57220543871); Morrison, Adam (14619745900); Fletcher, Christopher W. (36175594200); Torrellas, Josep (7003395953)","57220543871; 14619745900; 36175594200; 7003395953","Last-Level Cache Side-Channel Attacks Are Feasible in the Modern Public Cloud","2024","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","2","","","582","600","18","5","10.1145/3620665.3640403","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192172029&doi=10.1145%2f3620665.3640403&partnerID=40&md5=7b1643d51570081e111a8b920e5a0818","Last-level cache side-channel attacks have been mostly demonstrated in highly-controlled, quiescent local environments. Hence, it is unclear whether such attacks are feasible in a production cloud environment. In the cloud, side channels are flooded with noise from activities of other tenants and, in Function-as-a-Service (FaaS) workloads, the attacker has a very limited time window to mount the attack.In this paper, we show that such attacks are feasible in practice, although they require new techniques. We present an end-to-end, cross-tenant attack on a vulnerable ECDSA implementation in the public FaaS Google Cloud Run environment. We introduce several new techniques to improve every step of the attack. First, to speed-up the generation of eviction sets, we introduce L2-driven candidate address filtering and a Binary Search-based algorithm for address pruning. Second, to monitor victim memory accesses with high time resolution, we introduce Parallel Probing. Finally, we leverage power spectral density from signal processing to easily identify the victim's target cache set in the frequency domain. Overall, using these mechanisms, we extract a median value of 81% of the secret ECDSA nonce bits from a victim container in 19 seconds on average.  © 2024 Copyright is held by the owner/author(s). Publication rights licensed to ACM.","cloud computing; last-level cache side-channel attack; Prime+Probe attack","Cloud computing; Frequency domain analysis; Signal processing; Spectral density; Cloud environments; Cloud-computing; Last-level cache side-channel attack; Last-level caches; Local environments; Prime+probe attack; Public clouds; Side-channel; Side-channel attacks; Time windows; Side channel attack","Association for Computing Machinery","ACM Special Interest Group on Computer Architecture (SIGARCH); ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Group on Programming Languages (SIGPLAN)","29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2024","27 April 2024 through 1 May 2024","San Diego","198945","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85192172029"
"","","","Middleware 2024 - Proceedings of the 25th ACM International Middleware Conference","2024","Middleware 2024 - Proceedings of the 25th ACM International Middleware Conference","","","","","","509","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215507544&partnerID=40&md5=a5f485cb493938289744e2484330d7a0","The proceedings contain 39 papers. The topics discussed include: enhancing effective bidirectional isolation for function fusion in serverless architectures; on the semantic overlap of operators in stream processing engines; optimal resource efficiency with fairness in heterogeneous GPU clusters; SpotVerse: optimizing bioinformatics workflows with multi-region spot instances in galaxy and beyond; FLEdge: Benchmarking federated learning applications in edge computing systems; Dexter: a performance-cost efficient resource allocation manager for serverless data analytics; BASS: a resource orchestrator to account for vagaries in network conditions in community Wi-Fi mesh; near-storage processing in FaaS environments with Funclets; and chasing lightspeed consensus: fast wide-area byzantine replication with mercury.","","","Association for Computing Machinery, Inc","ACM","25th ACM International Middleware Conference, Middleware 2024","2 December 2024 through 6 December 2024","Hong Kong","205541","Conference review","Final","","Scopus","2-s2.0-85215507544"
"Triendl S.; Ristov S.","Triendl, Simon (59138695800); Ristov, Sashko (49561774300)","59138695800; 49561774300","Peeking behind the Serverless Implementations and Deployments of the Montage Workflow","2024","ICPE 2024 - Companion of the 15th ACM/SPEC International Conference on Performance Engineering","","","","196","203","7","0","10.1145/3629527.3651420","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193926998&doi=10.1145%2f3629527.3651420&partnerID=40&md5=a1f830afafe8592db6f378628c1160b6","The development of serverless scientific workflows is a complex and tedious procedure and opens several challenges in how to compose workflow processing steps as serverless functions and how much memory to assign to each serverless function, which affects not only the computing resources, but also the networking communication to the cloud storage. Merging multiple processing steps into a single serverless function (fusion) reduces the number of invocations, but restricts the developer to assign the maximum required memory of all fused processing steps, which may increase the overall costs. In this paper, we address the aforementioned challenges for the widely used Montage workflow. We created three different workflow implementations (fine, medium, and coarse) for two cloud providers AWS and GCP and deployed workflow functions with different memory assignments 135 MB, 512 MB, and 1 GB (function deployments). Our experiments show that many Montage functions run cheaper and faster with more memory on both providers. Consequently, selecting the most cost-effective memory configuration, as opposed to the minimal memory, resulted in a reduction of the makespan by 67.27% on AWS and 10.93% on GCP. Applying the same to workflow implementations with fewer functions (coarse) led to a further reduction in the makespan by 24.98% on AWS and 12.96% on GCP, while simultaneously reducing the total cost by 5.33% and 1.99%, respectively. Surprisingly, the fastest implementation was the medium implementation executed on AWS.  © 2024 Copyright held by the owner/author(s).","cost; faas; performance; serverless; workflows","Cost effectiveness; % reductions; Computing resource; Faas; Makespan; Performance; Processing steps; Scientific workflows; Serverless; Work-flows; Workflow implementations; Cost reduction","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC Reaserch","15th ACM/SPEC International Conference on Performance Engineering, ICPE 2024","7 May 2024 through 11 May 2024","London","199384","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85193926998"
"Xie M.; Qian C.; Litz H.","Xie, Minghao (57219773886); Qian, Chen (56081085200); Litz, Heiner (8395948600)","57219773886; 56081085200; 8395948600","En4S: Enabling SLOs in Serverless Storage Systems","2024","SoCC 2024 - Proceedings of the 2024 ACM Symposium on Cloud Computing","","","","160","177","17","1","10.1145/3698038.3698529","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215532562&doi=10.1145%2f3698038.3698529&partnerID=40&md5=274b3af408d3a4f260be6585d7d8b874","Serverless computing promises scalability and cost-efficiency by decomposing monolithic tasks into small, stateless, self-contained functions. As functions only reserve hardware resources during their lifetime, and serverless providers such as Amazon Lambda define strict data size limits [50], data required for the whole lifetime of a monolithic task needs to be kept in an external ephemeral data store. This approach increases costs and introduces performance variability, causing serverless applications to violate service level objectives (SLOs). Traditional cloud storage solutions, such as AWS S3 and Redis, fail to provide low-cost and the enforcement of SLOs, while prior works on disaggregated data stores do not scale sufficiently due to: (1) increased scheduling costs when supporting many SLOs; (2) performance degradation in the presence of burst allowances and worsened interference with lenient ones; and (3) failed service differentiation with increased number of SLO. These challenges make SLO enforcement in serverless environments difficult, leading to unpredictable performance and costs that undermine the benefits of serverless computing. We introduce En4S, a high-performance, flash-based storage system designed for data-intensive serverless applications. En4S employs a profile-based scheduling framework with adaptive strategies to efficiently scale to many tenants with different SLOs. Key features include dynamic tenant handling, adaptive burst control, token reclaim control, and various optimizations to minimize scheduling costs while maintaining superior performance. By re-enabling SLO enforcement for disaggregated flash storage in cloud-native environments, En4S is crucial for modern serverless applications. Our implementation on Amazon EC2 and Lambda demonstrates substantial performance and cost improvements while reliably ensuring SLO compliance, enhancing the viability of serverless storage systems. © 2024 Owner/Author.","Flash Disaggregation; QoS Scheduling; Serverless Computing","Cloud storage; Network security; Virtual storage; Data store; Disaggregation; Flash disaggregation; Lambda's; Monolithics; Performance; QoS scheduling; Serverless computing; Service level objective; Storage systems; Adaptive control systems","Association for Computing Machinery, Inc","ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Groups on Management of Data (SIGMOD); Google; IBM; Microsoft","15th Annual ACM Symposium on Cloud Computing, SoCC 2024","20 November 2024 through 22 November 2024","Redmond","205525","Conference paper","Final","","Scopus","2-s2.0-85215532562"
"Barcelona-Pons D.; García-López P.; Metzler B.","Barcelona-Pons, Daniel (57206657129); García-López, Pedro (24479469800); Metzler, Bernard (7007178968)","57206657129; 24479469800; 7007178968","Glider: Serverless Ephemeral Stateful Near-Data Computation","2023","Middleware 2023 - Proceedings of the 24th ACM/IFIP International Middleware Conference","","","","247","260","13","0","10.1145/3590140.3629119","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179884256&doi=10.1145%2f3590140.3629119&partnerID=40&md5=e7e9a2f3120dc29e66fb017bdadecc2d","Serverless data analytics generate a large amount of intermediate data during computation stages. However, serverless functions, which are short-lived and lack direct communication, face significant challenges in managing this data effectively. The traditional approach of using object storage to carry the data proves to be slow and costly, as it involves constant movement of data back and forth. Although specialized ephemeral storage solutions have been developed to address this issue, they fail to tackle the fundamental challenge of minimizing data movements. This work focuses on incorporating near-data computation into an ephemeral storage system to reduce the volume of transferred data in serverless analytics. We present Glider with the aim to enhance communication between serverless compute stages, allowing data to smoothly ""glide""through the processing pipeline instead of bouncing between different services. Glider achieves this by leveraging stateful near-data execution of complex data-bound operations and an efficient I/O streaming interface. Under evaluation, it reduces data transfers by up to 99.7%, improves storage utilization by up to 99.8%, and enhances performance by up to 2.7×. In sum, Glider improves serverless data analytics by optimizing data movement, streamlining processing, and avoiding redundant transfers.  © 2023 ACM.","cloud; ephemeral; intermediate data; near-data; Serverless; stateful","Cloud analytics; Data transfer; Digital storage; Interface states; Data analytics; Data computation; Data movements; Direct communications; Ephemeral; Intermediate data; Large amounts; Near-data; Serverless; Stateful; Data Analytics","Association for Computing Machinery, Inc","","24th ACM/IFIP International Middleware Conference, Middleware 2023","11 December 2023 through 15 December 2023","Bologna","194765","Conference paper","Final","","Scopus","2-s2.0-85179884256"
"Diao Y.; Horn D.; Kipf A.; Shchur O.; Benito I.; Dong W.; Pagano D.; Pfeil P.; Nathan V.; Narayanaswamy B.; Kraska T.","Diao, Yanlei (18433629600); Horn, Dominik (57226342905); Kipf, Andreas (57163874800); Shchur, Oleksandr (59904755700); Benito, Ines (59520347000); Dong, Wenjian (59350157000); Pagano, Davide (59179278600); Pfeil, Pascal (57712347700); Nathan, Vikram (57192235882); Narayanaswamy, Balakrishnan (14071642900); Kraska, Tim (25823846800)","18433629600; 57226342905; 57163874800; 59904755700; 59520347000; 59350157000; 59179278600; 57712347700; 57192235882; 14071642900; 25823846800","Forecasting Algorithms for Intelligent Resource Scaling: An Experimental Analysis","2024","SoCC 2024 - Proceedings of the 2024 ACM Symposium on Cloud Computing","","","","126","143","17","0","10.1145/3698038.3698564","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215529201&doi=10.1145%2f3698038.3698564&partnerID=40&md5=76b4f46ca21df1679aef81f879a17413","There has been a growing demand for making modern cloud-based data analytics systems cost-effective and easy to use. AI-powered intelligent resource scaling is one such effort, aiming at automating scaling decisions for serverless offerings like Amazon Redshift Serverless. The foundation of intelligent resource scaling lies in the ability to forecast query workloads and their resource consumption accurately. Although the forecasting problem has been extensively studied across various domains, there is a lack of thorough analysis of existing forecasting algorithms for large-scale, real-world cloud query workloads. This paper fills this gap by providing an in-depth analysis of forecasting algorithms for real-world cloud workloads, covering the fundamental data characteristics that distinguish query workload forecasting from prior problems and evaluating the strengths and limitations of existing algorithms in this new domain. We anticipate that our findings will provide valuable insights in informing the design of an efficient and effective solution for production use, as well as in steering the forecasting community toward more effective algorithms of high real-world impact. © 2024 Owner/Author.","","Structured Query Language; Analytics systems; Cloud-based; Data analytics; Experimental analysis; Forecasting algorithm; Growing demand; Intelligent resource; Real-world; Scalings; System costs; Cloud analytics","Association for Computing Machinery, Inc","ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Groups on Management of Data (SIGMOD); Google; IBM; Microsoft","15th Annual ACM Symposium on Cloud Computing, SoCC 2024","20 November 2024 through 22 November 2024","Redmond","205525","Conference paper","Final","","Scopus","2-s2.0-85215529201"
"Da Silva A.A.; Pablo Hong Enriquez R.; Rattihalli G.; Thurimella V.; Da Silva R.F.; Milojicic D.","Da Silva, Anderson Andrei (57219876361); Pablo Hong Enriquez, Rolando (59546177700); Rattihalli, Gourav (57193494996); Thurimella, Vijay (57221157850); Da Silva, Rafael Ferreira (57203350869); Milojicic, Dejan (6603838893)","57219876361; 59546177700; 57193494996; 57221157850; 57203350869; 6603838893","Enabling HPC Scientific Workflows for Serverless","2024","Proceedings of SC 2024-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis","","","","110","125","15","1","10.1109/SCW63240.2024.00022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217154339&doi=10.1109%2fSCW63240.2024.00022&partnerID=40&md5=2e89d2d1a45a14ca1f2d9274f23d8b81","The convergence of edge computing, big data analytics, and AI with traditional scientific calculations is increasingly being adopted in HPC workflows. Workflow management systems are crucial for managing and orchestrating these complex computational tasks. However, it is difficult to identify patterns within the growing population of HPC workflows. Serverless has emerged as a novel computing paradigm, offering dynamic resource allocation, quick response time, fine-grained resource management and auto-scaling. In this paper, we propose a framework to enable HPC scientific workflows on serverless. Our approach integrates a widely used traditional HPC workflow generator with an HPC serverless workflow management system to create benchmark suites of scientific workflows with diverse characteristics. These workflows can be executed on different serverless platforms. We comprehensively compare executing workflows on traditional local containers and serverless computing platforms. Our results show that serverless can reduce CPU and memory usage respectively by 78.11% and 73.92% without compromising performance.  © 2024 IEEE.","HPC Serverless Workflows; Scientific Workflows; Serverless Computing","","Institute of Electrical and Electronics Engineers Inc.","ACM; ACM�s Special Interest Group on High Performance Computing (SIGHPC); IEEE Technical Committee on High Performance Computing (TCHPC); Institute of Electrical and Electronics Engineers (IEEE) Computer Society","2024 Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC Workshops 2024","17 November 2024 through 22 November 2024","Atlanta","205946","Conference paper","Final","","Scopus","2-s2.0-85217154339"
"Holmes B.; Waterman J.; Williams D.","Holmes, Benjamin (57573371600); Waterman, Jason (23029634400); Williams, Dan (55696458600)","57573371600; 23029634400; 55696458600","SEVeriFast: Minimizing the root of trust for fast startup of SEV microVMs","2024","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","2","","","1045","1060","15","3","10.1145/3620665.3640424","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192168332&doi=10.1145%2f3620665.3640424&partnerID=40&md5=001962a69de6afc4bea8c86abcb6eca9","Serverless computing platforms rely on fast container initialization to provide low latency and high throughput for requests. While hardware enforced trusted execution environments (TEEs) have gained popularity, confidential computing has yet to be widely adopted by latency-sensitive platforms due to its additional initialization overhead. We investigate the application of AMD's Secure Encrypted Virtualization (SEV) to microVMs and find that current startup times for confidential VMs are prohibitively slow due to the high cost of establishing a root of trust for each new VM.We present SEVeriFast, a new bootstrap scheme for SEV VMs that reevaluates current microVM techniques for fast boot, such as eliminating bootstrap stages and bypassing guest kernel decompression. Counter-intuitively, we find that introducing an additional bootstrap component and reintroducing kernel compression optimizes the cold boot performance of SEV microVMs by reducing the cost of measurement on the critical boot path and producing a minimal root of trust. To our knowledge, SEVeriFast is the first work to explore the trade-offs associated with booting confidential microVMs and provide a set of guiding principles as a step toward confidential serverless. We show that SEVeriFast improves cold start performance of SEV VMs over current methods by 86-93%. © 2024 Association for Computing Machinery. All rights reserved.","","'current; Computing platform; Fast start-up; High-throughput; Low latency; Low-high; Performance; Root of trusts; Trusted execution environments; Virtualizations; Economic and social effects","Association for Computing Machinery","ACM Special Interest Group on Computer Architecture (SIGARCH); ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Group on Programming Languages (SIGPLAN)","29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2024","27 April 2024 through 1 May 2024","San Diego","198945","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85192168332"
"Basu Roy R.; Patel T.; Garg R.; Tiwari D.","Basu Roy, Rohan (57219249946); Patel, Tirthak (57200205359); Garg, Rohan (51963581600); Tiwari, Devesh (23467777300)","57219249946; 57200205359; 51963581600; 23467777300","CodeCrunch: Improving Serverless Performance via Function Compression and Cost-Aware Warmup Location Optimization","2024","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","1","","","85","101","16","4","10.1145/3617232.3624866","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191462807&doi=10.1145%2f3617232.3624866&partnerID=40&md5=2f5fba4a622b9b1f006e48d29dd1d5dd","Serverless computing has a critical problem of function cold starts. To minimize cold starts, state-of-the-art techniques predict function invocation times to warm them up. Warmed-up functions occupy space in memory and incur a keep-alive cost, which can become exceedingly prohibitive under bursty load. To address this issue, we design CodeCrunch, which introduces the concept of serverless function compression and exploits server heterogeneity to make serverless computing more efficient, especially under high memory pressure. © 2024 Copyright is held by the owner/author(s). Publication rights licensed to ACM.","function compression; serverless computing","Cold-start; Cold-start state; Cost-aware; Critical problems; Function compression; Keep-alive; Location optimization; Performance; Serverless computing; State-of-the-art techniques","Association for Computing Machinery","ACM Special Interest Group on Computer Architecture (SIGARCH); ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Group on Programming Languages (SIGPLAN)","29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2024","27 April 2024 through 1 May 2024","San Diego","198945","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85191462807"
"Tomaras D.; Buschjäger S.; Kalogeraki V.; Morik K.; Gunopulos D.","Tomaras, Dimitrios (56781461900); Buschjäger, Sebastian (57194760014); Kalogeraki, Vana (6701914099); Morik, Katharina (6701443861); Gunopulos, Dimitrios (6603923918)","56781461900; 57194760014; 6701914099; 6701443861; 6603923918","STRATA: Random Forests going Serverless","2024","Middleware 2024 - Proceedings of the 25th ACM International Middleware Conference","","","","22","35","13","0","10.1145/3652892.3654791","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215519124&doi=10.1145%2f3652892.3654791&partnerID=40&md5=5ff553f9a60b4f037344e833d22fa209","Serverless computing has received growing interest in recent years for supporting large-scale machine learning tasks. However, training a machine learning model in a serverless environment is a nontrivial procedure and several challenges still need to be addressed in the data distribution and result aggregation steps as well as the cost of execution due to the inherent complexity of the distributed computation and the coordination required in the learning algorithm. In this work, we focus on Random Forests, a state-of-the-art technique in many Machine Learning applications. We propose STRATA, a cost-effective middleware to train Random Forests atop a serverless environment that successfully addresses these training challenges. As we show in our extensive experimental evaluation STRATA achieves 3X better training times on average compared to a centralized approach and can withstand up to 70% of failures during training. © 2024 Copyright held by the owner/author(s).","middleware; random forests; serverless","Contrastive Learning; Federated learning; Middleware; Random forests; Data distribution; Distributed computations; Inherent complexity; Large-scale machine learning; Learning tasks; Machine learning applications; Machine learning models; Random forests; Serverless; State-of-the-art techniques; Adversarial machine learning","Association for Computing Machinery, Inc","ACM","25th ACM International Middleware Conference, Middleware 2024","2 December 2024 through 6 December 2024","Hong Kong","205541","Conference paper","Final","","Scopus","2-s2.0-85215519124"
"Jansen M.; Talluri S.; Doekemeijer K.; Tehrany N.; Iosup A.; Trivedi A.","Jansen, Matthijs (57211621736); Talluri, Sacheendra (57200562502); Doekemeijer, Krijn (57719320200); Tehrany, Nick (57760431900); Iosup, Alexandru (23392350500); Trivedi, Animesh (55014115500)","57211621736; 57200562502; 57719320200; 57760431900; 23392350500; 55014115500","Columbo: A Reasoning Framework for Kubernetes' Configuration Space","2025","ICPE 2025 - Proceedings of the 16th ACM/SPEC International Conference on Performance","","","","45","57","12","0","10.1145/3676151.3719374","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007285655&doi=10.1145%2f3676151.3719374&partnerID=40&md5=4439a4ee38d25b38d522407dcb46a9f0","Resource managers such as Kubernetes are rapidly evolving to support low-latency and scalable computing paradigms such as serverless and granular computing. As a result, Kubernetes supports dozens of workload deployment models and exposes roughly 1,600 configuration parameters. Previous work has shown that parameter tuning can significantly improve Kubernetes' performance, but identifying which parameters impact performance and should be tuned remains challenging. To help users optimize their Kubernetes deployments, we present Columbo, an offline reasoning framework to detect and resolve performance bottlenecks using configuration parameters. We study Kubernetes and define its workload deployment pipeline of 6 stages and 26 steps. To detect bottlenecks, Columbo uses an analytical model to predict the best-case deployment time of a workload per pipeline stage and compares it to empirical data from a novel benchmark suite. Columbo then uses a rule-based methodology to recommend parameter updates based on the detected bottleneck, deployed workload, and mapping of configurations to pipeline stages. We demonstrate that Columbo reduces workload deployment time across its benchmark suite by 28% on average and 79% at most. We report a total execution time decrease of 17% for data processing with Spark and up to 20% for serverless workflows with OpenWhisk. Columbo is open-source and available at https://github.com/atlarge-research/continuum/tree/columbo. © 2025 Copyright held by the owner/author(s).","configuration tuning; kubernetes; resource management","Distributed computer systems; Enterprise resource management; Granular computing; Memory management; Open Data; Open systems; Benchmark suites; Configuration parameters; Configuration space; Configuration tuning; Deployment time; Kubernetes; Pipeline stages; Reasoning framework; Resource management; Resource managers; Resource allocation","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC","16th ACM/SPEC International Conference on Performance, ICPE 2025","5 May 2025 through 9 May 2025","Toronto","208715","Conference paper","Final","","Scopus","2-s2.0-105007285655"
"Semjonov A.","Semjonov, Anton (59076662800)","59076662800","Distributed Computation Offloading in Heterogeneous Edge Environments","2024","MIDDLEWARE 2024 - Proceedings of the 25th ACM International Middleware Conference Demos, Posters and Doctoral Symposium, Part of: MIDDLEWARE 2024","","","","31","32","1","0","10.1145/3704440.3704793","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216396641&doi=10.1145%2f3704440.3704793&partnerID=40&md5=20dec1a5efeaa85e8e6715064f408e48","The very edge of the rapidly evolving computing continuum is characterized by a wide variety of different hardware architectures, software platforms, and in general diverse device constraints and capabilities. While in-network computing and smaller distributed data centers continue to receive a lot of attention, idle workstations and consumer devices remain an underappreciated and largely untapped computing resource at the edge. This warrants further exploration. My research focuses on leveraging WebAssembly, a portable binary instruction format, and other Web-standard APIs to design, implement and evaluate a novel middleware, to enable distributed computation offloading and context-aware task placement across this vastly heterogeneous resource pool. By employing concepts of volunteer computing in a new context and obviating complex setups, idle edge devices are transformed into a collaborative computational resource. © 2024 Copyright held by the owner/author(s).","collaborative computing; computation offloading; edge computing; serverless computing; webassembly","Middleware; Computation offloading; Distributed computations; Diverse devices; Edge computing; Hardware architecture; In networks; Network computing; Serverless computing; Software platforms; Webassembly; Computation offloading","Association for Computing Machinery, Inc","","25th ACM International Middleware Conference Demos, Posters and Doctoral Symposium, MIDDLEWARE 2024","2 December 2024 through 6 December 2024","Hong Kong","205814","Conference paper","Final","","Scopus","2-s2.0-85216396641"
"","","","HPDC 2024 - Proceedings of the 33rd International Symposium on High-Performance Parallel and Distributed Computing","2024","HPDC 2024 - Proceedings of the 33rd International Symposium on High-Performance Parallel and Distributed Computing","","","","","","426","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204976370&partnerID=40&md5=7ad558382bf05efc34378c0e4508541c","The proceedings contain 47 papers. The topics discussed include: ElasticRoom: multi-tenant DNN inference engine via co-design with resource-constrained compilation and strong priority scheduling; efficient all-to-all collective communication schedules for direct-connect topologies; ESG: pipeline-conscious efficient scheduling of DNN workflows on serverless platforms with shareable GPUs; ETS: deep learning training iteration time prediction based on execution trace sliding window; IDT: intelligent data placement for multi-tiered main memory with reinforcement learning; FaaSKeeper: learning from building serverless services with ZooKeeper as an example; accelerating function-centric applications by discovering, distributing, and retaining reusable context in workflow systems; and Faast: an efficient serverless framework made snapshot-based function response fast.","","","Association for Computing Machinery, Inc","ACM SIGARCH; CORNELIS NETWORKS; E4 COMPUTER ENGINEERING; NEC Laboratories America; VAST","33rd International Symposium on High-Performance Parallel and Distributed Computing, HPDC 2024","3 June 2024 through 7 June 2024","Pisa","202382","Conference review","Final","","Scopus","2-s2.0-85204976370"
"Liu Y.; Sun S.; Li Z.; Chen Q.; Gao S.; He B.; Li C.; Guo M.","Liu, Yushi (57489093300); Sun, Shixuan (57204178996); Li, Zijun (57278389100); Chen, Quan (36623232500); Gao, Sen (57853983800); He, Bingsheng (7402047189); Li, Chao (56697637700); Guo, Minyi (7201564780)","57489093300; 57204178996; 57278389100; 36623232500; 57853983800; 7402047189; 56697637700; 7201564780","FaaSGraph: Enabling Scalable, Efficient, and Cost-Effective Graph Processing with Serverless Computing","2024","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","2","","","385","400","15","3","10.1145/3620665.3640361","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192178817&doi=10.1145%2f3620665.3640361&partnerID=40&md5=f4436e04ea8d9e9f054793ef962fec6f","Graph processing is widely used in cloud services; however, current frameworks face challenges in efficiency and cost-effectiveness when deployed under the Infrastructure-as-a-Service model due to its limited elasticity. In this paper, we present FaaSGraph, a serverless-native graph computing scheme that enables efficient and economical graph processing through the co-design of graph processing frameworks and serverless computing systems. Specifically, we design a data-centric serverless execution model to efficiently power heavy computing tasks. Furthermore, we carefully design a graph processing paradigm to seamlessly cooperate with the data-centric model. Our experiments show that FaaS-Graph improves end-to-end performance by up to 8.3X and reduces memory usage by up to 52.4% compared to state-of-the-art IaaS-based methods. Moreover, FaaSGraph delivers steady 99%-ile performance in highly fluctuated workloads and reduces monetary cost by 85.7%.  © 2024 Copyright is held by the owner/author(s). Publication rights licensed to ACM.","cold start; graph processing; resource sharing; serverless computing","Infrastructure as a service (IaaS); 'current; Cloud services; Cold-start; Computing scheme; Cost effective; Data centric; Graph processing; Resources sharing; Serverless computing; Service modeling; Cost effectiveness","Association for Computing Machinery","ACM Special Interest Group on Computer Architecture (SIGARCH); ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Group on Programming Languages (SIGPLAN)","29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2024","27 April 2024 through 1 May 2024","San Diego","198945","Conference paper","Final","","Scopus","2-s2.0-85192178817"
"Xu C.; Liu Y.; Li Z.; Chen Q.; Zhao H.; Zeng D.; Peng Q.; Wu X.; Zhao H.; Fu S.; Guo M.","Xu, Chuhao (57949219100); Liu, Yiyu (59034942100); Li, Zijun (57278389100); Chen, Quan (36623232500); Zhao, Han (57221191601); Zeng, Deze (24722168400); Peng, Qian (57991583200); Wu, Xueqi (59031541100); Zhao, Haifeng (59032909100); Fu, Senbo (58323876900); Guo, Minyi (7201564780)","57949219100; 59034942100; 57278389100; 36623232500; 57221191601; 24722168400; 57991583200; 59031541100; 59032909100; 58323876900; 7201564780","FaaSMem: Improving Memory Efficiency of Serverless Computing with Memory Pool Architecture","2024","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","3","","","331","348","17","6","10.1145/3620666.3651355","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192191592&doi=10.1145%2f3620666.3651355&partnerID=40&md5=cfcd175e233ed523cac6807bb5da55da","In serverless computing, an idle container is not recycled directly, in order to mitigate time-consuming cold container startup. These idle containers still occupy the memory, exasperating the memory shortage of today’s data centers. By offloading their cold memory to remote memory pool could potentially resolve this problem. However, existing offloading policies either hurt the Quality of Service (QoS) or are too coarse-grained in serverless computing scenarios. We therefore propose FaaSMem, a dedicated memory offloading mechanism tailored for serverless computing with memory poor architecture. It is proposed based on our finding that the memory of a serverless container allocated in different s tages h as d ifferent us age pa tterns. Specifically, FaaSMem proposes Page Bucket (Pucket) to segregate the memory pages in different segments, and applies segment-wise offloading policies for them. FaaSMem also proposes a semi-warm period during keep-alive stage, to seek a sweet spot between the offloading effort and the remote access penalty. Experimental results show that FaaSMem reduces the average local memory footprint by 9.9% - 79.8% and improves the container deployment density to 108% - 218%, with negligible 95%-ile latency increase. © 2024 Copyright held by the owner/author(s).","FaaS; Memory Offloading; Memory Pool architecture; Serverless Computing","Lakes; Memory architecture; Quality of service; Datacenter; Faas; Memory efficiency; Memory offloading; Memory pool; Memory pool architecture; Memory shortage; Quality-of-service; Remote memory; Serverless computing; Containers","Association for Computing Machinery","ACM Special Interest Group on Computer Architecture (SIGARCH); ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Group on Programming Languages (SIGPLAN)","29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2024","27 April 2024 through 1 May 2024","San Diego","198945","Conference paper","Final","","Scopus","2-s2.0-85192191592"
"Lu C.; Xu H.; Li Y.; Chen W.; Ye K.; Xu C.","Lu, Chengzhi (57202307477); Xu, Huanle (56081119100); Li, Yudan (59513465500); Chen, Wenyan (57207952267); Ye, Kejiang (35957654600); Xu, Chengzhong (55600419500)","57202307477; 56081119100; 59513465500; 57207952267; 35957654600; 55600419500","SMIless: Serving DAG-based Inference with Dynamic Invocations under Serverless Computing","2024","International Conference for High Performance Computing, Networking, Storage and Analysis, SC","","","","","","","0","10.1109/SC41406.2024.00044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214975837&doi=10.1109%2fSC41406.2024.00044&partnerID=40&md5=4d3a7348c7fb35b3f2cb5bb402695d7c","The deployment of ML serving applications, featuring multiple inference functions on serverless platforms, has gained substantial popularity, leading to numerous developments of new systems. However, these systems often focus on optimizing resource provisioning and cold start management separately, ultimately resulting in higher monetary costs. This paper introduces SMIless, a highly efficient serverless system tailored for serving DAG-based ML inference in heterogeneous environments. SMIless effectively co-optimizes resource configuration and cold-start management in the context of dynamic invocations. This is achieved by seamlessly integrating adaptive pre-warming windows, striking an effective balance between performance and cost. We have implemented SMIless on top of OpenFaaS and conducted extensive evaluations using real-world ML serving applications. The experimental results demonstrate that SMIless can achieve up to a 5.73 × reduction in the overall costs while meeting the SLA requirements for all user requests, surpassing the performance of state-of-the-art solutions. © 2024 IEEE.","DAG-based Inference; Dynamic Invocation; Serverless","Co-optimize; Cold-start; DAG-based inference; Dynamic invocation; Heterogeneous environments; Inference functions; Monetary costs; Performance; Serverless; Serverless systems; Resource allocation","IEEE Computer Society","ACM; ACM�s Special Interest Group on High Performance Computing (SIGHPC); IEEE Technical Committee on High Performance Computing (TCHPC); Institute of Electrical and Electronics Engineers (IEEE) Computer Society","2024 International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2024","17 November 2024 through 22 November 2024","Atlanta","205531","Conference paper","Final","","Scopus","2-s2.0-85214975837"
"Parola F.; Qi S.; Narappa A.B.; Ramakrishnan K.K.; Risso F.","Parola, Federico (57226610314); Qi, Shixiong (57219267120); Narappa, Anvaya B. (58908763200); Ramakrishnan, K.K. (7101600362); Risso, Fulvio (7003461626)","57226610314; 57219267120; 58908763200; 7101600362; 7003461626","SURE: Secure Unikernels Make Serverless Computing Rapid and Efficient","2024","SoCC 2024 - Proceedings of the 2024 ACM Symposium on Cloud Computing","","","","668","688","20","0","10.1145/3698038.3698558","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215503181&doi=10.1145%2f3698038.3698558&partnerID=40&md5=f5c16663fdb5f1eee1b9dcc96c25d67c","Current serverless platforms introduce non-trivial overheads when chaining and orchestrating loosely-coupled microservices. Containerized function runtimes are also constrained by insufficient isolation and excessive startup time. This motivates our exploration of a more efficient, secure, and rapid serverless design. We describe SURE, a unikernel-based serverless framework for fast function startup, equipped with a high-performance and secure data plane. SURE's data plane supports distributed zero-copy communication via the seamless interaction between zero-copy protocol stack (Z-stack) and local shared memory processing. To establish a lightweight service mesh, SURE uses library-based sidecars instead of individual userspace sidecars. We leverage Intel's Memory Protection Keys (MPK) as a lightweight capability to ensure safe access to the shared memory data plane. It also isolates the Trusted Computing Base (TCB) components in SURE's function runtime (e.g., library-based sidecar, scheduler, etc) from untrusted user code, while preserving the efficient single-address-space nature of unikernels. In particular, SURE prevents unintended privilege escalation involving MPK with an enhanced TCB. These combined efforts create a more secure and robust data plane while improving throughput up to 79X over Knative, a representative open-source serverless platform. © 2024 Owner/Author.","Isolation; MPK; Serverless; Shared memory; Unikernel","Memory architecture; Data planes; Data-plane; Isolation; Memory protection; Memory protection key; Runtimes; Serverless; Shared memory; Unikernel; Zero copy; Trusted computing","Association for Computing Machinery, Inc","ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Groups on Management of Data (SIGMOD); Google; IBM; Microsoft","15th Annual ACM Symposium on Cloud Computing, SoCC 2024","20 November 2024 through 22 November 2024","Redmond","205525","Conference paper","Final","","Scopus","2-s2.0-85215503181"
"Nardelli M.; Russo G.R.","Nardelli, Matteo (57171962700); Russo, Gabriele Russo (57211560923)","57171962700; 57211560923","Function Offloading and Data Migration for Stateful Serverless Edge Computing","2024","ICPE 2024 - Proceedings of the 15th ACM/SPEC International Conference on Performance Engineering","","","","247","257","10","4","10.1145/3629526.3649293","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193794086&doi=10.1145%2f3629526.3649293&partnerID=40&md5=63ea5fe2f90372f897625956f4c41aab","Serverless computing and, in particular, Function-as-a-Service (FaaS) have emerged as valuable paradigms to deploy applications without the burden of managing the computing infrastructure. While initially limited to the execution of stateless functions in the cloud, serverless computing is steadily evolving. The paradigm has been increasingly adopted at the edge of the network to support latencysensitive services. Moreover, it is not limited to stateless applications, with functions often recurring to external data stores to exchange partial computation outcomes or to persist their internal state. To the best of our knowledge, several policies to schedule function instances to distributed hosts have been proposed, but they do not explicitly model the data dependency of functions and its impact on performance. In this paper, we study the allocation of functions and associated key-value state in geographically distributed environments. Our contribution is twofold. First, we design a heuristic for function offloading that satisfies performance requirements. Then, we formulate the state migration problem via Integer Linear Programming, taking into account the heterogeneity of data, its access patterns by functions, and the network resources. Extensive simulations demonstrate that our policies allow FaaS providers to effectively support stateful functions and also lead to improved response times.  © 2024 Copyright held by the owner/author(s).","cloud computing; data migration; edge computing; scheduling; serverless","Computation offloading; Cloud-computing; Computing infrastructures; Data dependencies; Data store; Data-migration; Edge computing; Internal state; Partial computation; Performance; Serverless; Integer programming","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC","15th ACM/SPEC International Conference on Performance Engineering, ICPE 2024","7 May 2024 through 11 May 2024","London","199383","Conference paper","Final","","Scopus","2-s2.0-85193794086"
"Fokaefs M.-E.; Oliveira F.; Ezzati-Jivan N.","Fokaefs, Marios-Eleftherios (24469811300); Oliveira, Filipe (59139908500); Ezzati-Jivan, Naser (55659743600)","24469811300; 59139908500; 55659743600","12th International Workshop on Load Testing and Benchmarking of Software Systems: LTB'24 Chairs' Welcome","2024","ICPE 2024 - Companion of the 15th ACM/SPEC International Conference on Performance Engineering","","","","226","","","0","10.1145/3629527.3651437","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193996825&doi=10.1145%2f3629527.3651437&partnerID=40&md5=40bac17152f4b242b2eefe9136a65bbc","It is our great pleasure to welcome you to the twelfth edition of the International Workshop on Load Testing and Benchmarking of Software Systems - LTB 2024, (https://ltb2024.github.io/). This one-day workshop brings together software testing and software performance researchers, practitioners, and tool developers to discuss the challenges and opportunities of conducting research on load testing and benchmarking software systems, including theory, applications, and experiences. LTB 2024 includes 2 keynote talks, 4 research papers, and 2 industry presentations. The topics cover performance of serverless computing, performance and load testing, performance-driven culture, workload generation, workload tracing, benchmarking, and performance verification.  © 2024 Copyright held by the owner/author(s).","","Application programs; Benchmarking; Computation theory; Software testing; Computing load; Computing performance; International workshops; Performance; Research papers; Software performance; Software testings; Software-systems; Testing performance; Theory applications; Load testing","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC Reaserch","15th ACM/SPEC International Conference on Performance Engineering, ICPE 2024","7 May 2024 through 11 May 2024","London","199384","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85193996825"
"Pfandzelter T.","Pfandzelter, Tobias (57208737730)","57208737730","Serverless Abstractions for Edge Computing in Large Low-Earth Orbit Satellite Networks","2023","Middleware Demos, Posters and Doctoral Symposium 2023: Proceedings of the 24th International Middleware Conference Demos, Posters and Doctoral Symposium, Part of: Middleware 2023","","","","3","6","3","1","10.1145/3626564.3629088","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180131155&doi=10.1145%2f3626564.3629088&partnerID=40&md5=d968eec38981a0ca402c9ded3c903aef","Private and public actors are building massive LEO satellite communication networks. Researchers have proposed extending edge computing to satellites for global low-latency access to application services for, e.g., IoT or metaverses. Building applications for this LEO edge means managing services at large scale on highly dynamic infrastructure, in addition to the usual constraints of edge computing. We seek to develop serverless abstractions for LEO edge applications. We introduce virtual testbed tooling that allows researchers, students, and practitioners to become familiar with the unique characteristics of the LEO edge and develop, test, and benchmark real software in a cost-efficient manner. Further, we develop abstractions for state and data management in geo-distributed edge-to-cloud environments. We then integrate these abstractions with a lightweight FaaS platform to allow building stateful yet scalable applications on the LEO edge. Finally, we propose applications for LEO edge computing to guide the evaluation of our design. © 2023 ACM.","edge computing; LEO satellite networks; serverless","Abstracting; Information management; Orbits; Satellite communication systems; Satellites; Software testing; Application services; Edge computing; LEO satellite networks; Leo satellites communications; Low earth orbit satellites; Low latency; Metaverses; Satellite communication networks; Satellite network; Serverless; Edge computing","Association for Computing Machinery, Inc","ACM","24th International Middleware Conference Demos, Posters and Doctoral Symposium, Middleware 2023","11 December 2023 through 15 December 2023","Bologna","194866","Conference paper","Final","","Scopus","2-s2.0-85180131155"
"Mahapatra R.; Ghodrati S.; Ahn B.H.; Kinzer S.; Wang S.-T.; Xu H.; Karthikeyan L.; Sharma H.; Yazdanbakhsh A.; Alian M.; Esmaeilzadeh H.","Mahapatra, Rohan (57113174300); Ghodrati, Soroush (57204894123); Ahn, Byung Hoon (57219505143); Kinzer, Sean (57219506885); Wang, Shu-Ting (57838593600); Xu, Hanyang (57226284190); Karthikeyan, Lavanya (57205462967); Sharma, Hardik (56270881400); Yazdanbakhsh, Amir (36184859000); Alian, Mohammad (57190014688); Esmaeilzadeh, Hadi (6506771295)","57113174300; 57204894123; 57219505143; 57219506885; 57838593600; 57226284190; 57205462967; 56270881400; 36184859000; 57190014688; 6506771295","In-Storage Domain-Specific Acceleration for Serverless Computing","2024","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","2","","","530","548","18","2","10.1145/3620665.3640413","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192208327&doi=10.1145%2f3620665.3640413&partnerID=40&md5=fe5fe84eae3455cd47670dc5e224586a","While (I) serverless computing is emerging as a popular form of cloud execution, datacenters are going through major changes: (II) storage dissaggregation in the system infrastructure level and (III) integration of domain-specific accelerators in the hardware level. Each of these three trends individually provide significant benefits; however, when combined the benefits diminish. On the convergence of these trends, the paper makes the observation that for serverless functions, the overhead of accessing dissaggregated storage overshadows the gains from accelerators. Therefore, to benefit from all these trends in conjunction, we propose In-Storage Domain-Specific Acceleration for Serverless Computing (dubbed DSCS-Serverless1). The idea contributes a server-less model that utilizes a programmable accelerator embedded within computational storage to unlock the potential of acceleration in disaggregated datacenters. Our results with eight applications show that integrating a comparatively small accelerator within the storage (DSCS-Serverless) that fits within the storage's power constraints (25 Watts), significantly outperforms a traditional disaggregated system that utilizes NVIDIA RTX 2080 Ti GPU (250 Watts). Further, the work highlights that disaggregation, serverless model, and the limited power budget for computation in storage device require a different design than the conventional practices of integrating microprocessors and FPGAs. This insight is in contrast with current practices of designing computational storage devices that are yet to address the challenges associated with the shifts in datacenters. In comparison with two such conventional designs that use ARM cores or a Xilinx FPGA, DSCS-Serverless provides 3.7× and 1.7× end-to-end application speedup, 4.3× and 1.9× energy reduction, and 3.2× and 2.3× better cost efficiency, respectively. © 2024 Association for Computing Machinery. All rights reserved.","accelerator; computational storage drive (CSD); deep neural network (DNN); disaggregated datacenter; domain specific architecture (DSA); in-storage acceleration; large language model (LLM); neural processing unit (NPU); serverless computing; serverless function; storage systems","Budget control; Computing power; Field programmable gate arrays (FPGA); Integrated circuit design; Virtual storage; Computational storage drive; Datacenter; Deep neural network; Disaggregated datacenter; Domain specific architecture; Domain specific architectures; In-storage acceleration; Language model; Large language model; Neural processing unit; Neural-processing; Processing units; Serverless computing; Serverless function; Storage drives; Storage systems; Deep neural networks","Association for Computing Machinery","ACM Special Interest Group on Computer Architecture (SIGARCH); ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Group on Programming Languages (SIGPLAN)","29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2024","27 April 2024 through 1 May 2024","San Diego","198945","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85192208327"
"Phung T.S.; Thomas C.; Ward L.; Chard K.; Thain D.","Phung, Thanh Son (57450908900); Thomas, Colin (58727979500); Ward, Logan (55556371700); Chard, Kyle (9132950200); Thain, Douglas (8900976600)","57450908900; 58727979500; 55556371700; 9132950200; 8900976600","Accelerating Function-Centric Applications by Discovering, Distributing, and Retaining Reusable Context in Workflow Systems","2024","HPDC 2024 - Proceedings of the 33rd International Symposium on High-Performance Parallel and Distributed Computing","","","","122","134","12","0","10.1145/3625549.3658663","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204938269&doi=10.1145%2f3625549.3658663&partnerID=40&md5=369c7aaec015b2880ec4b7d68c6aa776","Workflow systems provide a convenient way for users to write large-scale applications by composing independent tasks into large graphs that can be executed concurrently on high-performance clusters. In many newer workflow systems, tasks are often expressed as a combination of function invocations in a high-level language. Because necessary code and data are not statically known prior to execution, they must be moved into the cluster at runtime. An obvious way of doing this is to translate function invocations into self-contained executable programs and run them as usual, but this brings a hefty performance penalty: a function invocation now needs to piggyback its context with extra code and data to a remote node, and the remote node needs to take extra time to reconstruct the invocation's context before executing it, both detrimental to lightweight short-running functions.A better solution for workflow systems is to treat functions and invocations as first-class abstractions: subsequent invocations of the same function on a worker node should only pay for the cost of context setup once and reuse the context between different invocations. The remaining problems lie in discovering, distributing, and retaining the reusable context among workers. In this paper, we discuss the rationale and design requirement of these mechanisms to support context reuse, and implement them in TaskVine, a data-intensive distributed framework and execution engine. Our results from executing a large-scale neural network inference application and a molecular design application show that treating functions and invocations as first-class abstractions reduces the execution time of the applications by 94.5% and 26.9%, respectively. © 2024 held by the owner/author(s).","burst buffers; distributed storage; serverless computing; workflow systems","Abstract data types; Artificial intelligence; Buffer storage; Computer software reusability; Integrated circuit design; Problem oriented languages; Program debugging; Burst buffer; Class abstraction; Distributed storage; Independent tasks; Large graphs; Large-scale applications; Remote node; Reuse; Serverless computing; Work-flow systems; Reusability","Association for Computing Machinery, Inc","ACM SIGARCH; CORNELIS NETWORKS; E4 COMPUTER ENGINEERING; NEC Laboratories America; VAST","33rd International Symposium on High-Performance Parallel and Distributed Computing, HPDC 2024","3 June 2024 through 7 June 2024","Pisa","202382","Conference paper","Final","","Scopus","2-s2.0-85204938269"
"Cicconetti C.; Carlini E.; Hetzel R.; Mortier R.; Paradell A.; Sauer M.","Cicconetti, Claudio (22033885800); Carlini, Emanuele (35316758800); Hetzel, Raphael (57226165104); Mortier, Richard (6701782778); Paradell, Antonio (57204900609); Sauer, Markus (59075600600)","22033885800; 35316758800; 57226165104; 6701782778; 57204900609; 59075600600","EDGELESS: A Software Architecture for Stateful FaaS at the Edge","2024","HPDC 2024 - Proceedings of the 33rd International Symposium on High-Performance Parallel and Distributed Computing","","","","393","396","3","0","10.1145/3625549.3658817","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204935794&doi=10.1145%2f3625549.3658817&partnerID=40&md5=b62c3fee2b08475a304386958c49090f","EDGELESS is a serverless platform targeting edge computing that supports widely distributed deployments using heterogeneous devices. We present its components, architecture, and programming model. Our working prototype of EDGELESS enables executing lightweight functions and is already available as open-source. © 2024 held by the owner/author(s).","cloud-continuum; edge computing; resource-constrained devices; serverless computing","Cloud computing architecture; Mobile edge computing; Architecture modeling; Cloud-continuum; Component architectures; Component modeling; Component programming; Distributed deployment; Edge computing; Heterogeneous devices; Resourceconstrained devices; Serverless computing; Cloud platforms","Association for Computing Machinery, Inc","ACM SIGARCH; CORNELIS NETWORKS; E4 COMPUTER ENGINEERING; NEC Laboratories America; VAST","33rd International Symposium on High-Performance Parallel and Distributed Computing, HPDC 2024","3 June 2024 through 7 June 2024","Pisa","202382","Conference paper","Final","","Scopus","2-s2.0-85204935794"
"Pfandzelter T.; Bermbach D.","Pfandzelter, Tobias (57208737730); Bermbach, David (51461094200)","57208737730; 51461094200","Enoki: Stateful Distributed FaaS from Edge to Cloud","2023","MiddleWEdge 2023 - Proceedings of the 2nd International Workshop on Middleware for the Edge, Part of: ACM/IFIP Middleware 2023","","","","19","24","5","4","10.1145/3630180.3631203","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180123915&doi=10.1145%2f3630180.3631203&partnerID=40&md5=ba68849ffabbee9eaa1f8f6c8dc378b4","Function-as-a-Service (FaaS) is a promising paradigm for applications distributed across the edge-cloud continuum. FaaS functions are stateless by nature, leading to high elasticity and transparent invocation. Supporting stateful applications, however, requires integrating data storage in FaaS, which is not trivial in an edge-cloud environment. We propose Enoki, an architecture for stateful FaaS computing replicated across the edge-cloud continuum. Enoki integrates a replicated key-value store with single-node FaaS systems at edge and cloud nodes in order to provide low-latency local data access for functions without breaking the abstraction of the FaaS programming model. We evaluate Enoki with microbenchmarks on an open-source prototype and demonstrate building a stateful FaaS application with multiple functions distributed over edge and cloud. © 2023 ACM.","edge computing; FaaS; serverless","Edge computing; Open source software; Cloud environments; Data storage; Distributed function; Edge clouds; Edge computing; Function-as-a-service; Serverless; Service computing; Service functions; Transparent invocation; Digital storage","Association for Computing Machinery, Inc","ACM","2nd International Workshop on Middleware for the Edge, MiddleWEdge 2023, co-located with ACM Middleware 2023","11 December 2023","Bologna","194762","Conference paper","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85180123915"
"Hui X.; Xu Y.; Guo Z.; Shen X.","Hui, Xinning (57169771200); Xu, Yuanchao (54394647300); Guo, Zhishan (34771412500); Shen, Xipeng (7402721676)","57169771200; 54394647300; 34771412500; 7402721676","ESG: Pipeline-Conscious Efficient Scheduling of DNN Workflows on Serverless Platforms with Shareable GPUs","2024","HPDC 2024 - Proceedings of the 33rd International Symposium on High-Performance Parallel and Distributed Computing","","","","42","55","13","3","10.1145/3625549.3658657","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204934442&doi=10.1145%2f3625549.3658657&partnerID=40&md5=78ceda33120e96a37ddb19f10fd5d758","Recent years have witnessed increasing interest in machine learning inferences on serverless computing for its auto-scaling and cost effective properties. Existing serverless computing, however, lacks effective job scheduling methods to handle the schedule space dramatically expanded by GPU sharing, task batching, and intertask relations. Prior solutions have dodged the issue by neglecting some important factors, leaving some large performance potential locked. This paper presents ESG, a new scheduling algorithm that directly addresses the difficulties. ESG treats sharable GPU as a first-order factor in scheduling. It employs an optimality-guided adaptive method by combining A∗-search and a novel dual-blade pruning to dramatically prune the scheduling space without compromising the quality. It further introduces a novel method, dominator-based SLO distribution, to ensure the scalability of the scheduler. The results show that ESG can significantly improve the SLO hit rates (61%-80%) while saving 47%-187% costs over prior work. © 2024 held by the owner/author(s).","cloud computing; deep learning; function-as-a-service; machine learning for systems; quality of service; resource allocation; resource efficiency; resource management; serverless computing","Cloud platforms; Computer graphics equipment; Deep learning; Resource allocation; Cloud-computing; Deep learning; Function-as-a-service; Machine learning for system; Machine-learning; Quality-of-service; Resource efficiencies; Resource management; Resources allocation; Serverless computing; Scheduling algorithms","Association for Computing Machinery, Inc","ACM SIGARCH; CORNELIS NETWORKS; E4 COMPUTER ENGINEERING; NEC Laboratories America; VAST","33rd International Symposium on High-Performance Parallel and Distributed Computing, HPDC 2024","3 June 2024 through 7 June 2024","Pisa","202382","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85204934442"
"Kimovski D.; Toczé K.; Herbst N.; De Matteis T.","Kimovski, Dragi (55364582500); Toczé, Klervie (57189904776); Herbst, Nikolas (55747080400); De Matteis, Tiziano (55634576700)","55364582500; 57189904776; 55747080400; 55634576700","The Seventh Workshop on Hot Topics in Cloud Computing Performance (HotCloudPerf-2024)","2024","ICPE 2024 - Companion of the 15th ACM/SPEC International Conference on Performance Engineering","","","","163","164","1","0","10.1145/3629527.3651415","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193955791&doi=10.1145%2f3629527.3651415&partnerID=40&md5=1631d8fd17a1572d4bc9b0abe9b60cd5","It gives us immense pleasure to extend a warm welcome to you for the 2024 edition of the Workshop on Hot Topics in Cloud Computing Performance - HotCloudPerf 2024. Cloud computing represents one of the most significant transformations in the realm of IT infrastructure and usage. The adoption of global services within public clouds is on the rise, and the immensely lucrative global cloud market already sustains over 1 million IT-related jobs. However, optimizing the performance and efficiency of the IT services provided by both public and private clouds remains a considerable challenge. Emerging architectures, techniques, and real-world systems entail interactions with the computing continuum, serverless operation, everything as a service, complex workflows, auto-scaling and -tiering, etc. The extent to which traditional performance engineering, software engineering, and system design and analysis tools can contribute to understanding and engineering these emerging technologies is uncertain. The community requires practical tools and robust methodologies to address the hot topics in cloud computing performance effectively.  © 2024 Copyright held by the owner/author(s).","cloud performance; performance modelling","Software engineering; Cloud markets; Cloud performance; Cloud-computing; Computing performance; Global clouds; Global services; Hot topics; IT infrastructures; Performance Modeling; Public clouds; Cloud computing","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC Reaserch","15th ACM/SPEC International Conference on Performance Engineering, ICPE 2024","7 May 2024 through 11 May 2024","London","199384","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85193955791"
"Pfandzelter T.; Bermbach D.","Pfandzelter, Tobias (57208737730); Bermbach, David (51461094200)","57208737730; 51461094200","Komet: A Serverless Platform for Low-Earth Orbit Edge Services","2024","SoCC 2024 - Proceedings of the 2024 ACM Symposium on Cloud Computing","","","","866","882","16","1","10.1145/3698038.3698517","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215504137&doi=10.1145%2f3698038.3698517&partnerID=40&md5=4728fec57ed0a86304c32776d343ab76","Low-Earth orbit satellite networks can provide global broadband Internet access using constellations of thousands of satellites. Integrating edge computing resources in such networks can enable global low-latency access to compute services, supporting end users in rural areas, remote industrial applications, or the IoT. To achieve this, resources must be carefully allocated to various services from multiple tenants. Moreover, applications must navigate the dynamic nature of satellite networks, where orbital mechanics necessitate frequent client hand-offs. Therefore, managing applications on the low-Earth orbit edge will require the right platform abstractions. We introduce Komet, a serverless platform for low-Earth orbit edge computing. Komet integrates Function-as-a-Service compute with data replication, enabling on-demand elastic edge resource allocation and frequent service migration against satellite orbital trajectories to keep services deployed in the same geographic region. We implement Komet as a proof-of-concept prototype and demonstrate how its abstractions can be used to build low-Earth orbit edge applications with high availability despite constant mobility. Further, we propose simple heuristics for service migration scheduling in different application scenarios and evaluate them in simulation based on our experiment traces, showing the trade-off between selecting an optimal satellite server at every instance and minimizing service migration frequency. © 2024 Owner/Author.","edge computing; satellite networks; serverless computing","Edge computing; Frequency allocation; Geodetic satellites; Rural areas; Satellite communication systems; Tropics; Broadband internet access; Computing resource; Earth orbits; Edge computing; Edge services; Low earth orbit satellites; Low latency; Satellite network; Serverless computing; Service migration; Resource allocation","Association for Computing Machinery, Inc","ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Groups on Management of Data (SIGMOD); Google; IBM; Microsoft","15th Annual ACM Symposium on Cloud Computing, SoCC 2024","20 November 2024 through 22 November 2024","Redmond","205525","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85215504137"
"Zeng S.; Xie M.; Gao S.; Chen Y.; Lu Y.","Zeng, Shaoxun (58774960800); Xie, Minhui (57222343209); Gao, Shiwei (59401514900); Chen, Youmin (57208325103); Lu, Youyou (35759344800)","58774960800; 57222343209; 59401514900; 57208325103; 35759344800","Medusa: Accelerating Serverless LLM Inference with Materialization","2025","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","1","","","653","668","15","0","10.1145/3669940.3707285","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002392331&doi=10.1145%2f3669940.3707285&partnerID=40&md5=3b77a2fd8c62effac60d21dfbcfb004b","Serverless is a promising paradigm to provide scalable, cost-efficient, and easy-to-use model inference services. However, the cold start of model inference functions requires loading models to the devices, which incurs high latencies and undermines the benefits of serverless computing. In LLMs, things get even worse since two extra stages are introduced: a KV cache initialization stage that profiles and anticipates memory reservation for KV cache, and a capturing stage which dynamically constructs CUDA graphs for different batch sizes. Both stages are paramount to the inference performance, but become the main culprit of cold start latency. This paper proposes Medusa to mitigate the long cold start latency through state materialization. Instead of dynamic profiling and construction in the runtime, Medusa materializes the CUDA graphs as well as the information needed by the KV cache initialization in the offline phase, and restores them efficiently in the online phase. Medusa further introduces two novel techniques - offline-online cooperated parameters restoration and triggering-kernels enhanced kernel address restoration - to tackle non-deterministic issues in CUDA graphs. Medusa successfully materializes and restores CUDA graphs across 10 models (with a total of 139364 CUDA graph nodes), and reduces the latency of model loading by 42.5%. Under real-world LLM inference workloads, Medusa reduces the tail latency of the time to first token (TTFT) by 53.0%.  © 2025 ACM.","llm; machine learning system; serverless computing","Cache memory; Cold-start; Cost-efficient; Inference functions; Llm; Machine learning systems; Medusae; Model inference; Offline; Serverless computing; Use-model; Restoration","Association for Computing Machinery","ACM SIGARCH; ACM SIGOPS; ACM SIGPLAN","30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2025","30 March 2025 through 3 April 2025","Rotterdam","207994","Conference paper","Final","","Scopus","2-s2.0-105002392331"
"Bhasi V.M.; Sharma A.; Jain R.; Gunasekaran J.R.; Pattnaik A.; Kandemir M.T.; Das C.","Bhasi, Vivek M. (57343857600); Sharma, Aakash (57225853993); Jain, Rishabh (58281671900); Gunasekaran, Jashwant Raj (55440506600); Pattnaik, Ashutosh (57147328100); Kandemir, Mahmut Taylan (35549787100); Das, Chita (7201851990)","57343857600; 57225853993; 58281671900; 55440506600; 57147328100; 35549787100; 7201851990","Towards SLO-Compliant and Cost-Effective Serverless Computing on Emerging GPU Architectures","2024","Middleware 2024 - Proceedings of the 25th ACM International Middleware Conference","","","","211","224","13","1","10.1145/3652892.3700760","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215529068&doi=10.1145%2f3652892.3700760&partnerID=40&md5=9df3da43bd3ece0865caa171d7a617a6","Serverless platforms are supporting an increasing variety of applications (apps). Among these, apps such as Machine Learning (ML) inference serving can benefit significantly from leveraging accelerators like GPUs. Yet, major serverless providers, despite having GPU-equipped servers, do not offer GPU support for their serverless functions. While recent works have attempted to bridge this gap, they are agnostic to the capabilities of new-generation GPUs, thereby, overlooking several performance optimization opportunities. To address this, we leverage unique features of newer NVIDIA GPU architectures (specifically, their Multi-Instance GPU (MIG) and Multi-Process Service (MPS) capabilities) to devise a serverless framework, Protean, that can guarantee a higher degree of Service Level Objective (SLO) compliance than that offered by state-of-the-art works. Moreover, Protean also proposes to host its components on a combination of both on-demand (reliable) VMs and heavily discounted VMs to reduce costs to the end consumer, while offering high service availability. We extensively evaluate Protean using 22 ML inference workloads with real-world traces on an 8×A100 GPU cluster. Our results show that Protean significantly outperforms state-of-the-art works in terms of SLO compliance (up to ∼93% more) and tail latency (up to 82% less), while reducing cost by up to 70%. We also maintain reasonable tail latencies (< 200 ms) for best effort requests. © 2024 Copyright held by the owner/author(s).","GPU; heterogeneous; resource-management; scheduling; serverless; spot instances","Graphics processing unit; Art work; Cost effective; Heterogeneous; Machine-learning; Performance optimizations; Resource management; Serverless; Service level objective; Spot instances; State of the art; Computer graphics equipment","Association for Computing Machinery, Inc","ACM","25th ACM International Middleware Conference, Middleware 2024","2 December 2024 through 6 December 2024","Hong Kong","205541","Conference paper","Final","","Scopus","2-s2.0-85215529068"
"Yang Y.; Du D.; Song H.; Xia Y.","Yang, Yanning (58955147700); Du, Dong (57200438686); Song, Haitao (51964795600); Xia, Yubin (7403027696)","58955147700; 57200438686; 51964795600; 7403027696","On-demand and Parallel Checkpoint/Restore for GPU Applications","2024","SoCC 2024 - Proceedings of the 2024 ACM Symposium on Cloud Computing","","","","415","433","18","1","10.1145/3698038.3698510","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215512396&doi=10.1145%2f3698038.3698510&partnerID=40&md5=22c5a980bd267f3e37d8860532cd0c4f","Leveraging serverless computing for cloud-based machine learning services is on the rise, promising cost-efficiency and flexibility are crucial for ML applications relying on high-performance GPUs and substantial memory. However, despite modern serverless platforms handling diverse devices like GPUs seamlessly on a pay-as-you-go basis, a longstanding challenge remains: startup latency, a well-studied issue when serverless is CPU-centric. For example, initializing GPU apps with minor GPU models, like MobileNet, demands several seconds. For more intricate models such as GPT-2, startup latency can escalate to around 10 seconds, vastly overshadowing the short computation time for GPU-based inference. Prior solutions tailored for CPU serverless setups, like fork() and Checkpoint/Restore, cannot be directly and effectively applied due to differences between CPUs and GPUs. This paper presents gCROP (GPU Checkpoint/Restore made On-demand and Parallel), the first GPU runtime that achieves <100ms startup latency for GPU apps with up to 774 million parameters (3.1GB GPT-2-Large model). The key insight behind gCROP is to selectively restore essential states on demand and in parallel during boot from a prepared checkpoint image. To this end, gCROP first introduces a global service, GPU Restore Server, which can break the existing barrier between restore stages and achieve parallel restore. Besides, gCROP leverages both CPU and GPU page faults, and can on-demand restore both CPU and GPU data with profile-guided order to mitigate costs caused by faults. Moreover, gCROP designs a multi-checkpoint mechanism to increase the common contents among checkpoint images and utilizes deduplication to reduce storage costs. Implementation and evaluations on AMD GPUs show significant improvement in startup latency, 6.4x-24.7x compared with booting from scratch and 3.9x-23.5x over the state-of-the-art method (CRIU). © 2024 ACM.","Checkpoint and Restore; Cloud Computing; GPUs; Startup Latency","Cloud computing; Cloud platforms; Computer graphics equipment; Restoration; Checkpoint and restore; Cloud-based; Cloud-computing; Cost-efficiency; Diverse devices; Learning services; Machine-learning; On demands; Performance; Startup latency; Digital storage","Association for Computing Machinery, Inc","ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Groups on Management of Data (SIGMOD); Google; IBM; Microsoft","15th Annual ACM Symposium on Cloud Computing, SoCC 2024","20 November 2024 through 22 November 2024","Redmond","205525","Conference paper","Final","","Scopus","2-s2.0-85215512396"
"Yedidia Z.","Yedidia, Zachary (57219693315)","57219693315","Lightweight Fault Isolation: Practical, Efficient, and Secure Software Sandboxing","2024","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","2","","","649","665","16","7","10.1145/3620665.3640408","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192202315&doi=10.1145%2f3620665.3640408&partnerID=40&md5=a51c1dbfb45d9ac4724646f4f218eb6f","Software-based fault isolation (SFI) is a longstanding technique that allows isolation of one or more processes from each other with minimal or no use of hardware protection mechanisms. The demand for SFI systems has been increasing due to the advent of cloud and serverless computing, which require systems to run untrusted code with low latency and low context switch times. SFI systems must optimize for a combination of performance, trusted code base (TCB) size, scalability, and implementation complexity. With the rise of ARM64 in both cloud and personal computers, we revisit classic SFI in the context of ARM64 and present a new multi-sandbox SFI scheme that is practical to implement, efficient, and maintains a small TCB. Our technique, called Lightweight Fault Isolation (LFI), supports tens of thousands of 4GiB sandboxes in a single address space and does full software isolation of loads, stores, and jumps with a runtime overhead of 7% on the compatible subset of the SPEC 2017 benchmark suite. In addition to providing low runtime and code size overheads compared to existing multi-sandbox systems, LFI is implemented independently of existing compiler toolchains, has a small static verifier to reduce TCB size, is hardened against basic Spectre attacks, and has broad software support, including for language mechanisms like exceptions and ISA features such as SIMD. © 2024 Association for Computing Machinery. All rights reserved.","","ARM processors; Computer software; Side channel attack; Context switch time; Fault isolation; Hardware protection; Isolation systems; Low contexts; Low latency; Protection mechanisms; Sandboxing; Secure software; Untrusted code; Personal computers","Association for Computing Machinery","ACM Special Interest Group on Computer Architecture (SIGARCH); ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Group on Programming Languages (SIGPLAN)","29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2024","27 April 2024 through 1 May 2024","San Diego","198945","Conference paper","Final","","Scopus","2-s2.0-85192202315"
"Semjonov A.; Edinger J.","Semjonov, Anton (59076662800); Edinger, Janick (56178122500)","59076662800; 56178122500","Zero-Setup Computation Offloading to Heterogeneous Volunteer Devices Using Web Browsers","2024","MIDDLEWARE 2024 - Proceedings of the 25th ACM International Middleware Conference Demos, Posters and Doctoral Symposium, Part of: MIDDLEWARE 2024","","","","3","4","1","0","10.1145/3704440.3704776","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216407087&doi=10.1145%2f3704440.3704776&partnerID=40&md5=945dff7aa5ba513918d97d84fa805032","In the evolving landscape of distributed computing frameworks, Wasimoff emerges as an innovative middleware with a browser-based execution environment designed to facilitate computation offloading to heterogeneous volunteer devices. By leveraging WebAssembly as an executable format, Wasimoff allows users to seamlessly contribute computational resources through a simple web interface, thereby eliminating complex setup requirements. This demonstration presents the zero-setup functionality of the Wasimoff provider, and showcases the ease of connecting personal devices to a dynamic resource pool, highlighting the scaling efficiency and multitasking capabilities of the framework. By enabling spontaneous, barrier-free participation, Wasimoff transforms idle device capacity into a collaborative computational resource. © 2024 Copyright held by the owner/author(s).","collaborative computing; computation offloading; edge computing; serverless computing; webassembly","Middleware; Web browsers; Computation offloading; Computational resources; Distributed computing frameworks; Edge computing; Executables; Execution environments; Serverless computing; Simple++; Web interface; Webassembly; Computation offloading","Association for Computing Machinery, Inc","","25th ACM International Middleware Conference Demos, Posters and Doctoral Symposium, MIDDLEWARE 2024","2 December 2024 through 6 December 2024","Hong Kong","205814","Conference paper","Final","","Scopus","2-s2.0-85216407087"
"Bai Y.; Yang Z.; Gao F.","Bai, Yongshu (57191089722); Yang, Zhihui (57193668829); Gao, Feng (58080118600)","57191089722; 57193668829; 58080118600","Faast: An Efficient Serverless Framework Made Snapshot-based Function Response Fast","2024","HPDC 2024 - Proceedings of the 33rd International Symposium on High-Performance Parallel and Distributed Computing","","","","174","185","11","0","10.1145/3625549.3658681","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204940239&doi=10.1145%2f3625549.3658681&partnerID=40&md5=3da85bc62d7848aecc63c985de33d560","Recent years have seen the rapid popularity of serverless computing, benefiting from its seamless scalability and pay-as-you-go charging model. Despite recent efforts to enhance function cold-start performance by utilizing snapshot and working set mechanisms, the existing methods still suffered considerable overheads in end-to-end response time as functions' inputs varied. In this paper, we propose Faast, a novel solution designed to improve function response time by enhancing virtual machine snapshot-based startup while guaranteeing function execution performance. Our system introduces a preparation phase to generate a lightweight working set, which excludes stale pages that do not require loading from snapshot files for each function to expedite startup. Additionally, Faast facilitates network connectivity for the snapshot by implementing NAT within the virtio-net device model bypassing the time-consuming network namespace setup. To guarantee execution performance, a divergent page fault handling mechanism is designed to address page faults related to stale and non-stale pages separately, effectively mitigating sluggish disk I/O accesses. Faast also incrementally adapts the working set file across various function invocations to bolster functions' resilience to diverse inputs. Extensive experiments demonstrate that Faast reduces function end-to-end time by up to 63% compared to the state-of-the-art. © 2024 held by the owner/author(s).","cloud computing; serverless computing; snapshot; virtualization; working set","Virtual machine; Virtual reality; Charging models; Cloud-computing; Cold-start; Execution performance; Pay as you go; Performance; Serverless computing; Snapshot; Virtualizations; Working set; Response time (computer systems)","Association for Computing Machinery, Inc","ACM SIGARCH; CORNELIS NETWORKS; E4 COMPUTER ENGINEERING; NEC Laboratories America; VAST","33rd International Symposium on High-Performance Parallel and Distributed Computing, HPDC 2024","3 June 2024 through 7 June 2024","Pisa","202382","Conference paper","Final","","Scopus","2-s2.0-85204940239"
"Zhao Y.; Weng W.; van Nieuwpoort R.; Uta A.","Zhao, Yuxuan (57223129058); Weng, Weikang (59391428500); van Nieuwpoort, Rob (6602660244); Uta, Alexandru (56440094500)","57223129058; 59391428500; 6602660244; 56440094500","In Serverless, OS Scheduler Choice Costs Money: A Hybrid Scheduling Approach for Cheaper FaaS","2024","Middleware 2024 - Proceedings of the 25th ACM International Middleware Conference","","","","172","184","12","0","10.1145/3652892.3700757","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215517044&doi=10.1145%2f3652892.3700757&partnerID=40&md5=bdca8a6768673b15ab2269c1c4f68006","In Function-as-a-Service (FaaS) serverless, large applications are split into short-lived stateless functions. Deploying functions is mutually profitable: users need not be concerned with resource management, while providers can keep their servers at high utilization rates running thousands of functions concurrently on a single machine. It is exactly this high concurrency that comes at a cost. The standard Linux Completely Fair Scheduler (CFS) switches often between tasks, which leads to prolonged execution times. We present evidence that relying on the default Linux CFS scheduler increases serverless workloads cost by up to 10×. In this article, we raise awareness and make a case for rethinking the OS-level scheduling in Linux for serverless workloads composed of many short-lived processes. To make serverless more affordable we introduce a hybrid two-level scheduling approach that relies on FaaS characteristics. Short-running functions are executed in FIFO fashion without preemption, while longer-running functions are passed to CFS after a certain time period. We show that tailor-made OS scheduling is able to significantly reduce user-facing costs without adding any provider-facing overhead. © 2024 Copyright held by the owner/author(s).","Cost for serverless; CPU Scheduling; FaaS; Serverless computing","Artificial intelligence; Concurrency control; Costs; Resource allocation; Time switches; Cost for serverless; CPU scheduling; Function-as-a-service; High utilizations; Hybrid scheduling; Resource management; Serverless computing; Single- machines; User need; Utilization rates; Linux","Association for Computing Machinery, Inc","ACM","25th ACM International Middleware Conference, Middleware 2024","2 December 2024 through 6 December 2024","Hong Kong","205541","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85215517044"
"Jiang Y.; Roy R.B.; Li B.; Tiwari D.","Jiang, Yankai (58960464100); Roy, Rohan Basu (57219249946); Li, Baolin (57219261781); Tiwari, Devesh (23467777300)","58960464100; 57219249946; 57219261781; 23467777300","EcoLife: Carbon-Aware Serverless Function Scheduling for Sustainable Computing","2024","International Conference for High Performance Computing, Networking, Storage and Analysis, SC","","","","","","","5","10.1109/SC41406.2024.00018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215004851&doi=10.1109%2fSC41406.2024.00018&partnerID=40&md5=05d57b621ca1bf3cb3531930baaf2668","This work introduces ECOLIFE, the first carbon-aware serverless function scheduler to co-optimize carbon footprint and performance. ECOLIFE builds on the key insight of intelligently exploiting multi-generation hardware to achieve high performance and lower carbon footprint. ECOLIFE designs multiple novel extensions to Particle Swarm Optimization (PSO) in the context of serverless execution environment to achieve high performance while effectively reducing the carbon footprint. © 2024 IEEE.","Cloud Computing; Serverless Computing; Sustainable Computing","Carbon capture and storage; Carbon sequestration; Zero-carbon; Cloud-computing; Co-optimize; High-low; Low carbon; Multi generations; Particle swarm; Performance; Serverless computing; Sustainable computing; Swarm optimization; Carbon capture and utilization","IEEE Computer Society","ACM; ACM�s Special Interest Group on High Performance Computing (SIGHPC); IEEE Technical Committee on High Performance Computing (TCHPC); Institute of Electrical and Electronics Engineers (IEEE) Computer Society","2024 International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2024","17 November 2024 through 22 November 2024","Atlanta","205531","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85215004851"
"Gsteiger V.U.; Long P.H.(d).; Sun Y.(j).; Javanrood P.; Shahrad M.","Gsteiger, Viktor Urban (57429335300); Long, Pin Hong (Daniel) (59520893600); Sun, Yiran (Jerry) (59520997500); Javanrood, Parshan (59521107300); Shahrad, Mohammad (56943394900)","57429335300; 59520893600; 59520997500; 59521107300; 56943394900","Caribou: Fine-Grained Geospatial Shifting of Serverless Applications for Sustainability","2024","SOSP 2024 - Proceedings of the 2024 ACM SIGOPS 30th Symposium on Operating Systems Principles","","","","403","420","17","2","10.1145/3694715.3695954","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212881876&doi=10.1145%2f3694715.3695954&partnerID=40&md5=b5a6b61e440af4ba2a3f3d061f3789b2","Sustainability in computing is critical as environmental concerns rise. The cloud industry's carbon footprint is significant and rapidly growing. We show that dynamic geospatial shifting of cloud workloads to regions with lower carbon emission energy sources, particularly for more portable cloud workloads such as serverless applications, has a high potential to lower operational carbon emissions. To make the case, we build a comprehensive framework called Caribou that offloads serverless workflows across geo-distributed regions. Caribou requires no change in the application logic, nor on the provider side. It dynamically determines the best deployment plans, automatically (re-) deploys functions to appropriate regions, and redirects traffic to new endpoints. In reducing operational carbon through fine-grained, function-level offloading, Caribou does not undermine standard metrics such as performance and cost. We show how this approach can reduce the carbon footprint by an average of 22.9% to 66.6% across the North American continent. We demonstrate how a detailed specification of location constraints (e.g., to ensure compliance of one stage) can allow emission reductions for workflows (e.g., by offloading other stages). By showcasing the feasibility of carbon-aware geospatial application deployment, Caribou aims to push the boundaries of system techniques available to curtail cloud carbon emissions and provide a framework for future research.  © 2024 Copyright is held by the owner/author(s). Publication rights licensed to ACM.","carbon-aware scheduling; cloud computing; geospatial shifting; serverless computing; sustainability","Carbon sequestration; Low emission; Carbon emissions; Carbon-aware scheduling; Cloud-computing; Environmental concerns; Fine grained; Geo-spatial; Geospatial shifting; Low-carbon emissions; Serverless computing; Work-flows; Carbon capture and utilization","Association for Computing Machinery, Inc","ACM SIGOPS; Akamai; Amazon; et al.; FUTUREWEI Technologies; NSF","30th ACM Symposium on Operating Systems Principles, SOSP 2024","4 November 2024 through 6 November 2024","Austin","205375","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85212881876"
"Liu Y.; Guo J.; Jiang B.; Song Y.; Zhang P.; Wen R.; Lyu B.; Zhu S.; Wang X.","Liu, Yunzhuo (56966454500); Guo, Junchen (57202446664); Jiang, Bo (57210169854); Song, Yang (59259402800); Zhang, Pengyu (59871767400); Wen, Rong (59221405500); Lyu, Biao (57217212057); Zhu, Shunmin (57226894315); Wang, Xinbing (22136880500)","56966454500; 57202446664; 57210169854; 59259402800; 59871767400; 59221405500; 57217212057; 57226894315; 22136880500","FastIOV: Fast Startup of Passthrough Network I/O Virtualization for Secure Containers","2025","EuroSys 2025 - Proceedings of the 2025 20th European Conference on Computer Systems","","","","730","735","5","1","10.1145/3689031.3696066","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002245364&doi=10.1145%2f3689031.3696066&partnerID=40&md5=064eba3182300af803f89f4374e4c52c","Single Root I/O Virtualization (SR-IOV) technology has advanced in recent years and can simultaneously satisfy the network requirements of high data plane performance, high deployment density, and fast startup for applications in traditional containers. However, it falls short with secure containers, which have become the mainstream choice in multi-tenant clouds. SR-IOV requires secure containers to use passthrough I/O for higher data plane performance, which hinders the container startup performance and prevents its usage in time-sensitive tasks like serverless computing. In this paper, we advocate that the startup performance of SR-IOV enabled secure containers can be further boosted, making SR-IOV suitable for building a Container Network Interface (CNI) for secure containers. We first dissect the end-to-end concurrent startup process and identify three key bottlenecks that lead to the slow startup, including Virtual Function I/O device set management, Direct Memory Access memory mapping, and Virtual Function (VF) driver initialization. We then propose a CNI named FastIOV that addresses these bottlenecks through lock decomposition, unnecessary mapping skipping, decoupled zeroing, and asynchronous VF driver initialization. Our evaluation shows that FastIOV reduces the overhead of enabling SR-IOV for secure containers by 96.1%, achieving 65.7% and 75.4% reductions in the average and 99th percentile end-to-end startup time. © 2025 Copyright held by the owner/author(s).","Container Network; Overlay Network Startup; Passthrough I/O Virtualization; Secure Container; SR-IOV","Application programming interfaces (API); Benchmarking; Virtual addresses; Virtual reality; Container network; Data planes; Fast start-up; Overlay network startup; Passthrough I/O virtualization; Secure container; Single root I/O virtualization; Virtual functions; Virtualizations; Network function virtualization","Association for Computing Machinery, Inc","ACM SIGOPS; Amazon Web Services; AMD; et al.; Huawei; Microsoft","20th European Conference on Computer Systems, EuroSys 2025, co-located 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2025","30 March 2025 through 3 April 2025","Rotterdam","207851","Conference paper","Final","","Scopus","2-s2.0-105002245364"
"Dhakal A.; Raith P.; Ward L.; Hong Enriquez R.P.; Rattihalli G.; Chard K.; Foster I.; Milojicic D.","Dhakal, Aditya (57200495213); Raith, Philipp (57212228238); Ward, Logan (55556371700); Hong Enriquez, Rolando P. (55099055200); Rattihalli, Gourav (57193494996); Chard, Kyle (9132950200); Foster, Ian (35572232000); Milojicic, Dejan (6603838893)","57200495213; 57212228238; 55556371700; 55099055200; 57193494996; 9132950200; 35572232000; 6603838893","Fine-grained accelerator partitioning for Machine Learning and Scientific Computing in Function as a Service Platform","2023","ACM International Conference Proceeding Series","","","","1606","1613","7","2","10.1145/3624062.3624238","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178101165&doi=10.1145%2f3624062.3624238&partnerID=40&md5=e374b5c0524766b0faf55b6a56f03643","Function-as-a-service (FaaS) is a promising execution environment for high-performance computing (HPC) and machine learning (ML) applications as it offers developers a simple way to write and deploy programs. Nowadays, GPUs and other accelerators are indispensable for HPC and ML workloads. These accelerators are expensive to acquire and operate; consequently, multiplexing them can increase their financial profitability. However, we have observed that state-of-the-art FaaS frameworks usually treat accelerator as a single device to run single workload and have little support for multiplexing accelerators. In this work, we have presented techniques to multiplex GPUs with Parsl, a popular FaaS framework. We demonstrate why GPU multiplexing is beneficial for certain applications and how we have implemented GPU multiplexing in Parsl. With our enhancements, we show up to 60% lower task completion time and 250% improvement in the inference throughput of a large language model when multiplexing a GPU compared to running a single instance without multiplexing. We plan to extend the support for GPU multiplexing in FaaS platforms by tackling the challenges of changing compute resources in the partition and approximating how to right-size a GPU partition for a function. © 2023 ACM.","","Graphics processing unit; Machine learning; Program processors; Execution environments; Financial profitabilities; Fine grained; High-performance machines; Machine learning applications; Machine-learning; Performance computing; Service framework; Service platforms; Simple++; Application programs","Association for Computing Machinery","ACM's Special Interest Group on High Performance Computing (SIGHPC); Association for Computing Machinery; IEEE�s Technical Committee on High Performance Computing (TCHPC); Institute of Electrical and Electronics Engineers (IEEE) Computer Society","2023 International Conference on High Performance Computing, Network, Storage, and Analysis, SC Workshops 2023","12 November 2023 through 17 November 2023","Denver","194341","Conference paper","Final","","Scopus","2-s2.0-85178101165"
"","","","ASPLOS 2024 - Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems","2024","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","1","","","","","490","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191484448&partnerID=40&md5=5e7b2488b90931303b919f277a087158","The proceedings contain 28 papers. The topics discussed include: Amanda: unified instrumentation framework for deep neural networks; automatic generation of vectorizing compilers for customizable digital signal processors; Cocco: hardware-mapping co-exploration towards memory capacity-communication optimization; CodeCrunch: improving serverless performance via function compression and cost-aware warmup location optimization; EagleEye: nanosatellite constellation design for high-coverage, high-resolution sensing; expanding datacenter capacity with DVFS boosting: a safe and scalable deployment experience; exploiting human color discrimination for memory- and energy-efficient image encoding in virtual reality; formal mechanized semantics of CHERI C: capabilities, undefined behavior, and provenance; and lightweight, modular verification for WebAssembly-to-native instruction selection.","","","Association for Computing Machinery","ACM Special Interest Group on Computer Architecture (SIGARCH); ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Group on Programming Languages (SIGPLAN)","29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2024","27 April 2024 through 1 May 2024","San Diego","198945","Conference review","Final","","Scopus","2-s2.0-85191484448"
"Mvondo D.; Taiani F.; Bromberg Y.-D.","Mvondo, Djob (57208125838); Taiani, Francois (15137405200); Bromberg, Yerom-David (8729365100)","57208125838; 15137405200; 8729365100","Horse: Ultra-low latency workloads on FaaS platforms","2024","Middleware 2024 - Proceedings of the 25th ACM International Middleware Conference","","","","445","453","8","0","10.1145/3652892.3700784","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215507883&doi=10.1145%2f3652892.3700784&partnerID=40&md5=22dedd85e6b0bc76388bf26b3fc70608","We investigate if FaaS platforms can handle ultra-low latency workloads that run as low as less than 1μs and show that even for a warm start, the initialization time takes up to 99, 99% of the total execution time. This is due to the resume process of warm sandboxes that takes more time as the number of the sandbox’s allocated virtual CPUs (vCPUs) increases. We uncover that two operations use up to 93, 1% of the resume time. The first is the insertion of the paused sandbox’s vCPUs to a CPU-sorted run queue. The second is the update of a lock-protected variable, which represents the vCPUs’ load on each CPU. This variable is used for frequency scaling. We introduce Horse, for hot resume. Horse presents two simple approaches. The first is parallel precomputed sorted merge (P2SM), a parallel algorithm that leverages pre-computed data to have a parallel sorted merge of two sorted lists in O(1). The second is to coalesce the updates on the lock-protected variable used for frequency scaling. We implement Horse in Xen and Firecracker, two mainstream virtualization platforms. Our evaluation with real-world FaaS traces shows that Horse achieves up to 7, 16× resume time improvement and reduces sandbox initialization overhead by up to 142, 84× with no impact on functions. © 2024 Copyright held by the owner/author(s).","","Backpropagation; Interpolation; Program processors; Frequency-scaling; Low latency; Real-world; Simple approach; Virtualizations; Warm start; Virtual reality","Association for Computing Machinery, Inc","ACM","25th ACM International Middleware Conference, Middleware 2024","2 December 2024 through 6 December 2024","Hong Kong","205541","Conference paper","Final","","Scopus","2-s2.0-85215507883"
"Sui Y.; Yu H.; Hu Y.; Li J.; Wang H.","Sui, Yifan (59520950600); Yu, Hanfei (57220804031); Hu, Yitao (57188994639); Li, Jianxun (9236863000); Wang, Hao (57170260400)","59520950600; 57220804031; 57188994639; 9236863000; 57170260400","Pre-Warming is Not Enough: Accelerating Serverless Inference With Opportunistic Pre-Loading","2024","SoCC 2024 - Proceedings of the 2024 ACM Symposium on Cloud Computing","","","","178","195","17","0","10.1145/3698038.3698509","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215511458&doi=10.1145%2f3698038.3698509&partnerID=40&md5=982bdddb5e91d5cb5baa46e2d7a4d1ab","Serverless computing has rapidly prospered as a new cloud computing paradigm with agile scalability, pay-as-you-go pricing, and ease-to-use features for Machine Learning (ML) inference tasks. Users package their ML code into lightweight serverless functions and execute them using containers. Unfortunately, a notorious problem, called cold-starts, hinders serverless computing from providing low-latency function executions. To mitigate cold-starts, pre-warming, which keeps containers warm predictively, has been widely accepted by academia and industry. However, pre-warming fails to eliminate the unique latency incurred by loading ML artifacts. We observed that for ML inference functions, the loading of libraries and models takes significantly more time than container warming. Consequently, pre-warming alone is not enough to mitigate the ML inference function's cold-starts. This paper introduces InstaInfer, an opportunistic preloading technique to achieve instant inference by eliminating the latency associated with loading ML artifacts, thereby achieving minimal time cost in function execution. InstaInfer fully utilizes the memory of warmed containers to preload the function's libraries and model, striking a balance between maximum acceleration and resource wastage. We design InstaInfer to be transparent to providers and compatible with existing pre-warming solutions. Experiments on OpenWhisk with real-world workloads show that InstaInfer reduces up to 93% loading latency and achieves up to 8× speedup compared to state-of-the-art pre-warming solutions. © 2024 ACM.","Cloud Computing; Cold-Start; Machine Learning; Serverless Computing","Cloud computing; Cloud-computing; Cold-start; Computing paradigm; Inference functions; Learning artifacts; Loading machines; Machine-learning; Pay as you go; Preloading; Serverless computing","Association for Computing Machinery, Inc","ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Groups on Management of Data (SIGMOD); Google; IBM; Microsoft","15th Annual ACM Symposium on Cloud Computing, SoCC 2024","20 November 2024 through 22 November 2024","Redmond","205525","Conference paper","Final","","Scopus","2-s2.0-85215511458"
"Zhao X.; Yang S.; Wang J.; Diao L.; Qu L.; Wu C.","Zhao, Xiaoyang (57219165744); Yang, Siran (57222109987); Wang, Jiamang (57223760700); Diao, Lansong (57219737349); Qu, Lin (57224986115); Wu, Chuan (15836048100)","57219165744; 57222109987; 57223760700; 57219737349; 57224986115; 15836048100","FaPES: Enabling Efficient Elastic Scaling for Serverless Machine Learning Platforms","2024","SoCC 2024 - Proceedings of the 2024 ACM Symposium on Cloud Computing","","","","443","459","16","0","10.1145/3698038.3698548","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215514966&doi=10.1145%2f3698038.3698548&partnerID=40&md5=49c6debedd9e0598daf24da552096c5a","Serverless computing platforms have become increasingly popular for running machine learning (ML) tasks due to their user-friendliness and decoupling from underlying infrastructure. However, auto-scaling to efficiently serve incoming requests still remains a challenge, especially for distributed ML training or inference jobs in a serverless GPU cluster. Distributed training and inference jobs are highly sensitive to resource configurations, and demand high model efficiency throughout their lifecycle. We propose FaPES, a FaaS-oriented Performance-aware Elastic Scaling system to enable efficient resource allocation in serverless platforms for ML jobs. FaPES enables flexible resource loaning between virtual clusters for running training and inference jobs. For running inference jobs, servers are reclaimed on demand with minimal preemption overhead to guarantee service level objective (SLO); for training jobs, optimal GPU allocation and model hyperparameters are jointly adapted based on an ML-based performance model and a resource usage prediction board, alleviating users from model tuning and resource specification. Evaluation on a 128-GPU testbed demonstrates up to 24.8% job completion time reduction and ×1.8 Goodput improvement, as compared to representative elastic scaling schemes. © 2024 ACM.","Cluster Scheduling; Distributed System","Cluster computing; Cluster scheduling; Computing platform; Decouplings; Distributed machine learning; Distributed systems; Learning platform; Learning tasks; Machine-learning; Scalings; User friendliness; Resource allocation","Association for Computing Machinery, Inc","ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Groups on Management of Data (SIGMOD); Google; IBM; Microsoft","15th Annual ACM Symposium on Cloud Computing, SoCC 2024","20 November 2024 through 22 November 2024","Redmond","205525","Conference paper","Final","","Scopus","2-s2.0-85215514966"
"Nair A.; Szewczyk R.; Jennings D.; Barbalace A.","Nair, Alan (58271349200); Szewczyk, Raven (58043335600); Jennings, Donald (59520698000); Barbalace, Antonio (23491344500)","58271349200; 58043335600; 59520698000; 23491344500","Near-Storage Processing in FaaS with Funclets","2024","Middleware 2024 - Proceedings of the 25th ACM International Middleware Conference","","","","145","157","12","0","10.1145/3652892.3700755","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215531383&doi=10.1145%2f3652892.3700755&partnerID=40&md5=b5784751fcae582d734bc259fc8771f4","Serverless computing has disrupted how computation is performed in the Cloud. The ability to write Functions, and not care about infrastructure brings many benefits, including significantly lower deployment costs, improved developer workflow, scalability, resilience, and resource utilization. However, being stateless and not tied to specific machines, Functions need to access Cloud storage services to access data, which may require crossing the entire data center network incurring high overheads. Existing solutions either provide database APIs running on the storage servers to perform the data-intensive operations locally, or deploy entire Functions on storage servers. The former approach can not perform arbitrary computations locally on storage servers. The latter violates the principle of compute-storage disaggregation, resulting in poor scaling. We observe that allowing on-the-fly migration of I/O-intensive parts of Functions to storage nodes achieves both objectives. We propose a FaaS runtime that runs on both the compute servers and the servers running the storage services which introduces an efficient migration mechanism for Functions across machines to move I/O-intensive parts of Functions to the relevant storage node, with minimal code changes. This allows Functions to perform arbitrary computations on storage nodes, benefiting from the locality of data, without sacrificing the scalability offered by compute-storage disaggregation. We implement our approach in a state-of-the-art FaaS runtime and show that it improves latency and throughput in bandwidth-constrained FaaS workloads while making better utilization of idle CPU cycles on storage servers. © 2024 Copyright held by the owner/author(s).","Cloud Computing; Function-as-a-Service; Serverless","Cloud storage; Ferroelectric RAM; Picture archiving and communication systems; Program debugging; Storage as a service (STaaS); Cloud-computing; Deployment costs; Disaggregation; Function-as-a-service; Runtimes; Serverless; Storage nodes; Storage servers; Work-flows; Write functions; Scalability","Association for Computing Machinery, Inc","ACM","25th ACM International Middleware Conference, Middleware 2024","2 December 2024 through 6 December 2024","Hong Kong","205541","Conference paper","Final","","Scopus","2-s2.0-85215531383"
"Lan Y.; Peng X.; Wang Y.","Lan, Yuqiao (59520347100); Peng, Xiaohui (57198517501); Wang, Yifan (57203008237)","59520347100; 57198517501; 57203008237","Snapipeline: Accelerating Snapshot Startup for FaaS Containers","2024","SoCC 2024 - Proceedings of the 2024 ACM Symposium on Cloud Computing","","","","144","159","15","0","10.1145/3698038.3698513","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215537808&doi=10.1145%2f3698038.3698513&partnerID=40&md5=62f446e2ba60601fc3e4a6b43d481844","Due to the frequent starts and stops of numerous services in FaaS (Function as a Service), reducing cold start overhead is a core issue in improving the performance of container-based FaaS services. Snapshot and restore-based mechanisms effectively reduce the cold start time of containers by transforming container initialization overhead into restoration overhead. Consequently, this mechanism has become a research hotspot in accelerating the cold start of FaaS containers. Researchers introduce snapshot compression and decompress the snapshots to reduce the storage cost before starting instances. However, existing works have the following shortcomings: (1) File-mapped memory pages are not processed during snapshot compression, resulting in a significant amount of redundant data in memory; (2) The serial execution of snapshot decompression and instance restoration leads to high instance startup latency. To address these shortcomings, we propose the Snapipeline mechanism, which implements the following optimizations: (1) In the snapshot compression phase, it extends deduplication to file-mapped memory; (2) In the restoration phase, it leverages the hot and cold distinction in FaaS application memory to prioritize the memory pages restoration, pipelining snapshot decompression, memory pages restoration, and function execution. This mechanism hides the expensive snapshot decompression latency behind the instance restoration and function execution latency and removes the snapshot decompression from the critical path of instance startup. Evaluation on real-world FaaS application datasets shows that Snapipeline reduces memory usage by up to 28% and decreases end-to-end latency by an average of 53%, compared to the baseline. © 2024 Owner/Author.","Cloud Computing; Container; Serverless; Snapshot","Artificial intelligence; Cloud computing; Cloud-computing; Cold-start; Hotspots; Mapped memory; Memory pages; Performance; Serverless; Service container; Services applications; Snapshot; Restoration","Association for Computing Machinery, Inc","ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Groups on Management of Data (SIGMOD); Google; IBM; Microsoft","15th Annual ACM Symposium on Cloud Computing, SoCC 2024","20 November 2024 through 22 November 2024","Redmond","205525","Conference paper","Final","","Scopus","2-s2.0-85215537808"
"Katsakioris C.; Alverti C.; Nikas K.; Siakavaras D.; Psomadakis S.; Koziris N.","Katsakioris, Christos (57778523000); Alverti, Chloe (57216301139); Nikas, Konstantinos (57189018285); Siakavaras, Dimitrios (56147079500); Psomadakis, Stratos (55830537900); Koziris, Nectarios (6701648998)","57778523000; 57216301139; 57189018285; 56147079500; 55830537900; 6701648998","FaaSRail: Employing Real Workloads to Generate Representative Load for Serverless Research","2024","HPDC 2024 - Proceedings of the 33rd International Symposium on High-Performance Parallel and Distributed Computing","","","","214","226","12","3","10.1145/3625549.3658684","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204933396&doi=10.1145%2f3625549.3658684&partnerID=40&md5=2e420bd6dc7aea27aae7c0d584b07d35","With the proliferation of Serverless Computing, the Function-asa-Service (FaaS) paradigm is nowadays ubiquitous. As a result, the domain has attracted extensive research, both in industry and academia, identifying opportunities and addressing limitations across all aspects of this new Cloud paradigm. Recently, FaaS providers have released production workload traces of their commercial platforms. These expose important characteristics, such as the execution time of function invocations, their number and the distribution of their inter-arrival times, which must be taken into account for a concrete evaluation of innovative solutions. Nevertheless, the Serverless ecosystem still lacks a unified evaluation methodology based on such information.In this paper we attempt to fill this gap, by developing a methodology for fitting existing, real, open-source workloads found in FaaS benchmarking suites to production FaaS workload traces, in a way that sufficiently preserves the aforementioned core statistical properties of such traces. Based on this, we build FaaSRail, an open-source load generator that receives a target maximum request rate and a target total execution duration as inputs from the user and generates representative, scaled down FaaS load. © 2024 held by the owner/author(s).","benchmarking; cloud; datacenter; FaaS; load generation; open source; serverless","Benchmarking; Datacenter; Function-asa-service; Inter-arrival time; Load generation; Open-source; Production workloads; Real workloads; Serverless; Service paradigm; Service provider; Cloud platforms","Association for Computing Machinery, Inc","ACM SIGARCH; CORNELIS NETWORKS; E4 COMPUTER ENGINEERING; NEC Laboratories America; VAST","33rd International Symposium on High-Performance Parallel and Distributed Computing, HPDC 2024","3 June 2024 through 7 June 2024","Pisa","202382","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85204933396"
"Seshagiri V.; Gupta A.; Jabrayilov V.; Wildani A.; Kaffes K.","Seshagiri, Vishwanath (57202923946); Gupta, Abhinav (59520842400); Jabrayilov, Vahab (59156949700); Wildani, Avani (35423002000); Kaffes, Kostis (57212510735)","57202923946; 59520842400; 59156949700; 35423002000; 57212510735","Rethinking the Networking Stack for Serverless Environments: A Sidecar Approach","2024","SoCC 2024 - Proceedings of the 2024 ACM Symposium on Cloud Computing","","","","213","222","9","1","10.1145/3698038.3698561","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215501921&doi=10.1145%2f3698038.3698561&partnerID=40&md5=c0e359cb93357d0d23e71ca3d65f3314","Serverless platforms rely on legacy networking stacks for communication and data movement. We quantitatively analyze the performance of these stacks and show their mismatch with highly consolidated, virtualized modern serverless environments, focusing on Firecracker, the most common serverless virtualization framework. As serverless applications grow in complexity and interaction, the resulting network bottleneck is a prime source of user-perceived, end-to-end latency. In this paper, we present a detailed vision of a new, sidecar-based networking stack for serverless environments. Our primary design goal is to provide low-overhead networking while maintaining existing security guarantees. We outline the research challenges in both the control and the data plane that the community needs to tackle before such a sidecar architecture can be used in practice. © 2024 Owner/Author.","Isolation; Network Performance; Scalability; Serverless Networking; Sidecar Architecture; Virtualization","Virtual reality; Virtualization; Data movements; Design goal; End to end latencies; Isolation; Network bottlenecks; Performance; Primary design; Serverless networking; Sidecar architecture; Virtualizations; Network architecture","Association for Computing Machinery, Inc","ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Groups on Management of Data (SIGMOD); Google; IBM; Microsoft","15th Annual ACM Symposium on Cloud Computing, SoCC 2024","20 November 2024 through 22 November 2024","Redmond","205525","Conference paper","Final","","Scopus","2-s2.0-85215501921"
"Mohapatra A.D.; Oh K.","Mohapatra, Anshuman Das (58510078400); Oh, Kwangsung (56403899200)","58510078400; 56403899200","Smartpick: Workload Prediction for Serverless-enabled Scalable Data Analytics Systems","2023","Middleware 2023 - Proceedings of the 24th ACM/IFIP International Middleware Conference","","","","29","42","13","3","10.1145/3590140.3592850","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179896425&doi=10.1145%2f3590140.3592850&partnerID=40&md5=515f00ee4fc0b14cb1d0309b4cca2187","Many data analytic systems have adopted a newly emerging compute resource, serverless (SL), to handle data analytics queries in a timely and cost-efficient manner, i.e., serverless data analytics. While these systems can start processing queries quickly thanks to the agility and scalability of SL, they may encounter performance-and cost-bottlenecks based on workloads due to SL's worse performance and more expensive cost than traditional compute resources, e.g., virtual machine (VM). In this paper, we introduce Smartpick, a SL-enabled scalable data analytics system that exploits SL and VM together to realize composite benefits, i.e., agility from SL and better performance with reduced cost from VM. Smartpick uses a machine learning prediction scheme, decision-tree based Random Forest with Bayesian Optimizer, to determine SL and VM configurations, i.e., how many SL and VM instances for queries, that meet cost-performance goals. Smartpick offers a knob for applications to allow them to explore a richer cost-performance tradeoff space opened by exploiting SL and VM together. To maximize the benefits of SL, Smartpick supports a simple but strong mechanism, called relay-instances. Smartpick also supports event-driven prediction model retraining to deal with workload dynamics. A Smartpick prototype was implemented on Spark and deployed on live testbeds, Amazon AWS and Google Cloud Platform. Evaluation results indicate 97.05% and 83.49% prediction accuracies respectively with up to 50% cost reduction as opposed to the baselines. The results also confirm that Smartpick allows data analytics applications to navigate the richer cost-performance tradeoff space efficiently and to handle workload dynamics effectively and automatically.  © 2023 ACM.","cost-performance tradeoff; machine learning; prediction model; relay; serverless-enabled","Cloud analytics; Cost reduction; Decision trees; Forecasting; Machine learning; Virtual machine; Analytics systems; Cost performance; Cost-performance tradeoff; Data analytics; Machine-learning; Performance; Performance tradeoff; Prediction modelling; Relay; Serverless-enabled; Data Analytics","Association for Computing Machinery, Inc","","24th ACM/IFIP International Middleware Conference, Middleware 2023","11 December 2023 through 15 December 2023","Bologna","194765","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85179896425"
"Nõu A.; Talluri S.; Iosup A.; Bonetta D.","Nõu, Anders (58651729400); Talluri, Sacheendra (57200562502); Iosup, Alexandru (23392350500); Bonetta, Daniele (36647322000)","58651729400; 57200562502; 23392350500; 36647322000","Investigating Performance Overhead of Distributed Tracing in Microservices and Serverless Systems","2025","ICPE Companion 2025 - Companion of the 16th ACM/SPEC International Conference on Performance Engineering","","","","162","166","4","0","10.1145/3680256.3721316","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007284835&doi=10.1145%2f3680256.3721316&partnerID=40&md5=b51743f4c42331e6254e977e3b460c15","Distributed tracing is crucial to achieve observability in modern distributed systems. However, its adoption introduces performance trade-offs, impacting throughput and latency. This paper investigates the overhead of distributed tracing in microservices and serverless applications. We provide an analysis of the popular OpenTelemetry and Elastic APM distributed tracing frameworks, evaluating their performance impact on microservices and serverless workloads. We highlight and categorize the primary sources of overhead and measure their contribution to performance degradation. The results reveal significant throughput reductions (19-80%) and latency increases (up to 175%) depending on application configurations and execution environments. Our findings reveal that serializing trace data for export is the largest cause of overhead. © 2025 Owner/Author.","distributed tracing; instrumentation; open telemetry; performance","Client server computer systems; Message passing; Distributed systems; Distributed tracing; Instrumentation; Open telemetry; Performance; Performance impact; Performance tradeoff; Primary sources; Serverless systems; Tracing framework; Multiprocessing systems","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC","16th ACM/SPEC International Conference on Performance Engineering, ICPE Companion 2025","5 May 2025 through 9 May 2025","Toronto","208717","Conference paper","Final","","Scopus","2-s2.0-105007284835"
"Huang J.; Zhang M.; Ma T.; Liu Z.; Lin S.; Chen K.; Jiang J.; Liao X.; Shan Y.; Zhang N.; Lu M.; Ma T.; Gong H.; Wu Y.","Huang, Jialiang (59259960100); Zhang, MingXing (56733793100); Ma, Teng (57193406137); Liu, Zheng (57219260316); Lin, Sixing (59520872700); Chen, Kang (55515774700); Jiang, Jinlei (7404829238); Liao, Xia (57219054135); Shan, Yingdi (57338966300); Zhang, Ning (59259385400); Lu, Mengting (57210605762); Ma, Tao (57211901430); Gong, Haifeng (57194542550); Wu, YongWei (8417507400)","59259960100; 56733793100; 57193406137; 57219260316; 59520872700; 55515774700; 7404829238; 57219054135; 57338966300; 59259385400; 57210605762; 57211901430; 57194542550; 8417507400","TrEnv: Transparently Share Serverless Execution Environments Across Different Functions and Nodes","2024","SOSP 2024 - Proceedings of the 2024 ACM SIGOPS 30th Symposium on Operating Systems Principles","","","","421","437","16","1","10.1145/3694715.3695967","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214164766&doi=10.1145%2f3694715.3695967&partnerID=40&md5=08b18a71e325d91bab6d4b47fbdd6420","Serverless computing is renowned for its computation elasticity, yet its full potential is often constrained by the requirement for functions to operate within local and dedicated background environments, resulting in limited memory elasticity. To address this limitation, this paper introduces TrEnv, a co-designed integration of the serverless platform with the operating system and CXL/RDMA-based remote memory pools in two key areas. Firstly, TrEnv introduces repurposable sandboxes, which can be shared across different functions and hence, substantially decrease the overhead associated with creating isolation sandboxes. Secondly, it augments the OS with ""memory templates"" that enable rapid restoration of function states stored on remote memory. These innovations allow TrEnv to facilitate rapid transitions between instances of different functions and enable memory sharing across multiple nodes. Our evaluations using a variety of representative and real-world workloads demonstrate that TrEnv can initiate a container within 10 milliseconds, achieving up to a 7× speedup in P99 end-to-end latency and reducing memory usage by 48% on average compared to state-of-the-art on-demand restoring systems. © 2024 Copyright held by the owner/author(s).","cold start; CXL; remote memory; serverless","Background environment; Cold-start; CXL; Execution environments; Limited memory; Memory pool; Memory-sharing; Rapid transitions; Remote memory; Serverless; Memory architecture","Association for Computing Machinery, Inc","ACM SIGOPS; Akamai; Amazon; et al.; FUTUREWEI Technologies; NSF","30th ACM Symposium on Operating Systems Principles, SOSP 2024","4 November 2024 through 6 November 2024","Austin","205375","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85214164766"
"","","","Middleware Industrial Track 2024 - Proceedings of the Middleware Industrial Track, Part of: Middleware 2024","2024","Middleware Industrial Track 2024 - Proceedings of the Middleware Industrial Track, Part of: Middleware 2024","","","","","","51","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215515090&partnerID=40&md5=541f4b44ed4253a0481718b6cd96604e","The proceedings contain 7 papers. The topics discussed include: a conversational assistant framework for automation; ColdPurge: efficient metadata cache cleaning via accurate online data hotness tracking; serverful functions: leveraging servers in complex serverless workflows; microcosm: a scalable transaction processing scheme for modular blockchain; StreamSense: policy-driven semantic video search in streaming systems; A2L: user-transparent workload-adaptive asynchronous I/O layer; and ‘back to the byte’: towards byte-oriented semantics for streaming storage.","","","Association for Computing Machinery, Inc","ACM","2024 Middleware Industrial Track, Middleware Industrial Track 2024","2 December 2024 through 6 December 2024","Hong Kong","205526","Conference review","Final","","Scopus","2-s2.0-85215515090"
"Cvetković L.; Costa F.; Djokic M.; Friedman M.; Klimovic A.","Cvetković, Lazar (57223248954); Costa, François (59096445100); Djokic, Mihajlo (58698185800); Friedman, Michal (57214449110); Klimovic, Ana (56039431800)","57223248954; 59096445100; 58698185800; 57214449110; 56039431800","Dirigent: Lightweight Serverless Orchestration","2024","SOSP 2024 - Proceedings of the 2024 ACM SIGOPS 30th Symposium on Operating Systems Principles","","","","369","384","15","1","10.1145/3694715.3695966","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215504359&doi=10.1145%2f3694715.3695966&partnerID=40&md5=88cecd785c719ab429256575da1dd05f","While Function as a Service (FaaS) platforms can initialize function sandboxes on worker nodes in 10-100s of milliseconds, the latency to schedule functions in real FaaS clusters can be orders of magnitude higher. The current approach of building FaaS cluster managers on top of legacy orchestration systems (e.g., Kubernetes) leads to high scheduling delays when clusters experience high sandbox churn, which is common for FaaS. Generic cluster managers use many hierarchical abstractions and internal components to manage and reconcile cluster state with frequent persistent updates. This becomes a bottleneck for FaaS since the cluster state frequently changes as sandboxes are created on the critical path of requests. Based on our root cause analysis of performance issues in existing FaaS cluster managers, we propose Dirigent, a clean-slate system architecture for FaaS orchestration with three key principles. First, Dirigent optimizes internal cluster manager abstractions to simplify state management. Second, it eliminates persistent state updates on the critical path of function invocations, leveraging the fact that FaaS abstracts sandbox locations from users to relax exact state reconstruction guarantees. Finally, Dirigent runs monolithic control and data planes to minimize internal communication overheads and maximize throughput. We compare Dirigent to state-of-the-art FaaS platforms and show that Dirigent reduces 99th percentile per-function scheduling latency for a production workload by 2.79× compared to AWS Lambda. Dirigent can spin up 2500 sandboxes per second at low latency, which is 1250× more than Knative. © 2024 Copyright held by the owner/author(s).","","Artificial intelligence; Hierarchical systems; Platform as a Service (PaaS); Systems analysis; 'current; Cluster managers; Cluster state; Critical Paths; Orders of magnitude; Real functions; Root cause analysis; Service clusters; Service platforms; Worker nodes; Information management","Association for Computing Machinery, Inc","ACM SIGOPS; Akamai; Amazon; et al.; FUTUREWEI Technologies; NSF","30th ACM Symposium on Operating Systems Principles, SOSP 2024","4 November 2024 through 6 November 2024","Austin","205375","Conference paper","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85215504359"
"Zhao Z.N.; Morrison A.; Fletcher C.W.; Torrellas J.","Zhao, Zirui Neil (57220543871); Morrison, Adam (14619745900); Fletcher, Christopher W. (36175594200); Torrellas, Josep (7003395953)","57220543871; 14619745900; 36175594200; 7003395953","Everywhere All at Once: Co-Location Attacks on Public Cloud FaaS","2024","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","1","","","133","149","16","7","10.1145/3617232.3624867","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191466253&doi=10.1145%2f3617232.3624867&partnerID=40&md5=522638198e34652f9453748e8efff7e3","Microarchitectural side-channel attacks exploit shared hardware resources, posing significant threats to modern systems. A pivotal step in these attacks is achieving physical host co-location between attacker and victim. This step is especially challenging in public cloud environments due to the widespread adoption of the virtual private cloud (VPC) and the ever-growing size of the data centers. Furthermore, the shift towards Function-as-a-Service (FaaS) environments, characterized by dynamic function instance placements and limited control for attackers, compounds this challenge.In this paper, we present the first comprehensive study on risks of and techniques for co-location attacks in public cloud FaaS environments. We develop two physical host fingerprinting techniques and propose a new, inexpensive methodology for large-scale instance co-location verification. Using these techniques, we analyze how Google Cloud Run places function instances on physical hosts and identify exploitable placement behaviors. Leveraging our findings, we devise an effective strategy for instance launching that achieves 100% probability of co-locating the attacker with at least one victim instance. Moreover, the attacker co-locates with 61% - 100% of victim instances in three major Cloud Run data centers. © 2024 Copyright is held by the owner/author(s). Publication rights licensed to ACM.","cloud computing; co-location vulnerability; function-as-a-service (FaaS); timestamp counter","Cloud computing; Location; All-at-once; Cloud-computing; Co-location vulnerability; Colocations; Datacenter; Function-as-a-service; Public clouds; Service environment; Time-stamp; Timestamp counter; Side channel attack","Association for Computing Machinery","ACM Special Interest Group on Computer Architecture (SIGARCH); ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Group on Programming Languages (SIGPLAN)","29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2024","27 April 2024 through 1 May 2024","San Diego","198945","Conference paper","Final","","Scopus","2-s2.0-85191466253"
"Segarra C.; Durev I.; Pietzuch P.","Segarra, Carlos (57209334604); Durev, Ivan (59520829100); Pietzuch, Peter (22734675800)","57209334604; 59520829100; 22734675800","Is It Time To Put Cold Starts In The Deep Freeze?","2024","SoCC 2024 - Proceedings of the 2024 ACM Symposium on Cloud Computing","","","","259","268","9","0","10.1145/3698038.3698527","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215508040&doi=10.1145%2f3698038.3698527&partnerID=40&md5=7cfd80b8c578eca0ae29700f4406e497","Cold-start times have been the ""end-all, be-all""metric for research in serverless cloud computing over the past decade. Reducing the impact of cold starts matters, because they can be the biggest contributor to a serverless function's end-to-end execution time. Recent studies from cloud providers, however, indicate that, in practice, a majority of serverless functions are triggered by non-interactive workloads. To substantiate this, we study the types of serverless functions used in 35 publications and find that over 80% of functions are not semantically latency sensitive. If a function is non-interactive and latency insensitive, is end-to-end execution time the right metric to optimize in serverless? What if cold starts do not matter that much, after all? In this vision paper, we explore what serverless environments in which cold starts do not matter would look like. We make the case that serverless research should focus on supporting latency insensitive, i.e., batch, workloads. Based on this, we explore the design space for DFaaS, a serverless framework with an execution model in which functions can be arbitrarily delayed. DFaaS users annotate each function with a delay tolerance and, as long as the deadline has not passed, the runtime may interrupt or migrate function execution. Our micro-benchmarks suggest that, by targeting batch workloads, DFaaS can improve substantially the resource usage of serverless clouds and lower costs for users. © 2024 ACM.","batch processing; cloud computing; cold-starts; function-as-a-service; serverless; serverless accelerators","Benchmarking; Cloud computing; Radiation hardening; Surface hardening; Batch processing; Batch workloads; Cloud providers; Cloud-computing; Cold-start; Design spaces; End to end; Function-as-a-service; Serverless; Serverless accelerator; Batch data processing","Association for Computing Machinery, Inc","ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Groups on Management of Data (SIGMOD); Google; IBM; Microsoft","15th Annual ACM Symposium on Cloud Computing, SoCC 2024","20 November 2024 through 22 November 2024","Redmond","205525","Conference paper","Final","","Scopus","2-s2.0-85215508040"
"Panda A.; Sarangi S.R.","Panda, Abhisek (57704711100); Sarangi, Smruti R. (12344573600)","57704711100; 12344573600","SnapStore: A Snapshot Storage System for Serverless Systems","2023","Middleware 2023 - Proceedings of the 24th ACM/IFIP International Middleware Conference","","","","261","274","13","3","10.1145/3590140.3629120","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179889589&doi=10.1145%2f3590140.3629120&partnerID=40&md5=e5b3c9a7f4fd4940c45d9e759ad157c8","Serverless computing is getting increasingly popular because of its fine-grained billing model and autoscaling features. To speed up the process of functions' sandbox creation, cloud providers typically utilize snapshot and restore-based mechanisms for pre-warmed snapshots. This effectively trades off the startup latency with the storage requirements and the overhead of creating/restoring these snapshots. Hence, there is a need to compress the snapshots by identifying identical data chunks across snapshots and then design methods to quickly deduplicate snapshots and retrieve them. We propose SnapStore - a novel method of finding such duplicates. As opposed to conventional work that relies on better hashing methods, we use the natural structure of the program's memory map to reduce wasted work during deduplication. Furthermore, we sequentialize and minimize disk accesses as much as possible while retrieving a snapshot into a RAM-based cache. Both of these optimizations, yield a reasonably large speedup in the deduplication process as compared to the state-of-the-art (≈ 46% in the snapshot deduplication time and ≈ 82.6% in the retrieval time on HDDs). Upon integration with FaaSnap (a state-of-the-art serverless platform), SnapStore improves the end-to-end latency of serverless functions by 25.9% along with 2.4× storage space reduction over vanilla FaaSnap on HDDs. With SSDs, our deduplication time and retrieval time reduce by 36.2% and 75.8%, respectively, with almost no degradation in the end-to-end latency.  © 2023 ACM.","Deduplication; Function-as-a-Service; Serverless computing; Snapshot storage systems","Deduplication; End to end latencies; Fine grained; Function-as-a-service; Retrieval time; Serverless computing; Serverless systems; Snapshot storage system; State of the art; Storage systems; Random access storage","Association for Computing Machinery, Inc","","24th ACM/IFIP International Middleware Conference, Middleware 2023","11 December 2023 through 15 December 2023","Bologna","194765","Conference paper","Final","","Scopus","2-s2.0-85179889589"
"Zhang J.; Yuan H.; Wang Z.; Yu P.; Song H.; Qu D.; Yan S.; Zheng X.","Zhang, Junye (57221634854); Yuan, Hui (59697490200); Wang, Zhe (55966670700); Yu, Peng (36664072900); Song, Hexiang (57226551379); Qu, Di (59695984100); Yan, Siyu (59696846000); Zheng, Xiaolong (59696846100)","57221634854; 59697490200; 55966670700; 36664072900; 57226551379; 59695984100; 59696846000; 59696846100","Revisiting the Underlying Causes of RDMA Scalability Issues","2024","Proceedings - 2024 IEEE International Symposium on Parallel and Distributed Processing with Applications, ISPA 2024","","","","227","235","8","0","10.1109/ISPA63168.2024.00037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000179557&doi=10.1109%2fISPA63168.2024.00037&partnerID=40&md5=481eb04da617ba71cbe0aee322ece1ac","Remote direct memory access (RDMA) networks are widely deployed in clouds and data centers for low latency and high throughput. Emerging applications like artificial intelligence training and serverless computing demand RDMA networks with high connection scalability. However, increasing connections can reduce throughput, as many academic research states. Conversely, some industry reports indicate the scalability issues are situation-specific. Despite this, existing work lacks a comprehensive analysis of RDMA scalability issue triggers and causes. In this paper, we revisit triggering conditions and underlying causes of RDMA scalability issues. First, we comprehensively analyze RDMA data flows and potential resource contentions, particularly with direct cache access. Second, we conduct extensive tests across varied measurement settings and RDMA NICs (RNICs). We identify triggering conditions including frequent switching of queue pair connections, rapid request posting, intensive memory access demands and inappropriate program configurations. We also systematically uncover causes of scalability issues, mainly due to RNIC cache misses, RNIC processing unit backpressure, host last-level cache misses, and software inefficiency. Finally, we provide guidelines to mitigate the RDMA scalability issues.  © 2024 IEEE.","RDMA; Scalability Issues; Underlying Cause","Cache memory; Data centers; Access network; Cache Miss; Condition; Datacenter; High-throughput; Low latency; Low-high; Remote direct memory access; Scalability issue; Underlying cause; Scalability","Institute of Electrical and Electronics Engineers Inc.","et al.; IEEE; IEEE Computer Society; IEEE SC Technical Committee on Hyper-Intelligence (HI-TC); IEEE Technical Committee on Scalable Computing (TCSC); IEEE Technical Committee on Smart World","22nd IEEE International Symposium on Parallel and Distributed Processing with Applications, ISPA 2024","30 October 2024 through 2 November 2024","Kaifeng","207145","Conference paper","Final","","Scopus","2-s2.0-105000179557"
"","","","ICPE 2023 - Companion of the 2023 ACM/SPEC International Conference on Performance Engineering","2023","ICPE 2023 - Companion of the 2023 ACM/SPEC International Conference on Performance Engineering","","","","","","371","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158877288&partnerID=40&md5=16c0ab07d26233b0083394d8558cbb3f","The proceedings contain 62 papers. The topics discussed include: parallel performance engineering using score-P and Vampir; incremental change detection method for data center power efficiency metrics; enhancing the configuration tuning pipeline of large-scale distributed applications using large language models; a case of multi-resource fairness for serverless workflows; challenges and future directions in efficiency benchmarking; a reference architecture for datacenter scheduler programming abstractions: design and experiments; uncovering steady state executions in Java microbenchmarking with call graph analysis; analyzing static source code features to determine a correlation to steady state performance in java microbenchmarks; searching for the ground truth: assessing the similarity of benchmarking runs; and software mining - investigating correlation between source code features and michrobenchmark's steady state.","","","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC Research","14th Annual ACM/SPEC International Conference on Performance Engineering, ICPE 2023","15 April 2023 through 19 April 2023","Coimbra","187962","Conference review","Final","","Scopus","2-s2.0-85158877288"
"Ristov S.; Farahani R.; Prodan R.","Ristov, Sashko (49561774300); Farahani, Reza (57220368259); Prodan, Radu (8858675500)","49561774300; 57220368259; 8858675500","Large-scale Graph Processing and Simulation with Serverless Workflows in Federated FaaS","2023","ICPE 2023 - Companion of the 2023 ACM/SPEC International Conference on Performance Engineering","","","","227","231","4","3","10.1145/3578245.3585333","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158875300&doi=10.1145%2f3578245.3585333&partnerID=40&md5=98b0d4a2fd8915105f9f611d61fed5bc","Serverless computing offers an affordable and easy way to code lightweight functions that can be invoked based on some events to perform simple tasks. For more complicated processing, multiple serverless functions can be orchestrated as a directed acyclic graph to form a serverless workflow, so-called function choreography (FC). Although most famous cloud providers offer FC management systems such as AWS Step Functions, and there are also several open-source FC management systems (e.g., Apache OpenWhisk), their primary focus is on describing the control flow and data flow between serverless functions in the FC. Moreover, the existing FC management systems rarely consider the processed data, which is commonly represented in a graph format. In this paper, we review the capabilities of the existing FC management systems in supporting graph processing applications. We also raise two key research questions related to large-scale graph processing using serverless computing in federated Function-as-a-Service (FaaS). As part of the Graph-Massivizer project, funded by the Horizon Europe research and innovation program, we will research and develop (prototype) solutions that will address these challenges. © 2023 Owner/Author.","computing continuum; graph processing; massive graph; serverless computing; workflows","Data flow analysis; Workflow management; Acyclic graphs; Cloud providers; Computing continuum; Graph processing; Large-scales; Management systems; Massive graph; Serverless computing; Simple++; Work-flows; Open systems","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC Research","14th Annual ACM/SPEC International Conference on Performance Engineering, ICPE 2023","15 April 2023 through 19 April 2023","Coimbra","187962","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85158875300"
"Zhou Z.; Zhang Y.; Delimitrou C.","Zhou, Zhuangzhuang (57223096332); Zhang, Yanqi (57208405061); Delimitrou, Christina (38361424300)","57223096332; 57208405061; 38361424300","AQUATOPE: QoS-and-Uncertainty-Aware Resource Management for Multi-stage Serverless Workflows","2022","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","","","","1","14","13","43","10.1145/3567955.3567960","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145571203&doi=10.1145%2f3567955.3567960&partnerID=40&md5=49488eadf164ba2e8e30a632fcbf2d16","Multi-stage serverless applications, i.e., workflows with many computation and I/O stages, are becoming increasingly representative of FaaS platforms. Despite their advantages in terms of fine-grained scalability and modular development, these applications are subject to suboptimal performance, resource inefficiency, and high costs to a larger degree than previous simple serverless functions. We present Aquatope, a QoS-and-uncertainty-aware resource scheduler for end-to-end serverless workflows that takes into account the inherent uncertainty present in FaaS platforms, and improves performance predictability and resource efficiency. Aquatope uses a set of scalable and validated Bayesian models to create pre-warmed containers ahead of function invocations, and to allocate appropriate resources at function granularity to meet a complex workflow's end-to-end QoS, while minimizing resource cost. Across a diverse set of analytics and interactive multi-stage serverless workloads, Aquatope significantly outperforms prior systems, reducing QoS violations by 5X, and cost by 34% on average and up to 52% compared to other QoS-meeting methods.  © 2022 ACM.","Cloud computing; datacenter; function-as-a-service; machine learning for systems; quality of service; resource allocation; resource efficiency; resource management; serverless computing","Bayesian networks; Efficiency; Natural resources management; Quality of service; Cloud-computing; Datacenter; Function-as-a-service; Machine learning for system; Machine-learning; Quality-of-service; Resource efficiencies; Resource management; Resources allocation; Serverless computing; Resource allocation","Association for Computing Machinery","ACM; ACM SIGARCH; ACM SIGOPS; ACM SIGPLAN","28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2023","25 March 2023 through 29 March 2023","Vancouver","185370","Conference paper","Final","","Scopus","2-s2.0-85145571203"
"Roy R.B.; Patel T.; Tiwari D.","Roy, Rohan Basu (57219249946); Patel, Tirthak (57200205359); Tiwari, Devesh (23467777300)","57219249946; 57200205359; 23467777300","IceBreaker:Warming Serverless Functions Better with Heterogeneity","2022","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","","","","753","767","14","96","10.1145/3503222.3507750","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126392187&doi=10.1145%2f3503222.3507750&partnerID=40&md5=4d1d09d52deaeffca81b8fbf8ec8bd96","Serverless computing, an emerging computing model, relies on ""warming up""functions prior to its anticipated execution for faster and cost-effective service to users. Unfortunately, warming up functions can be inaccurate and incur prohibitively expensive cost during the warmup period (i.e., keep-Alive cost). In this paper, we introduce IceBreaker, a novel technique that reduces the service time and the ""keep-Alive""cost by composing a system with heterogeneous nodes (costly and cheaper). IceBreaker does so by dynamically determining the cost-effective node type to warm up a function based on the function's time-varying probability of the next invocation. By employing heterogeneity, IceBreaker allows for more number of nodes under the same cost budget and hence, keeps more number of functions warm and reduces the wait time during high load. Our real-system evaluation confirms that IceBreaker reduces the overall keep-Alive cost by 45% and execution time by 27% using representative serverless applications and industry-grade workload trace. IceBreaker is the first technique to employ and leverage the idea of mixing expensive and cheaper nodes to improve both service time and keep-Alive cost for serverless functions-opening up a new research avenue of serverless computing on heterogeneous servers for researchers and practitioners.  © 2022 ACM.","Cloud Computing; Cold Start; Heterogeneous Hardware; Keep-Alive Cost; Serverless Computing","Cost effectiveness; Cost reduction; Cloud-computing; Cold-start; Computing model; Cost effective; Heterogeneous hardware; Keep-alive; Keep-alive cost; Serverless computing; Service time; Warm-up period; Budget control","Association for Computing Machinery","ACM; ACM SIGARCH; ACM SIGOPS; ACM SIGPLAN","27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2022","28 February 2022 through 4 March 2022","Virtual, Online","177381","Conference paper","Final","","Scopus","2-s2.0-85126392187"
"","","","Middleware 2021 Industry Track - Proceedings of the 2021 International Middleware Conference Industrial Track","2021","Middleware 2021 Industry Track - Proceedings of the 2021 International Middleware Conference Industrial Track","","","","","","34","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121445182&partnerID=40&md5=03ee1ea2adde9a5f85cf7b73a38cf9b2","The proceedings contain 3 papers. The topics discussed include: authenticated key-value stores with hardware enclaves; the serverless shell; polygon: a QUIC-based CDN server selection system supporting multiple resource demands; and procedure-driven deployment support for the microservice era.","","","Association for Computing Machinery, Inc","ACM; ETS; University Laval","22nd International Middleware Conference, Middleware 2021","6 December 2021 through 10 December 2021","Virtual, Online","175054","Conference review","Final","","Scopus","2-s2.0-85121445182"
"Ristov S.; Hautz M.; Hollaus C.; Prodan R.","Ristov, Sashko (49561774300); Hautz, Mika (57850530700); Hollaus, Christian (57849339300); Prodan, Radu (8858675500)","49561774300; 57850530700; 57849339300; 8858675500","SimLess: Simulate ServerlessWorkflows and Their Twins and Siblings in Federated FaaS","2022","SoCC 2022 - Proceedings of the 13th Symposium on Cloud Computing","","","","323","339","16","13","10.1145/3542929.3563478","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143256924&doi=10.1145%2f3542929.3563478&partnerID=40&md5=a7bbefe9e73505e5ff7913480b8eeaaf","Many researchers migrate scientific serverless workflows or function choreographies (FCs) on Function-as-a-Service (FaaS) to benefit from its high scalability and elasticity. Unfortunately, the heterogeneity of federated FaaS hampers decisions on appropriate parameter setup to run FCs. Consequently, scientists must choose between accurate but tedious and expensive experiments or simple but cheap and less accurate simulations. Unfortunately, related works support either simulation models for serverfull workflows running on virtual machines and containers or partial FaaS models for individual serverless functions focused on execution time and neglecting various kinds of federated overheads. This paper introduces SimLess, an FC simulation framework for accurate FC simulations across multiple FaaS providers with a simple and lightweight parameter setup. Unlike the costly approaches that use machine learning over time series to predict the FC behavior, SimLess introduces two light concepts: (1) twins, representing the same function deployed with the same computing, communication, and storage resources, but in other regions of the same FaaS provider, and (2) siblings, representing the same function deployed in the same region with different computing resources. The novel SimLess FC simulation model splits the round trip time of a function into several parameters reused among twins and siblings without necessarily running them. We evaluated SimLess with two scientific FCs deployed across 18 AWS, Google, and IBM regions. SimLess simulates the cumulative overhead with an average inaccuracy of 8.9 % without significant differences between regions for learning and validation. Moreover, SimLess uses measurements of a low-concurrency FC executed in a single region to simulate a high-concurrency FC with 2,500 functions in the other areas with an inaccuracy of up to 9.75 %. Finally, SimLess reduces the parameter setup effort by 77.23 % compared to other simulation approaches.  © 2022 Owner/Author.","FaaS; modeling; performance; serverless; simulation; workflows","Computing resource; Function-as-a-service; Modeling; Performance; Serverless; Service provider; Simple++; Simulation; Simulation model; Work-flows; Learning systems","Association for Computing Machinery, Inc","ACM Special Interest Grou on Operating Systems (SIGOPS); ACM Special Interest Group on Management of Data (SIGMOD); EBay; et al.; FutureWei; Microsoft","13th Annual ACM Symposium on Cloud Computing, SoCC 2022","7 November 2022 through 11 November 2022","San Francisco","184095","Conference paper","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85143256924"
"Li Z.; Liu Y.; Guo L.; Chen Q.; Cheng J.; Zheng W.; Guo M.","Li, Zijun (57278389100); Liu, Yushi (57489093300); Guo, Linsong (57394013200); Chen, Quan (36623232500); Cheng, Jiagan (57393989800); Zheng, Wenli (56258882500); Guo, Minyi (7201564780)","57278389100; 57489093300; 57394013200; 36623232500; 57393989800; 56258882500; 7201564780","FaaSFlow: Enable efficient work flow execution for function-As-A-service","2022","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","","","","782","796","14","65","10.1145/3503222.3507717","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126391771&doi=10.1145%2f3503222.3507717&partnerID=40&md5=d4be04fd961baae1d303b5cb8b52889a","Serverless computing (Function-As-A-Service) provides fine-grain resource sharing by running functions (or Lambdas) in containers. Data-dependent functions are required to be invoked following a pre-defined logic, which is known as serverless workflows. However, our investigation shows that the traditional master-worker based workflow execution architecture performs poorly in serverless context. One significant overhead results from the master-side workflow schedule pattern, with which the functions are triggered in the master node and assigned to worker nodes for execution. Besides, the data movement between workers also reduces the throughput. To this end, we present a worker-side workflow schedule pattern for serverless workflow execution. Following the design, we implement FaaSFlow to enable efficient workflow execution in the serverless context. Besides, we propose an adaptive storage library FaaStore that enables fast data transfer between functions on the same node without through the database. Experiment results show that FaaSFlow effectively mitigates the workflow scheduling overhead by 74.6% on average and data transmission overhead by 95% at most. When the network bandwidth fluctuates, FaaSFlow-FaaStore reduces the throughput degradation by 23.0%, and is able to multiply the utilization of network bandwidth by 1.5X-4X.  © 2022 ACM.","FaaS; graph partition; master-worker; serverless workflows","Bandwidth; Computation theory; Data flow analysis; Digital storage; Flow graphs; Faas; Graph partition; Master/worker; Network bandwidth; Schedule patterns; Serverless workflow; Work-flows; Workers'; Workflow execution; Workflow schedules; Data transfer","Association for Computing Machinery","ACM; ACM SIGARCH; ACM SIGOPS; ACM SIGPLAN","27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2022","28 February 2022 through 4 March 2022","Virtual, Online","177381","Conference paper","Final","","Scopus","2-s2.0-85126391771"
"Shu J.; Zhu R.; Ma Y.; Huang G.; Mei H.; Liu X.; Jin X.","Shu, Junyi (57236553800); Zhu, Ruidong (58080542000); Ma, Yun (55808012300); Huang, Gang (57220845527); Mei, Hong (7103007678); Liu, Xuanzhe (22035687800); Jin, Xin (57189270771)","57236553800; 58080542000; 55808012300; 57220845527; 7103007678; 22035687800; 57189270771","Disaggregated RAID Storage in Modern Datacenters","2023","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","3","","","147","163","16","11","10.1145/3582016.3582027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159302338&doi=10.1145%2f3582016.3582027&partnerID=40&md5=2f916473576f224d8f1eae84755bba6a","RAID (Redundant Array of Independent Disks) has been widely adopted for decades, as it provides enhanced throughput and redundancy beyond what a single disk can offer. Today, enabled by fast datacenter networks, accessing remote block devices with acceptable overhead (i.e. disaggregated storage) becomes a reality (e.g., for serverless applications). Combining RAID with remote storage can provide the same benefits while creating better fault tolerance and flexibility than its monolithic counterparts. The key challenge of disaggregated RAID is to handle extra network traffic generated by RAID, which can consume a vast amount of NIC bandwidth. We present dRAID, a disaggregated RAID system that achieves near-optimal read and write throughput. dRAID exploits peer-to-peer disaggregated data access to reduce bandwidth consumption in both normal and degraded states. It employs non-blocking multi-stage writes to maximize inter-node parallelism, and applies pipelined I/O processing to maximize inter-device parallelism. We introduce bandwidth-aware reconstruction for better load balancing. We show that dRAID provides up to 3× bandwidth improvement. The results on a lightweight object store show that dRAID brings 1.5×-2.35× throughput improvement on various workloads. © 2023 ACM.","Disaggregated Storage; NVMe-oF; RAID; RDMA","Digital storage; Fault tolerance; Peer to peer networks; Pipeline processing systems; Block devices; Data center networks; Datacenter; Disaggregated storage; Monolithic counterparts; Network traffic; NVMe-of; RDMA; Redundant array of independent disks; Remote storage; Bandwidth","Association for Computing Machinery","et al.; FURIOSA; Huawei; IMO Ventures; MANGOBOOST; Qualcomm","28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2023","25 March 2023 through 29 March 2023","Vancouver","187381","Conference paper","Final","","Scopus","2-s2.0-85159302338"
"Lambion D.; Schmitz R.; Cordingly R.; Heydari N.; Lloyd W.","Lambion, Danielle (57222723781); Schmitz, Robert (57572361900); Cordingly, Robert (57220806485); Heydari, Navid (57223141628); Lloyd, Wes (8537012400)","57222723781; 57572361900; 57220806485; 57223141628; 8537012400","Characterizing x86 and arm serverless performance variation: A natural language processing case study","2022","ICPE 2022 - Companion of the 2022 ACM/SPEC International Conference on Performance Engineering","","","","69","75","6","8","10.1145/3491204.3543506","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135986451&doi=10.1145%2f3491204.3543506&partnerID=40&md5=1b7534dad9ec2aba68887a1c34317011","In this paper, we leverage a Natural Language Processing (NLP) pipeline for topic modeling consisting of three functions for data preprocessing, model training, and inferencing to analyze serverless platform performance variation. Specifically, we investigated performance using x8664 and ARM64 processors over a 24-hour day starting at midnight local time on four cloud regions across three continents on AWS Lambda. We identified public cloud resource contention by leveraging the CPU steal metric, and examined relationships to NLP pipeline runtime. Intel x86_64 Xeon processors at the same clock rate as ARM64 processors (Graviton 2) were more than 23% faster for model training, but ARM64 processors were faster for data preprocessing and inferencing. Use of the Intel x8664 architecture for the NLP pipeline was up to 33.4% more expensive than ARM64 as a result of incentivized pricing from the cloud provider and slower pipeline runtime due to greater resource contention for Intel processors.  © 2022 Owner/Author.","function-as-a-service; performance variation; resource contention; serverless computing; topic modeling","ARM processors; Modeling languages; Natural language processing systems; Pipeline processing systems; Data preprocessing; Function-as-a-service; Language processing; Model training; Natural languages; Performance variations; Resource contention; Runtimes; Serverless computing; Topic Modeling; Pipelines","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC","13th Annual ACM/SPEC International Conference on Performance Engineering, ICPE 2022","9 April 2022 through 13 April 2022","Virtual, Online","181030","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85135986451"
"Fuerst A.; Rehman A.; Sharma P.","Fuerst, Alexander (57218225304); Rehman, Abdul (58560104800); Sharma, Prateek (58273575200)","57218225304; 58560104800; 58273575200","Ilúvatar: A Fast Control Plane for Serverless Computing","2023","HPDC 2023 - Proceedings of the 32nd International Symposium on High-Performance Parallel and Distributed Computing","","","","267","280","13","7","10.1145/3588195.3592995","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169620742&doi=10.1145%2f3588195.3592995&partnerID=40&md5=bc5cb16b59013ecd148b29c3344731bd","Providing efficient Functions as a Service (FaaS) is challenging due to the serverless programming model and highly heterogeneous and dynamic workloads. Great strides have been made in optimizing FaaS performance through scheduling, caching, virtualization, and other resource management techniques. The combination of these advances and growing FaaS workloads have pushed the performance bottleneck into the control plane itself. Current FaaS control planes like OpenWhisk introduce 100s of milliseconds of latency overhead, and are becoming unsuitable for high performance FaaS research and deployments. We present the design and implementation of Ilúvatar, a fast, modular, extensible FaaS control plane which reduces the latency overhead by more than two orders of magnitude. Ilúvatar has a worker-centric architecture and introduces a new function queue technique for managing function scheduling and overcommitment. Ilúvatar is implemented in Rust in about 13,000 lines of code, and introduces only 3ms of latency overhead under a wide range of loads, which is more than 2 orders of magnitude lower than OpenWhisk.  © 2023 ACM.","cloud computing; functions as a service; open source; serverless computing","Open systems; Cloud-computing; Control planes; Fast control; Function as a service; Open-source; Optimizing functions; Orders of magnitude; Programming models; Serverless computing; Service control; Open source software","Association for Computing Machinery, Inc","ACM SIGARCH; ACM SIGHPC","32nd International Symposium on High-Performance Parallel and Distributed Computing, HPDC 2023","16 June 2023 through 23 June 2023","Orlando","191522","Conference paper","Final","","Scopus","2-s2.0-85169620742"
"Chen C.; Nagel L.; Cui L.; Tso F.P.","Chen, Chen (57393198600); Nagel, Lars (35590499600); Cui, Lin (46960891100); Tso, Fung Po (23494172300)","57393198600; 35590499600; 46960891100; 23494172300","S-Cache: Function Caching for Serverless Edge Computing","2023","EdgeSys 2023 - Proceedings of the 6th International Workshop on Edge Systems, Analytics and Networking, Part of EuroSys 2023","","","","1","6","5","17","10.1145/3578354.3592865","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159351879&doi=10.1145%2f3578354.3592865&partnerID=40&md5=f239ea7e98b4d5c1f872967155355f46","Serverless edge computing uses an event-driven model in which Internet-of-Things (IoT) services are run in short-lived, stateless containers only when invoked, leading to significant reduction of resource utilization. However, a cold-start of a container can take up to several seconds which significantly degrades the response time of serverless applications. Container caching can mitigate the cold-start problem at the cost of extra computing resources which violates the spirit of serverless computing. Therefore, we need to balance the cold-start overheads with the extra resource utilization for serverless edge computing. Nevertheless, the diverse ranges of containers lead to different cold-start overheads, resource consumption and invocation frequencies and these characteristics of containers are largely overlooked by existing caching policies. In this paper, we study the request distribution and caching problem for serverless edge computing. We devise an online request distribution algorithm with performance guarantee and present an adaptive caching policy which incorporates container frequency, container size and cold-start time. Via real-system implementation, the superiority of the proposed algorithm is verified by comparing with existing caching policies, including fixed caching and histogram based policies. Our results show that the proposed algorithm reduces both the average response time and cold-start frequency by a factor of 3 compared to current approaches.  © 2023 Owner/Author(s).","caching; function as a service; serverless computing","Edge computing; Internet of things; % reductions; Caching; Caching policy; Cold-start; Edge computing; Event driven model; Function as a service; Request distributions; Resources utilizations; Serverless computing; Containers","Association for Computing Machinery, Inc","ACM SIGOPS","6th International Workshop on Edge Systems, Analytics and Networking, EdgeSys 2023, in conjunction with ACM EuroSys 2023","8 May 2023","Rome","188212","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85159351879"
"Kousiouris G.; Pnevmatikakis A.","Kousiouris, George (26031026200); Pnevmatikakis, Aristodemos (6507698316)","26031026200; 6507698316","Performance Experiences From Running An E-health Inference Process As FaaS Across Diverse Clusters","2023","ICPE 2023 - Companion of the 2023 ACM/SPEC International Conference on Performance Engineering","","","","289","295","6","5","10.1145/3578245.3585023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158835493&doi=10.1145%2f3578245.3585023&partnerID=40&md5=a245b37897367b482b8aefe1536d88be","In this paper we report our experiences from the migration of an AI model inference process, used in the context of an E-health platform to the Function as a Service model. To that direction, a performance analysis is applied, across three available Cloud or Edge FaaS clusters based on the open source Apache Openwhisk FaaS platform. The aim is to highlight differences in performance based on the characteristics of each cluster, the request rates and the parameters of Openwhisk. The conclusions can be applied for understanding the expected behavior of the inference function in each of these clusters as well as the effect of the Openwhisk execution model. Key observations and findings are reported on aspects such as function execution duration, function sizing, wait time in the system, network latency and concurrent container overheads for different load rates. These can be used to detect in a black box manner capabilities of unknown clusters, guide or fine-tune performance models as well as private cloud FaaS deployment setup. © 2023 Owner/Author.","benchmarking; cloud; edge; function as a service; model inference; performance analysis","eHealth; Cluster-based; E health; Edge; Function as a service; Inference process; Model inference; Open-source; Performance; Performances analysis; Service modeling; Benchmarking","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC Research","14th Annual ACM/SPEC International Conference on Performance Engineering, ICPE 2023","15 April 2023 through 19 April 2023","Coimbra","187962","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85158835493"
"Gregory Pauloski J.; Hayot-Sasson V.; Ward L.; Hudson N.; Sabino C.; Baughman M.; Chard K.; Foster I.","Gregory Pauloski, J. (57208001241); Hayot-Sasson, Valerie (57202280425); Ward, Logan (55556371700); Hudson, Nathaniel (57220489824); Sabino, Charlie (58352689400); Baughman, Matt (57202993030); Chard, Kyle (9132950200); Foster, Ian (35572232000)","57208001241; 57202280425; 55556371700; 57220489824; 58352689400; 57202993030; 9132950200; 35572232000","Accelerating Communications in Federated Applications with Transparent Object Proxies","2023","International Conference for High Performance Computing, Networking, Storage and Analysis, SC","","","","","","","5","10.1145/3581784.3607047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174320825&doi=10.1145%2f3581784.3607047&partnerID=40&md5=9044d36e145241e889955c1bcee9d074","Advances in networks, accelerators, and cloud services encourage programmers to reconsider where to compute-such as when fast networks make it cost-effective to compute on remote accelerators despite added latency. Workflow and cloud-hosted serverless computing frameworks can manage multi-step computations spanning federated collections of cloud, high-performance computing (HPC), and edge systems, but passing data among computational steps via cloud storage can incur high costs. Here, we overcome this obstacle with a new programming paradigm that decouples control flow from data flow by extending the pass-by-reference model to distributed applications. We describe ProxyStore, a system that implements this paradigm by providing object proxies that act as wide-area object references with just-in-time resolution. This proxy model enables data producers to communicate data unilaterally, transparently, and efficiently to both local and remote consumers. We demonstrate the benefits of this model with synthetic bench-marks and real-world scientific applications, running across various computing platforms. © 2023 ACM.","Data Communication and Storage; Distributed Computing; Federated Computing; Open-source Software; Python","Application programs; Cloud storage; Cost effectiveness; Information management; Open source software; Open systems; Cloud services; Computing frameworks; Cost effective; Data storage; Data-communication; Federated computing; In-network services; Open-source softwares; Transparent objects; Work-flows; Python","IEEE Computer Society","ACM's Special Interest Group on High Performance Computing (SIGHPC); Association for Computing Machinery (ACM); IEEE�s Technical Committee on High Performance Computing (TCHPC); Institute of Electrical and Electronics Engineers (IEEE) Computer Society","2023 International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2023","11 November 2023 through 17 November 2023","Denver","198645","Conference paper","Final","","Scopus","2-s2.0-85174320825"
"Baughman M.; Hudson N.; Chard R.; Bauer A.; Foster I.; Chard K.","Baughman, Matt (57202993030); Hudson, Nathaniel (57220489824); Chard, Ryan (55588066400); Bauer, Andre (57194177145); Foster, Ian (35572232000); Chard, Kyle (9132950200)","57202993030; 57220489824; 55588066400; 57194177145; 35572232000; 9132950200","Tournament-Based Pretraining to Accelerate Federated Learning","2023","ACM International Conference Proceeding Series","","","","109","115","6","2","10.1145/3624062.3626089","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178131110&doi=10.1145%2f3624062.3626089&partnerID=40&md5=0adcc94e8a37f2a6597b097ea2696f53","Advances in hardware, proliferation of compute at the edge, and data creation at unprecedented scales have made federated learning (FL) necessary for the next leap forward in pervasive machine learning. For privacy and network reasons, large volumes of data remain stranded on endpoints located in geographically austere (or at least austere network-wise) locations. However, challenges exist to the effective use of these data. To solve the system and functional level challenges, we present an three novel variants of a serverless federated learning framework. We also present tournament-based pretraining, which we demonstrate significantly improves model performance in some experiments. Overall, these extensions to FL and our novel training method enable greater focus on science rather than ML development. © 2023 ACM.","","Data creation; Functional levels; Large volumes; Learning frameworks; Machine-learning; Modeling performance; Pre-training; System levels; Training methods; Learning systems","Association for Computing Machinery","ACM's Special Interest Group on High Performance Computing (SIGHPC); Association for Computing Machinery; IEEE�s Technical Committee on High Performance Computing (TCHPC); Institute of Electrical and Electronics Engineers (IEEE) Computer Society","2023 International Conference on High Performance Computing, Network, Storage, and Analysis, SC Workshops 2023","12 November 2023 through 17 November 2023","Denver","194341","Conference paper","Final","","Scopus","2-s2.0-85178131110"
"Ao L.; Porter G.; Voelker G.M.","Ao, Lixiang (56482929900); Porter, George (36442743000); Voelker, Geoffrey M. (7003306507)","56482929900; 36442743000; 7003306507","FaaSnap: FaaS Made Fast Using Snapshot-based VMs","2022","EuroSys 2022 - Proceedings of the 17th European Conference on Computer Systems","","","","730","746","16","38","10.1145/3492321.3524270","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128026673&doi=10.1145%2f3492321.3524270&partnerID=40&md5=7b487f971d7398e8ac367d0f5f076bd4","FaaSnap is a VM snapshot-based platform that uses a set of complementary optimizations to improve function cold-start performance for Function-as-a-Service (FaaS) applications. Compact loading set files take better advantage of prefetching. Per-region memory mapping tailors page fault handling depending on the contents of different guest VM memory regions. Hierarchical overlapping memory-mapped regions simplify the mapping process. Concurrent paging allows the guest VM to start execution immediately, rather than pausing until the working set is loaded. Altogether, FaaSnap significantly reduces guest VM page fault handling time on the critical path and improves overall function loading performance. Experiments on serverless benchmarks show that it reduces end-to-end function execution by up to 3.5x compared to state-of-the-art, and on average is only 3.5% slower than snapshots cached in memory. Moreover, we show that FaaSnap is resilient to changes of working set and remains efficient under bursty workloads and when snapshots are located in remote storage.  © 2022 Owner/Author.","Caching; Cloud computing; Cold Starts; FaaS; Serverless; Snapshots; Virtualization","Cloud computing; Mapping; Virtual machine; Caching; Cloud-computing; Cold-start; Fault handling; Function-as-a-service; Optimisations; Serverless; Snapshot; Virtualizations; Working set; Virtualization","Association for Computing Machinery, Inc","ACM SIGOPS","17th European Conference on Computer Systems, EuroSys 2022","5 April 2022 through 8 April 2020","Rennes","178105","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85128026673"
"Carlini E.; Kavalionak H.; Dazzi P.; Ferrucci L.; Coppola M.; Mordacchini M.","Carlini, Emanuele (35316758800); Kavalionak, Hanna (55508167300); Dazzi, Patrizio (57193263850); Ferrucci, Luca (55356728900); Coppola, Massimo (16174598400); Mordacchini, Matteo (14632521700)","35316758800; 55508167300; 57193263850; 55356728900; 16174598400; 14632521700","Network Measurements with Function-As-A-Service for Distributed Low-latency Edge Applications","2022","FRAME 2022 - Proceedings of the 2nd Workshop on Flexible Resource and Application Management on the Edge, co-located with HPDC 2022","","","","25","28","3","2","10.1145/3526059.3533622","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134484890&doi=10.1145%2f3526059.3533622&partnerID=40&md5=b351f02e2aefb46cbaa05f24bfa4cd07","Edge computing promises to bring computation and storage close to end-users, opening exciting new areas of improvement for applications with a high level of interactivity and requiring low latency. However, these improvements require careful scheduling of applications in the correct Edge resource. This decision is generally taken by considering multiple parameters, including the network capabilities. In this paper, we discuss an approach that measures latency and bandwidth between multiple clients and Edge servers. The approach is based on recent Serverless computing technologies, and it is meant as a support to take timely and correct scheduling decisions in the Edge. We also provide the description of a proof of concept implementation of the said approach.  © 2022 ACM.","edge computing; latency measurement; serverless computing","Scheduling; Edge computing; Edge resources; End-users; Interactivity; Latency measurements; Low latency; Multiple parameters; Network capability; Network measurement; Serverless computing; Edge computing","Association for Computing Machinery, Inc","ACM SIGARCH","2nd Workshop on Flexible Resource and Application Management on the Edge, FRAME 2022, held in conjunction with the HPDC 2022 Conference","1 July 2022","Virtual, Online","180411","Conference paper","Final","","Scopus","2-s2.0-85134484890"
"","","","WoC 2022 - Proceedings of the 8th International Workshop on Container Technologies and Container Clouds, Part of Middleware 2022","2022","WoC 2022 - Proceedings of the 8th International Workshop on Container Technologies and Container Clouds, Part of Middleware 2022","","","","","","27","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143224488&partnerID=40&md5=1714fa0440775508f8785c5639bf6747","The proceedings contain 3 papers. The topics discussed include: accelerating automation of digital health applications via cloud native approach; engram: the one security platform for modern software supply chain risks; and Floki: a proactive data forwarding system for direct inter-function communication for serverless workflows.","","","Association for Computing Machinery, Inc","ACM; ETS; Raytheon BBN; rti","8th International Workshop on Container Technologies and Container Clouds, WoC 2022 - Part of Middleware 2022","7 November 2022","Quebec","184464","Conference review","Final","","Scopus","2-s2.0-85143224488"
"Cordingly R.; Lloyd W.","Cordingly, Robert (57220806485); Lloyd, Wes (8537012400)","57220806485; 8537012400","Faaset: A jupyter notebook to streamline every facet of serverless development","2022","ICPE 2022 - Companion of the 2022 ACM/SPEC International Conference on Performance Engineering","","","","49","52","3","2","10.1145/3491204.3527464","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135977102&doi=10.1145%2f3491204.3527464&partnerID=40&md5=4737b62f155f92ef4724200ec997e982","Function-as-a-Service platforms require developers to use many different tools and services for function development, packaging, deployment, debugging, testing, orchestration of experiments, and analysis of results. Diverse toolchains are necessary due to the differences in how each platform is designed, the technologies they support, and the APIs they provide, leading to usability challenges for developers. To combine support for all of the tasks and tools into a unified workspace, we created the FaaS Experiment Toolkit (FaaSET). At the core of FaaSET is a Jupyter notebook development environment that enables developers to write functions, deploy them across multiple platforms, invoke and test them, automate experiments, and perform data analysis all in a single environment.  © 2022 Owner/Author.","development; function-as-a-service; jupyter; profiling; serverless; tools","Development; Development environment; Experiment and analysis; Function-as-a-service; Jupyter; Multiple platforms; Profiling; Serverless; Service platforms; Write functions; Program debugging","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC","13th Annual ACM/SPEC International Conference on Performance Engineering, ICPE 2022","9 April 2022 through 13 April 2022","Virtual, Online","181030","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85135977102"
"Giantsidi D.; Giortamis E.; Tornow N.; Dinu F.; Bhatotia P.","Giantsidi, Dimitra (57210601667); Giortamis, Emmanouil (58560424600); Tornow, Nathaniel (58560110300); Dinu, Florin (36463390700); Bhatotia, Pramod (36456669900)","57210601667; 58560424600; 58560110300; 36463390700; 36456669900","FlexLog: A Shared Log for Stateful Serverless Computing","2023","HPDC 2023 - Proceedings of the 32nd International Symposium on High-Performance Parallel and Distributed Computing","","","","195","209","14","3","10.1145/3588195.3592993","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169545990&doi=10.1145%2f3588195.3592993&partnerID=40&md5=fc2307b1b7eadc1d8d5665af7b4597ab","Stateful serverless applications need to persist their state and data. The existing approach is to store the data in general purpose storage systems. However, these approaches are not designed to meet the demands of serverless applications in terms of consistency, fault tolerance and performance. We present FlexLog, a storage system, specifically a distributed shared log, distinctively designed to meet the requirements of stateful serverless computing while mitigating the relevant system bottlenecks. FlexLog's data layer leverages the state-of-the-art persistent memory (PM) to offer low latency I/O and improve performance. To match the performance, FlexLog's ordering layer employs a scalable design, namely a tree-structure set of sequencer nodes. Importantly, this design provides serverless applications with the flexibility to implement different consistency guarantees and to seamlessly support multi-tenancy configurations. We implement FlexLog from the ground up on a real hardware testbed and we also prove the correctness of our protocols. In particular, we evaluate FlexLog on a cluster of 6 machines with 800 GB Intel Optane DC PM over a 10 Gbps interconnect. Our evaluation shows that FlexLog scales to millions of operations per second while maintaining minimal latency. Our comparison with the state-of-the-art shared log for serverless, Boki, shows that we achieve 10x better throughput in the storage layer and 2x-4x lower latency in the ordering layer, while also providing flexibility to support different consistency properties and multi-tenancy.  © 2023 ACM.","persistent memory; serverless computing; shared distributed log","Digital storage; Distributed computer systems; Trees (mathematics); Data layer; Improve performance; Low latency; Multi tenancies; Performance; Persistent memory; Serverless computing; Shared distributed log; State of the art; Storage systems; Fault tolerance","Association for Computing Machinery, Inc","ACM SIGARCH; ACM SIGHPC","32nd International Symposium on High-Performance Parallel and Distributed Computing, HPDC 2023","16 June 2023 through 23 June 2023","Orlando","191522","Conference paper","Final","","Scopus","2-s2.0-85169545990"
"Li Y.; Zhao L.; Yang Y.; Qu W.","Li, Yiming (57212459721); Zhao, Laiping (35243865000); Yang, Yanan (57208671879); Qu, Wenyu (8917789900)","57212459721; 35243865000; 57208671879; 8917789900","Rethinking Deployment for Serverless Functions: A Performance-first Perspective","2023","International Conference for High Performance Computing, Networking, Storage and Analysis, SC","","","","","","","2","10.1145/3581784.3613211","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190478900&doi=10.1145%2f3581784.3613211&partnerID=40&md5=d15510c5f1485dfd083f3dc1d9dd073b","Serverless computing commonly adopts strong isolation mechanisms for deploying functions, which may bring significant performance overhead because each function needs to run in a completely new environment (i.e., the 'one-to-one' model). To accelerate the function computation, prior work has proposed using sand-box sharing to reduce the overhead, i.e., the 'many-to-one' model. Nonetheless, either process-based true parallelism or thread-based pseudo-parallelism still causes high latency, preventing its adaptation for latency-sensitive web services. To achieve optimal performance and resource efficiency for server-less workflow, we argue an 'm-to-n' deployment model that manipulates multiple granularities of computing abstractions such as processes, threads, and sandboxes to amortize overhead. We propose wrap, a new deployment abstraction that balances the trade-offs between interaction overhead, startup overhead and function execution. We further design Chiron, a wrap-based deployment manager that can automatically perform the orchestration of multiple computing abstractions based on performance prioritization. Our comprehensive evaluation indicates that Chiron outperforms state-of-the-art systems by 1.3×-21.8× on system throughput. © 2023 ACM.","Deployment Model; Graph Partition; Serverless Workflows","Abstracting; Economic and social effects; Deployment models; Function computations; Graph partition; Many-to-one; Performance; Process-based; Sand boxes; Serverless workflow; Webs services; Work-flows; Web services","IEEE Computer Society","ACM's Special Interest Group on High Performance Computing (SIGHPC); Association for Computing Machinery (ACM); IEEE�s Technical Committee on High Performance Computing (TCHPC); Institute of Electrical and Electronics Engineers (IEEE) Computer Society","2023 International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2023","11 November 2023 through 17 November 2023","Denver","198645","Conference paper","Final","","Scopus","2-s2.0-85190478900"
"Babuji Y.; Chard K.; Foster I.; Li Z.","Babuji, Yadu (57193607019); Chard, Kyle (9132950200); Foster, Ian (35572232000); Li, Zhuozhao (56275944000)","57193607019; 9132950200; 35572232000; 56275944000","FRAME 2022: The 2nd Workshop on Flexible Resource and Application Management on the Edge","2022","HPDC 2022 - Proceedings of the 31st International Symposium on High-Performance Parallel and Distributed Computing","","","","290","291","1","0","10.1145/3502181.3535104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134192942&doi=10.1145%2f3502181.3535104&partnerID=40&md5=9938db91f8620e292ee091c575d465fc","Serverless computing presents an attractive model for general distributed computing as it focuses on abstracting the infrastructure required to execute an application. This workshop investigates the intersection between high performance computing and serverless computing, looking both at the high performance and distributed systems used to deliver serverless platforms and also at the use of serverless models for high performance and distributed systems. © 2022 Owner/Author.","cloud computing; distributed systems; serverless","Application management; Cloud-computing; Distributed systems; Flexible applications; Flexible resource managements; High performance systems; It focus; Performance computing; Serverless; Computer programming","Association for Computing Machinery, Inc","ACM SIGARCH","31st International Symposium on High-Performance Parallel and Distributed Computing, HPDC 2022","27 June 2022 through 30 June 2022","Virtual, Online","180402","Conference paper","Final","","Scopus","2-s2.0-85134192942"
"Kaffes K.; Yadwadkar N.J.; Kozyrakis C.","Kaffes, Kostis (57212510735); Yadwadkar, Neeraja J. (56429519900); Kozyrakis, Christos (6602525246)","57212510735; 56429519900; 6602525246","Hermod: Principled and Practical Scheduling for Serverless Functions","2022","SoCC 2022 - Proceedings of the 13th Symposium on Cloud Computing","","","","289","305","16","43","10.1145/3542929.3563468","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143254731&doi=10.1145%2f3542929.3563468&partnerID=40&md5=807b795a0ada8518432c3d6a9f7e76b9","Serverless computing has seen rapid growth due to the ease-of-use and cost-efficiency it provides. However, function scheduling, a critical component of serverless systems, has been overlooked. In this paper, we take a fist-principles approach toward designing a scheduler that caters to the unique characteristics of serverless functions as seen in real-world deployments. We first create a taxonomy of scheduling policies along three dimensions. Next, we use simulation to explore the scheduling policy space and show that frequently used features such as late binding and random load balancing are sub-optimal for common execution time distributions and load ranges. We use these insights to design Hermod, a scheduler for serverless functions with two key characteristics. First, to avoid head-of-line blocking due to high function execution time variability, Hermod uses a combination of early binding and processor sharing for scheduling at individual worker machines. Second, Hermod is cost, load, and locality-aware. It improves consolidation at low load, it employs least-loaded balancing at high load to retain high performance, and it reduces the number of cold starts compared to pure load-based policies. We implement Hermod for Apache OpenWhisk and demonstrate that, for the case of the function patterns observed in real-world traces, it achieves up to 85% lower function slowdown and 60% higher throughput compared to existing production and state-of-the-art research schedulers.  © 2022 ACM.","cloud computing; scheduling; serverless","Balancing; Throughput; Cloud-computing; Cost-efficiency; Critical component; Ease-of-use; Rapid growth; Real world deployment; Scheduling policies; Serverless; Serverless systems; Use efficiency; Scheduling","Association for Computing Machinery, Inc","ACM Special Interest Grou on Operating Systems (SIGOPS); ACM Special Interest Group on Management of Data (SIGMOD); EBay; et al.; FutureWei; Microsoft","13th Annual ACM Symposium on Cloud Computing, SoCC 2022","7 November 2022 through 11 November 2022","San Francisco","184095","Conference paper","Final","","Scopus","2-s2.0-85143254731"
"","","","FRAME 2022 - Proceedings of the 2nd Workshop on Flexible Resource and Application Management on the Edge, co-located with HPDC 2022","2022","FRAME 2022 - Proceedings of the 2nd Workshop on Flexible Resource and Application Management on the Edge, co-located with HPDC 2022","","","","","","50","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134515260&partnerID=40&md5=2dca51f582ab324b7cc225d81712760a","The proceedings contain 8 papers. The topics discussed include: towards a distributed storage framework for edge computing infrastructures; a minicloud specification enabling the federation of heterogeneous edge resources for latency sensitive applications’ requirements; an automated pipeline for advanced fault tolerance in edge computing infrastructures; network measurements with function-as-a-service for distributed low-latency edge applications; a mathematical model for latency constrained self-organizing application placement in the edge; a novel approach to distributed model aggregation using Apache Kafka; and CoTree: region-free and decentralized edge server cooperation.","","","Association for Computing Machinery, Inc","ACM SIGARCH","2nd Workshop on Flexible Resource and Application Management on the Edge, FRAME 2022, held in conjunction with the HPDC 2022 Conference","1 July 2022","Virtual, Online","180411","Conference review","Final","","Scopus","2-s2.0-85134515260"
"Fuerst A.; Sharma P.","Fuerst, Alexander (57218225304); Sharma, Prateek (58273575200)","57218225304; 58273575200","Locality-aware Load-Balancing for Serverless Clusters","2022","HPDC 2022 - Proceedings of the 31st International Symposium on High-Performance Parallel and Distributed Computing","","","","227","239","12","27","10.1145/3502181.3531459","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134166307&doi=10.1145%2f3502181.3531459&partnerID=40&md5=b5d361331b49abc004061dc068a0a34d","While serverless computing provides more convenient abstractions for developing and deploying applications, the Function-as-a-Service (FaaS) programming model presents new resource management challenges for the FaaS provider. In this paper, we investigate load-balancing policies for serverless clusters. Locality, i.e., running repeated invocations of a function on the same server, is a key determinant of performance because it increases warm-starts and reduces cold-start overheads. We find that the locality vs. load tradeoff is crucial and presents a large design space. We enhance consistent hashing for FaaS, and develop CH-RLU: Consistent Hashing with Random Load Updates, a simple practical load-balancing policy which provides more than 2x reduction in function latency. Our policy deals with highly heterogeneous, skewed, and bursty function workloads, and is a drop-in replacement for OpenWhisk's existing load-balancer. We leverage techniques from caching such as SHARDS for popularity detection, and develop a new approach that places functions based on a tradeoff between locality, load, and randomness. © 2022 ACM.","","Consistent hashing; Key determinants; Load balancing policies; Load-Balancing; Locality aware; Management challenges; Performance; Programming models; Resource management; Service provider","Association for Computing Machinery, Inc","ACM SIGARCH","31st International Symposium on High-Performance Parallel and Distributed Computing, HPDC 2022","27 June 2022 through 30 June 2022","Virtual, Online","180402","Conference paper","Final","","Scopus","2-s2.0-85134166307"
"Joosen A.; Hassan A.; Asenov M.; Singh R.; Darlow L.; Wang J.; Barker A.","Joosen, Artjom (58743605000); Hassan, Ahmed (58743078800); Asenov, Martin (57208301443); Singh, Rajkarn (56297011800); Darlow, Luke (56543450800); Wang, Jianfeng (58855387700); Barker, Adam (56066309500)","58743605000; 58743078800; 57208301443; 56297011800; 56543450800; 58855387700; 56066309500","How Does It Function? Characterizing Long-term Trends in Production Serverless Workloads","2023","SoCC 2023 - Proceedings of the 2023 ACM Symposium on Cloud Computing","","","","443","458","15","31","10.1145/3620678.3624783","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178515850&doi=10.1145%2f3620678.3624783&partnerID=40&md5=56f937a52df8dbd8f9976e121a4b16f7","This paper releases and analyzes two new Huawei cloud serverless traces. The traces span a period of over 7 months with over 1.4 trillion function invocations combined. The first trace is derived from Huawei’s internal workloads and contains detailed per-second statistics for 200 functions running across multiple Huawei cloud data centers. The second trace is a representative workload from Huawei’s public FaaS platform. This trace contains per-minute arrival rates for over 5000 functions running in a single Huawei data center. We present the internals of a production FaaS platform by characterizing resource consumption, cold-start times, programming languages used, periodicity, per-second versus per-minute burstiness, correlations, and popularity. Our findings show that there is considerable diversity in how serverless functions behave: requests vary by up to 9 orders of magnitude across functions, with some functions executed over 1 billion times per day; scheduling time, execution time and cold-start distributions vary across 2 to 4 orders of magnitude and have very long tails; and function invocation counts demonstrate strong periodicity for many individual functions and on an aggregate level. Our analysis also highlights the need for further research in estimating resource reservations and time-series prediction to account for the huge diversity in how serverless functions behave. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.","cloud; datasets; neural networks; serverless; time series","Time series analysis; Arrival rates; Cloud data centers; Cold-start; Dataset; It functions; Long-term trend; Neural-networks; Orders of magnitude; Serverless; Times series; Time series","Association for Computing Machinery, Inc","ACM SIGMOD; ACM SIGOPS; ActiveLoop; Google; IBM; Microsoft","14th ACM Symposium on Cloud Computing, SoCC 2023","30 October 2023 through 1 November 2023","Santa Cruz","194526","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85178515850"
"Khandelwal A.; Tang Y.; Agarwal R.; Akella A.; Stoica I.","Khandelwal, Anurag (57189265710); Tang, Yupeng (57226779139); Agarwal, Rachit (12345377700); Akella, Aditya (8383144400); Stoica, Ion (7007009125)","57189265710; 57226779139; 12345377700; 8383144400; 7007009125","Jiffy: Elastic Far-Memory for Stateful Serverless Analytics","2022","EuroSys 2022 - Proceedings of the 17th European Conference on Computer Systems","","","","697","713","16","35","10.1145/3492321.3527539","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128056353&doi=10.1145%2f3492321.3527539&partnerID=40&md5=25e943e8be5f49e89ee57b6979aa3914","Stateful serverless analytics can be enabled using a remote memory system for inter-task communication, and for storing and exchanging intermediate data. However, existing systems allocate memory resources at job granularity-jobs specify their memory demands at the time of the submission; and, the system allocates memory equal to the job's demand for the entirety of its lifetime. This leads to resource underutilization and/or performance degradation when intermediate data sizes vary during job execution. This paper presents Jiffy, an elastic far-memory system for stateful serverless analytics that meets the instantaneous memory demand of a job at seconds timescales. Jiffy efficiently multiplexes memory capacity across concurrently running jobs, reducing the overheads of reads and writes to slower persistent storage, resulting in 1.6-2.5× improvements in job execution time over production workloads. Jiffy implementation currently runs on Amazon EC2, enables a wide variety of distributed programming models including MapReduce, Dryad, StreamScope, and Piccolo, and natively supports a large class of analytics applications on AWS Lambda.  © 2022 ACM.","Data analytics; Far-Memory; Function-as-a-Service; Serverless computing","Digital storage; MapReduce; Data analytics; Existing systems; Far-memory; Function-as-a-service; Is-enabled; Job execution; Memory resources; Memory systems; Remote memory; Serverless computing; Data Analytics","Association for Computing Machinery, Inc","ACM SIGOPS","17th European Conference on Computer Systems, EuroSys 2022","5 April 2022 through 8 April 2020","Rennes","178105","Conference paper","Final","","Scopus","2-s2.0-85128056353"
"Gu D.; Zhao Y.; Zhong Y.; Xiong Y.; Han Z.; Cheng P.; Yang F.; Huang G.; Jin X.; Liu X.","Gu, Diandian (57211949418); Zhao, Yihao (57210290574); Zhong, Yinmin (58096709800); Xiong, Yifan (57440627400); Han, Zhenhua (56452084200); Cheng, Peng (58040448600); Yang, Fan (57214194267); Huang, Gang (57220845527); Jin, Xin (57189270771); Liu, Xuanzhe (22035687800)","57211949418; 57210290574; 58096709800; 57440627400; 56452084200; 58040448600; 57214194267; 57220845527; 57189270771; 22035687800","ElasticFlow: An Elastic Serverless Training Platform for Distributed Deep Learning","2023","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","2","","","266","280","14","25","10.1145/3575693.3575721","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147735264&doi=10.1145%2f3575693.3575721&partnerID=40&md5=c48d43207cda8a432de14a0555914b79","This paper proposes ElasticFlow, an elastic serverless training platform for distributed deep learning. ElasticFlow provides a serverless interface with two distinct features: (i) users specify only the deep neural network (DNN) model and hyperparameters for a job, but not the number of GPUs; (ii) users specify the deadline for a job, but not the amount of time to occupy GPUs. In contrast to existing server-centric platforms, ElasticFlow provides performance guarantees in terms of meeting deadlines while alleviating tedious, low-level, and manual resource management for deep learning developers. The characteristics of distributed training introduce two challenges. First, the training throughput scales non-linearly with the number of GPUs. Second, the scaling efficiency is affected by worker placement. To address these challenges, we propose Minimum Satisfactory Share to capture the resource usage of training jobs to meet deadlines, and ElasticFlow performs admission control based on it. We develop a greedy algorithm that dynamically allocates resources to admitted jobs based on diminishing returns. We apply buddy allocation to worker placement to eliminate the effect of topology. Evaluation results on a cluster of 128 GPUs show that ElasticFlow increases the number of jobs that can meet their deadlines by 1.46-7.65× compared to existing solutions.  © 2023 ACM.","Cluster Scheduling; Distributed Deep Learning; GPU Cluster; Serverless Computing","Cluster computing; Deep neural networks; Program processors; Cluster scheduling; Distributed deep learning; GPU cluster; Hyper-parameter; Neural network model; Performance guarantees; Resource management; Serverless computing; Training platform; Workers'; Graphics processing unit","Association for Computing Machinery","ACM; ACM SIGARCH; ACM SIGOPS; ACM SIGPLAN","28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2023","25 March 2023 through 29 March 2023","Vancouver","186350","Conference paper","Final","","Scopus","2-s2.0-85147735264"
"Sahraei A.; Demetriou S.; Sobhgol A.; Zhang H.; Nagaraja A.; Pathak N.; Joshi G.; Souza C.; Huang B.; Cook W.; Golovei A.; Venkat P.; McFague A.; Skarlatos D.; Patel V.; Thind R.; Gonzalez E.; Jin Y.; Tang C.","Sahraei, Alireza (58698780800); Demetriou, Soteris (55949468400); Sobhgol, Amirali (58698384100); Zhang, Haoran (57219048671); Nagaraja, Abhigna (58698654600); Pathak, Neeraj (57196404367); Joshi, Girish (58698384200); Souza, Carla (58698253700); Huang, Bo (58698110000); Cook, Wyatt (58697971800); Golovei, Andrii (58698654700); Venkat, Pradeep (58698780900); McFague, Andrew (58697971900); Skarlatos, Dimitrios (58582074900); Patel, Vipul (58698384300); Thind, Ravinder (57322712800); Gonzalez, Ernesto (58697833200); Jin, Yun (58697833300); Tang, Chunqiang (8845243400)","58698780800; 55949468400; 58698384100; 57219048671; 58698654600; 57196404367; 58698384200; 58698253700; 58698110000; 58697971800; 58698654700; 58698780900; 58697971900; 58582074900; 58698384300; 57322712800; 58697833200; 58697833300; 8845243400","XFaaS: Hyperscale and Low Cost Serverless Functions at Meta","2023","SOSP 2023 - Proceedings of the 29th ACM Symposium on Operating Systems Principles","","","","231","246","15","21","10.1145/3600006.3613155","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176947577&doi=10.1145%2f3600006.3613155&partnerID=40&md5=c1ed47966a6bdd653e2ee85a80644279","Function-as-a-Service (FaaS) has become a popular programming paradigm in Serverless Computing. As the responsibility of resource provisioning shifts from users to cloud providers, the ease of use of FaaS for users may come at the expense of extra hardware costs for cloud providers. Currently, there is no report on how FaaS platforms address this challenge and the level of hardware utilization they achieve.This paper presents the FaaS platform called XFaaS in Meta's hyperscale private cloud. XFaaS currently processes trillions of function calls per day on more than 100,000 servers. We describe a set of optimizations that help XFaaS achieve a daily average CPU utilization of 66%. Based on our anecdotal knowledge, this level of utilization might be several times higher than that of typical FaaS platforms.Specifically, to eliminate the cold start time of functions, XFaaS strives to approximate the effect that every worker can execute every function immediately. To handle load spikes without over-provisioning resources, XFaaS defers the execution of delay-tolerant functions to off-peak hours and globally dispatches function calls across datacenter regions. To prevent functions from overloading downstream services, XFaaS uses a TCP-like congestion-control mechanism to pace the execution of functions.  © 2023 ACM.","cloud; FaaS; measurement; serverless; systems","Computer hardware; Cloud providers; Ease-of-use; Function calls; Function-as-a-service; Hardware cost; Low-costs; Programming paradigms; Serverless; Service platforms; System; Costs","Association for Computing Machinery, Inc","ACM SIGOPS in cooperation with USENIX","29th ACM Symposium on Operating Systems Principles, SOSP 2023","23 October 2023 through 26 October 2023","Koblenz","193843","Conference paper","Final","","Scopus","2-s2.0-85176947577"
"","","","ASPLOS 2023 - Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems","2022","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","","","","","","138","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145570627&partnerID=40&md5=501b9fe6e1bdfaf530d210e1645aad7c","The proceedings contain 9 papers. The topics discussed include: AQUATOPE: QoS-and-uncertainty-aware resource management for multi-stage serverless workflows; CAFQA: a classical simulation bootstrap for variational quantum algorithms; cooperative concurrency control for write-intensive key-value workloads; DecoMine: a compilation-based graph pattern mining system with pattern decomposition; Erms: efficient resource management for shared microservices with SLA guarantees; Glign: taming misaligned graph traversals in concurrent graph processing; overlap communication with dependent computation via decomposition in large deep learning models; risotto: a dynamic binary translator for weak memory model architectures; and TelaMalloc: efficient on-chip memory allocation for production machine learning accelerators.","","","Association for Computing Machinery","ACM; ACM SIGARCH; ACM SIGOPS; ACM SIGPLAN","28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2023","25 March 2023 through 29 March 2023","Vancouver","185370","Conference review","Final","","Scopus","2-s2.0-85145570627"
"Qiu H.; Mao W.; Patke A.; Wang C.; Franke H.; Kalbarczyk Z.T.; Başar T.; Iyer R.K.","Qiu, Haoran (57219785413); Mao, Weichao (57204468822); Patke, Archit (57216790460); Wang, Chen (57157809200); Franke, Hubertus (7202800815); Kalbarczyk, Zbigniew T. (6603680588); Başar, Tamer (7102588845); Iyer, Ravishankar K. (35597694000)","57219785413; 57204468822; 57216790460; 57157809200; 7202800815; 6603680588; 7102588845; 35597694000","Reinforcement learning for resource management in multi-Tenant serverless platforms","2022","EuroMLSys 2022 - Proceedings of the 2nd European Workshop on Machine Learning and Systems","","","","20","28","8","18","10.1145/3517207.3526971","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128360409&doi=10.1145%2f3517207.3526971&partnerID=40&md5=746f2c18bcd302ef65256edafa02a665","Serverless Function-As-A-Service (FaaS) is an emerging cloud computing paradigm that frees application developers from infrastructure management tasks such as resource provisioning and scaling. To reduce the tail latency of functions and improve resource utilization, recent research has been focused on applying online learning algorithms such as reinforcement learning (RL) to manage resources. Compared to existing heuristics-based resource management approaches, RL-based approaches eliminate humans in the loop and avoid the painstaking generation of heuristics. In this paper, we show that the state-of-The-Art single-Agent RL algorithm (S-RL) suffers up to 4.6x higher function tail latency degradation on multi-Tenant serverless FaaS platforms and is unable to converge during training. We then propose and implement a customized multi-Agent RL algorithm based on Proximal Policy Optimization, i.e., multi-Agent PPO (MA-PPO). We show that in multi-Tenant environments, MA-PPO enables each agent to be trained until convergence and provides online performance comparable to S-RL in single-Tenant cases with less than 10% degradation. Besides, MA-PPO provides a 4.4x improvement in S-RL performance (in terms of function tail latency) in multi-Tenant cases.  © 2022 ACM.","function-As-A-service; multi-Agent; reinforcement learning; resource allocation; serverless computing","Learning algorithms; Multi agent systems; Natural resources management; Reinforcement learning; Cloud-computing; Computing paradigm; Function-as-A-service; Multi agent; Multi tenants; Reinforcement learning algorithms; Resource management; Resources allocation; Serverless computing; Single-agent; Resource allocation","Association for Computing Machinery, Inc","ACM SIGOPS","2nd European Workshop on Machine Learning and Systems, EuroMLSys 2022, in conjunction with ACM EuroSys 2022","5 April 2022 through 8 April 2022","Virtual, Online","178630","Conference paper","Final","","Scopus","2-s2.0-85128360409"
"","","","EdgeSys 2022 - Proceedings of the 5th International Workshop on Edge Systems, Analytics and Networking, Part of EuroSys 2022","2022","EdgeSys 2022 - Proceedings of the 5th International Workshop on Edge Systems, Analytics and Networking, Part of EuroSys 2022","","","","","","66","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128342166&partnerID=40&md5=bdb093d12e83e1602e20edc004b15fe6","The proceedings contain 10 papers. The topics discussed include: edge offloading for microservice architectures; do we need sophisticated system design for edge-assisted augmented reality?; towards wan-aware join sampling over geo-distributed data; combining DNN partitioning and early exit; real-time style transfer with efficient vision transformers; CLEAVE: scalable and edge-native benchmarking of networked control systems; DeepDish on a diet: low-latency, energy-efficient object-detection and tracking at the edge; towards efficient processing of latency-sensitive serverless DAGs at the edge; and SyncMesh: improving data locality for function-as-a-service in meshed edge networks.","","","Association for Computing Machinery, Inc","ACM SIGOPS","5th International Workshop on Edge Systems, Analytics and Networking, EdgeSys 2022, in conjunction with ACM EuroSys 2022","5 April 2022 through 8 April 2022","Virtual, Online","178625","Conference review","Final","","Scopus","2-s2.0-85128342166"
"Shin W.; Kim W.-H.; Min C.","Shin, Wonseok (57572982300); Kim, Wook-Hee (57189853709); Min, Changwoo (36716451200)","57572982300; 57189853709; 36716451200","Fireworks: A Fast, Efcient, and Safe Serverless Framework using VM-level post-JIT Snapshot","2022","EuroSys 2022 - Proceedings of the 17th European Conference on Computer Systems","","","","663","677","14","16","10.1145/3492321.3519581","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128045058&doi=10.1145%2f3492321.3519581&partnerID=40&md5=385d09f4fa8f00b87fce25786365911b","Serverless computing is a new paradigm that is rapidly gaining popularity in Cloud computing. One unique property in serverless computing is that the unit of deployment and execution is a serverless function, which is much smaller than a typical server program. Serverless computing introduces a new pay-as-you-go billing model and provides a high economic benefit from highly elastic resource provisioning. However, serverless computing also brings new challenges such as (1) long start-up times compared to relatively short function execution times, (2) security risks from a highly consolidated environment, and (3) memory efficiency problems from unpredictable function invocations. These problems not only degrade performance but also lower the economic benefits of Cloud providers. To address these challenges without any compromises, we propose a novel VM-level post-JIT snapshot approach and develop a new serverless framework, Fireworks. Our key idea is to synergistically leverage a virtual machine (VM)-level snapshot with a language runtime-level just-in-time (JIT) compilation in tandem. Fireworks leverages JITted serverless function code to reduce both start-up time and execution time of functions and improves memory efficiency by sharing the JITted code. Also, Fireworks can provide a high level of isolation by using a VM as a sandbox to execute a serverless function. Our evaluation results show that Fireworks outperforms state-of-art serverless platforms by 20.6× and provides higher memory efficiency of up to 7.3×.  © 2022 Owner/Author.","Just-in-time compilation; Serverless computing; Snapshot; Start-up time","Codes (symbols); Economic and social effects; Explosives; Just in time production; Virtual machine; Cloud-computing; Economic benefits; Just-in-time; Just-in-time compilation; Machine level; Property; Server programs; Serverless computing; Snapshot; Startup time; Efficiency","Association for Computing Machinery, Inc","ACM SIGOPS","17th European Conference on Computer Systems, EuroSys 2022","5 April 2022 through 8 April 2020","Rennes","178105","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85128045058"
"Tian H.; Li S.; Wang A.; Wang W.; Wu T.; Yang H.","Tian, Huangshi (57203228417); Li, Suyi (57221639733); Wang, Ao (57215285001); Wang, Wei (57234263600); Wu, Tianlong (57991641100); Yang, Haoran (57224666403)","57203228417; 57221639733; 57215285001; 57234263600; 57991641100; 57224666403","Owl: Performance-Aware Scheduling for Resource-Efficient Function-as-a-Service Cloud","2022","SoCC 2022 - Proceedings of the 13th Symposium on Cloud Computing","","","","78","93","15","33","10.1145/3542929.3563470","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143251250&doi=10.1145%2f3542929.3563470&partnerID=40&md5=108290907cec42bde2644dc5fda54707","This work documents our experience of improving the scheduler in Alibaba Function Compute, a public FaaS platform. It commences with our observation that memory and CPU are under-utilized in most FaaS sandboxes. A natural solution is to overcommit VM resources when allocating sandboxes, whereas the ensuing contention may cause performance degradation and compromise user experience. To complicate matters, the degradation in FaaS can arise from external factors, such as failed dependencies of user functions. We design Owl to achieve both high utilization and performance stability. It introduces a customizable rule system for users to specify their toleration of degradation, and overcommits resources with a dual approach. (1) For less-invoked functions, it allocates resources to the sandboxes with usage-based heuristic, keeps monitoring their performance, and remedies any detected degradation. It differentiates whether a degraded sandbox is affected externally by separating a contention-free environment and migrating the affected sandbox into there as a comparison baseline. (2) For frequently-invoked functions, Owl profiles the interference patterns among collocated sandboxes and place the sandboxes under the guidance of profiles. The collocation profiling is designed to tackle the constraints that profiling has to be conducted in production. Owl further consolidates idle sandboxes to reduce resource waste. We prototype Owl in our production system and implement a representative benchmark suite to evaluate it. The results demonstrate that the prototype could reduce VM cost by 43.80% and effectively mitigate latency degradation, with negligible overhead incurred.  © 2022 ACM.","overcommitment; resource-management; scheduling; serverless","Birds; User interfaces; External factors; High utilizations; Overcommitment; Performance; Performance degradation; Resource management; Resource-efficient; Serverless; Service clouds; Users' experiences; Scheduling","Association for Computing Machinery, Inc","ACM Special Interest Grou on Operating Systems (SIGOPS); ACM Special Interest Group on Management of Data (SIGMOD); EBay; et al.; FutureWei; Microsoft","13th Annual ACM Symposium on Cloud Computing, SoCC 2022","7 November 2022 through 11 November 2022","San Francisco","184095","Conference paper","Final","","Scopus","2-s2.0-85143251250"
"Fu Y.; Liu L.; Wang H.; Cheng Y.; Chen S.","Fu, Yuqi (58733678000); Liu, Li (57203373707); Wang, Haoliang (57209143273); Cheng, Yue (56022559100); Chen, Songqing (9338029700)","58733678000; 57203373707; 57209143273; 56022559100; 9338029700","SFS: Smart OS Scheduling for Serverless Functions","2022","International Conference for High Performance Computing, Networking, Storage and Analysis, SC","2022-November","","","","","","20","10.1109/SC41404.2022.00047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149274643&doi=10.1109%2fSC41404.2022.00047&partnerID=40&md5=af7f7242d271f7fe6b9188a1e7e59c2a","Serverless computing enables a new way of building and scaling cloud applications by allowing developers to write fine-grained serverless or cloud functions. The execution duration of a cloud function is typically short-ranging from a few milliseconds to hundreds of seconds. However, due to resource contentions caused by public clouds' deep consolidation, the function execution duration may get significantly prolonged and fail to accurately account for the function's true resource usage. We observe that the function duration can be highly unpredictable with huge amplification of more than 50× for an open-source FaaS platform (OpenLambda). Our experiments show that the OS scheduling policy of cloud functions' host server can have a crucial impact on performance. The default Linux scheduler, CFS (Completely Fair Scheduler), being oblivious to workloads, frequently context-switches short functions, causing a turnaround time that is much longer than their service time. We propose SFS (Smart Function Scheduler), which works entirely in the user space and carefully orchestrates existing Linux FIFO and CFS schedulers to approximate Shortest Remaining Time First (SRTF). SFS uses two-level scheduling that seamlessly combines a new FILTER policy with Linux CFS, to trade off increased duration of long functions for significant performance improvement for short functions. We implement SFS in the Linux user space and port it to OpenLambda. Evaluation results show that SFS significantly improves short functions' duration with a small impact on relatively longer functions, compared to CFS. © 2022 IEEE.","Cloud computing; Operating systems; Performance evaluation","Cloud computing; Economic and social effects; Linux; Scheduling algorithms; Time switches; Cloud applications; Cloud-computing; Fine grained; Operating system; Performance; Performances evaluation; Public clouds; Resource contention; Scalings; User spaces; Function evaluation","IEEE Computer Society","ACM's Special Interest Group on High Performance Computing (SIGHPC); Association for Computing Machinery; IEEE Computer Society; IEEE's Technical Committee on High Performance Computing (TCHPC)","2022 International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2022","13 November 2022 through 18 November 2022","Dallas","186921","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85149274643"
"Kim S.-J.; You M.; Kim B.J.; Shin S.","Kim, Seong-Joong (59627636300); You, Myoungsung (57919087200); Kim, Byung Joon (57202782904); Shin, Seungwon (56185462100)","59627636300; 57919087200; 57202782904; 56185462100","Cryonics: Trustworthy Function-as-a-Service using Snapshot-based Enclaves","2023","SoCC 2023 - Proceedings of the 2023 ACM Symposium on Cloud Computing","","","","528","543","15","1","10.1145/3620678.3624789","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178516055&doi=10.1145%2f3620678.3624789&partnerID=40&md5=f1a449eda9f46d43d6b8f7618468f43c","Recent research has proposed the use of trusted execution environments (TEEs), such as SGX, in serverless computing to safeguard against threats from insecure system software, malicious co-located tenants, or suspicious cloud operators. However, integrating SGX, one of the most mature TEE, with serverless computing results in significant performance degradation due to the function startup latency caused by enclave creation. This performance degradation arises because SGX is not designed with serverless function startup procedures in mind, where numerous application codes, libraries, and data are re-initialized upon each function invocation. The inherent limitations of SGX contribute to significant performance degradation, whether through the addition of every page into the enclave, or the restriction of page permissions, which ultimately cause TLB flushes, context switches, and re-entering the enclave. In this paper, we first take key observations resident in the intrinsic features of the serverless function and propose Cryonics, a method of serving snapshot-based enclave that accelerates the startup time of the function instance by creating a future-proof working set of that. We consider the page locality and obsolete pages of the enclaved function instance to create a lightweight working set used for serving requests. Our evaluation shows that Cryonics achieves up to 100x outperformed startup time compared to existing cold-start-based methods and reveals the stability of the startup time. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Serverless Computing; Trusted Execution Environment","Application codes; Co-located; Performance degradation; Recent researches; Serverless computing; Start-up procedure; Startup time; System softwares; Trusted execution environments; Working set; Trusted computing","Association for Computing Machinery, Inc","ACM SIGMOD; ACM SIGOPS; ActiveLoop; Google; IBM; Microsoft","14th ACM Symposium on Cloud Computing, SoCC 2023","30 October 2023 through 1 November 2023","Santa Cruz","194526","Conference paper","Final","","Scopus","2-s2.0-85178516055"
"Li Z.; Xu C.; Chen Q.; Zhao J.; Chen C.; Guo M.","Li, Zijun (57278389100); Xu, Chuhao (57949219100); Chen, Quan (36623232500); Zhao, Jieru (57201133878); Chen, Chen (57257554800); Guo, Minyi (7201564780)","57278389100; 57949219100; 36623232500; 57201133878; 57257554800; 7201564780","DataFlower: Exploiting the Data-flow Paradigm for Serverless Workflow Orchestration","2023","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","4","","","57","72","15","7","10.1145/3623278.3624755","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202174499&doi=10.1145%2f3623278.3624755&partnerID=40&md5=b8cf6120cb5a17dd98d7a1fcb6a8f56f","Serverless computing that runs functions with auto-scaling is a popular task execution pattern in the cloud-native era. By connecting serverless functions into workflows, tenants can achieve complex functionality. Prior research adopts the control-flow paradigm to orchestrate a serverless workflow. However, the control-flow paradigm inherently results in long response latency, due to the heavy data persistence overhead, sequential resource usage, and late function triggering. Our investigation shows that the data-flow paradigm has the potential to resolve the above problems, with careful design and optimization. We propose DataFlower, a scheme that achieves the data-flow paradigm for serverless workflows. In DataFlower, a container is abstracted to be a function logic unit and a data logic unit. The function logic unit runs the functions, and the data logic unit handles the data transmission asynchronously. Moreover, a host-container collaborative communication mechanism is used to support efficient data transfer. Our experimental results show that compared to state-of-the-art serverless designs, DataFlower reduces the 99%-ile latency of the benchmarks by up to 35.4%, and improves the peak throughput by up to 3.8X. © 2023 Copyright held by the owner/author(s).","control-flow paradigm; data-flow paradigm; FaaS; Function-as-a-Service; serverless workflow; workflow orchestration","Asynchronous sequential logic; Data flow analysis; Network security; Plastic bottles; Control-flow; Control-flow paradigm; Data-flow paradigm; Faas; Function-as-a-service; Logic unit; Serverless workflow; Work-flows; Workflow orchestration; Benchmarking","Association for Computing Machinery","ACM SIGARCH; ACM SIGOPS; ACM SIGPLAN","28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2023","25 March 2023 through 29 March 2023","Vancouver","201666","Conference paper","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85202174499"
"Matricardi A.; Bocci A.; Forti S.; Brogi A.","Matricardi, Alessio (58589976200); Bocci, Alessandro (57222340356); Forti, Stefano (57195946378); Brogi, Antonio (57193752782)","58589976200; 57222340356; 57195946378; 57193752782","Simulating FaaS Orchestrations In The Cloud-Edge Continuum","2023","FRAME 2023 - Proceedings of the 3rd Workshop on Flexible Resource and Application Management on the Edge","","","","19","26","7","3","10.1145/3589010.3594893","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171448274&doi=10.1145%2f3589010.3594893&partnerID=40&md5=d1765f19ec7c3c27e87f28355055c248","Deploying Function-as-a-Service (FaaS) applications to resources in the Cloud-Edge continuum calls for suitable simulation environments to assess new proposals for managing those applications while accounting for their specificity, i.e. ephemeral deployment and orchestrated execution. Indeed, exploiting real infrastructures for extensive experiments, while considering heterogeneous nodes and different applications, is costly and time-consuming. In this article, we propose a novel open-source discrete-time simulator for the management (i.e. deployment and migration) of orchestrated FaaS in the Cloud-Edge continuum. It enables assessing customised management policies against varying infrastructure conditions. It collects data about execution time, energy consumption, and successes/failures of management operations. We illustrate the prototype at work over a lifelike case study. © 2023 ACM.","cloud-edge continuum; function-as-a-service; simulation","Cloud-edge continuum; Discrete time; Function-as-a-service; Heterogeneous nodes; Management policy; Open-source; Service orchestration; Services applications; Simulation; Simulation environment; Energy utilization","Association for Computing Machinery, Inc","ACM SIGARCH","3rd Workshop on Flexible Resource and Application Management on the Edge, FRAME 2023, held in conjunction with the HPDC 2023 and the FCRC 2023 Conferences","20 June 2023","Orlando","191790","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85171448274"
"Farahani R.; Kimovski D.; Ristov S.; Iosup A.; Prodan R.","Farahani, Reza (57220368259); Kimovski, Dragi (55364582500); Ristov, Sashko (49561774300); Iosup, Alexandru (23392350500); Prodan, Radu (8858675500)","57220368259; 55364582500; 49561774300; 23392350500; 8858675500","Towards Sustainable Serverless Processing of Massive Graphs on the Computing Continuum","2023","ICPE 2023 - Companion of the 2023 ACM/SPEC International Conference on Performance Engineering","","","","221","226","5","6","10.1145/3578245.3585331","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158885440&doi=10.1145%2f3578245.3585331&partnerID=40&md5=b90acad93c7ca1323b5cea902ddfc13b","With the ever-increasing volume of data and the demand to analyze and comprehend it, graph processing has become an essential approach for solving complex problems in various domains, like social networks, bioinformatics, and finance. Despite the potential benefits of current graph processing platforms, they often encounter difficulties supporting diverse workloads, models, and languages. Moreover, existing platforms suffer from limited portability and interoperability, resulting in redundant efforts and inefficient resource and energy utilization due to vendor and even platform lock-in. To bridge the aforementioned gaps, the Graph-Massivizer project, funded by the Horizon Europe research and innovation program, conducts research and develops a high-performance, scalable, and sustainable platform for information processing and reasoning based on the massive graph (MG) representation of extreme data. In this paper, we briefly introduce the Graph-Massivizer platform. We explore how the emerging serverless computing paradigm can be leveraged to devise a scalable graph analytics tool over a codesigned computing continuum infrastructure. Finally, we sketch seven crucial research questions in our design and outline three ongoing and future research directions for addressing them. © 2023 Owner/Author.","computing continuum; graph processing; massive graph; serverless computing; sustainability","'current; Complex problems; Computing continuum; Graph processing; Lock-in; Massive graph; Potential benefits; Processing platform; Serverless computing; Workload model; Energy utilization","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC Research","14th Annual ACM/SPEC International Conference on Performance Engineering, ICPE 2023","15 April 2023 through 19 April 2023","Coimbra","187962","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85158885440"
"","","","FRAME 2023 - Proceedings of the 3rd Workshop on Flexible Resource and Application Management on the Edge","2023","FRAME 2023 - Proceedings of the 3rd Workshop on Flexible Resource and Application Management on the Edge","","","","","","45","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171484962&partnerID=40&md5=c87986ab9cf07c33dc4c521b7f9f38d0","The proceedings contain 8 papers. The topics discussed include: DATA7: a dataset for assessing resource and application management solutions at the edge; multi-agent deep reinforcement learning for weighted multi-path routing; real-time monitoring and analysis of edge and cloud resources; simulating FaaS orchestrations in the cloud-edge continuum; contention-aware performance modeling for heterogeneous edge and cloud systems; innovation potential of the ACCORDION Platform; TEACHING: a computing toolkit for building efficient autonomous applications leveraging humanistic intelligence; and EDGELESS Project: on the road to serverless edge AI.","","","Association for Computing Machinery, Inc","ACM SIGARCH","3rd Workshop on Flexible Resource and Application Management on the Edge, FRAME 2023, held in conjunction with the HPDC 2023 and the FCRC 2023 Conferences","20 June 2023","Orlando","191790","Conference review","Final","","Scopus","2-s2.0-85171484962"
"Ngo K.L.; Mukherjee J.; Jiang Z.M.; Litoiu M.","Ngo, Kim Long (57566457300); Mukherjee, Joydeep (56242469200); Jiang, Zhen Ming (57197839751); Litoiu, Marin (6603658203)","57566457300; 56242469200; 57197839751; 6603658203","Evaluating the Scalability and Elasticity of Function as a Service Platform","2022","ICPE 2022 - Proceedings of the 2022 ACM/SPEC International Conference on Performance Engineering","","","","117","124","7","4","10.1145/3489525.3511682","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128592950&doi=10.1145%2f3489525.3511682&partnerID=40&md5=b12e8a85d107d8bf324ca0998b538703","Function as a Service (FaaS) is a new software technology with promising features such as automated resource management and auto-scaling. Since these operational aspects are transparent, software engineers may not fully understand the scaling characteristics as well as limitations of this technology and this lack of information can lead to undesired performance results. To address these concerns, we perform a study to characterize FaaS' scalability with intensive workloads on three popular FaaS cloud platforms, namely Amazon AWS Lambda, IBM and Azure Cloud Function. We also study a workload smoother design pattern to examine if it enhances FaaS overall performance. The results show that different FaaS platforms adopt distinct scaling strategies and by applying a workload smoother, software engineers can achieve 99 - 100% success rates compared to 60 - 80% when FaaS' system is saturated.  © 2022 ACM.","elasticity; function as a service; scalability; serverless; software performance; workload smoother","Elasticity; Software engineering; Function as a service; Operational aspects; Performance; Resource management; Scalings; Serverless; Service scalability; Software performance; Software technology; Workload smooth; Scalability","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC Research","13th Annual ACM/SPEC International Conference on Performance Engineering, ICPE 2022","9 April 2022 through 13 April 2022","Virtual, Online","178683","Conference paper","Final","","Scopus","2-s2.0-85128592950"
"Pauloski J.G.; Hayot-Sasson V.; Ward L.; Hudson N.; Sabino C.; Baughman M.; Chard K.; Foster I.","Pauloski, J. Gregory (57208001241); Hayot-Sasson, Valerie (57202280425); Ward, Logan (55556371700); Hudson, Nathaniel (57220489824); Sabino, Charlie (58352689400); Baughman, Matt (57202993030); Chard, Kyle (9132950200); Foster, Ian (35572232000)","57208001241; 57202280425; 55556371700; 57220489824; 58352689400; 57202993030; 9132950200; 35572232000","Accelerating Communications in Federated Applications with Transparent Object Proxies","2023","Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2023","","","59","","","","5","10.1145/3581784.3607047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175072847&doi=10.1145%2f3581784.3607047&partnerID=40&md5=94338dd4714eda274e7886022ead00dc","Advances in networks, accelerators, and cloud services encourage programmers to reconsider where to compute - -such as when fast networks make it cost-effective to compute on remote accelerators despite added latency. Workflow and cloud-hosted serverless computing frameworks can manage multi-step computations spanning federated collections of cloud, high-performance computing (HPC), and edge systems, but passing data among computational steps via cloud storage can incur high costs. Here, we overcome this obstacle with a new programming paradigm that decouples control flow from data flow by extending the pass-by-reference model to distributed applications. We describe ProxyStore, a system that implements this paradigm by providing object proxies that act as wide-area object references with just-in-time resolution. This proxy model enables data producers to communicate data unilaterally, transparently, and efficiently to both local and remote consumers. We demonstrate the benefits of this model with synthetic benchmarks and real-world scientific applications, running across various computing platforms.  © 2023 ACM.","data communication and storage; distributed computing; federated computing; open-source software; python","Application programs; Benchmarking; Cloud storage; Cost effectiveness; Information management; Open source software; Open systems; Cloud services; Computing frameworks; Cost effective; Data storage; Data-communication; Federated computing; In-network services; Open-source softwares; Transparent objects; Work-flows; Python","Association for Computing Machinery, Inc","ACM's Special Interest Group on High Performance Computing (SIGHPC); Association for Computing Machinery; IEEE�s Technical Committee on High Performance Computing (TCHPC); Institute of Electrical and Electronics Engineers (IEEE) Computer Society","2023 International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2023","12 November 2023 through 17 November 2023","Denver","194664","Conference paper","Final","","Scopus","2-s2.0-85175072847"
"Nestorov A.M.; Berral J.L.; Misale C.; Wang C.; Carrera D.; Youssef A.","Nestorov, Anna Maria (57194447320); Berral, Josep Lluís (23090036200); Misale, Claudia (56132082000); Wang, Chen (57157809200); Carrera, David (9736961100); Youssef, Alaa (57214412197)","57194447320; 23090036200; 56132082000; 57157809200; 9736961100; 57214412197","Floki: A Proactive Data Forwarding System for Direct Inter-Function Communication for Serverless Workflows","2022","WoC 2022 - Proceedings of the 8th International Workshop on Container Technologies and Container Clouds, Part of Middleware 2022","","","","13","18","5","4","10.1145/3565384.3565890","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143202833&doi=10.1145%2f3565384.3565890&partnerID=40&md5=6f905af1aefac34a6202ec3a8d266482","Serverless computing emerges as an architecture choice to build and run containerized data-intensive pipelines. It leaves the tedious work of infrastructure management and operations to the cloud provider, allowing developers to focus on their core business logic, decomposing their jobs into small containerized functions. To increase platform scalability and flexibility, providers take advantage of hardware disaggregation and require inter-function communication to go through shared object storage. Despite data persistence and recovery advantages, object storage is expensive in terms of performance and resources when dealing with data-intensive workloads. In this paper, we present Floki, a data forwarding system for direct and inter-function data exchange proactively enabling point-to-point communication between pipeline producer-consumer pairs of containerized functions through fixed-size memory buffers, pipes, and sockets. Compared with state-of-practice object storage, Floki shows up to 74.95× of end-to-end time performance increase, reducing the largest data sharing time from 12.55 to 4.33 minutes, while requiring up to 50,738× fewer disk resources, with up to roughly 96GB space release.  © 2022 ACM.","communication; containers; kubernetes; orchestration; serverless","Computation theory; Digital storage; Electronic data interchange; Pipelines; Cloud providers; Data intensive; Data-forwarding; Infrastructure managements; Kubernetes; Object storages; Orchestration; Performance; Serverless; Work-flows; Containers","Association for Computing Machinery, Inc","ACM; ETS; Raytheon BBN; rti","8th International Workshop on Container Technologies and Container Clouds, WoC 2022 - Part of Middleware 2022","7 November 2022","Quebec","184464","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85143202833"
"Yu H.; Fontenot C.; Wang H.; Li J.; Yuan X.; Park S.-J.","Yu, Hanfei (57220804031); Fontenot, Christian (58559623600); Wang, Hao (57170260400); Li, Jian (59080155500); Yuan, Xu (55973028200); Park, Seung-Jong (7501830301)","57220804031; 58559623600; 57170260400; 59080155500; 55973028200; 7501830301","Libra: Harvesting Idle Resources Safely and Timely in Serverless Clusters","2023","HPDC 2023 - Proceedings of the 32nd International Symposium on High-Performance Parallel and Distributed Computing","","","","181","194","13","7","10.1145/3588195.3592996","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169604638&doi=10.1145%2f3588195.3592996&partnerID=40&md5=4e9410b6ff372979833e71b1ed9a2de0","Serverless computing has been favored by users and infrastructure providers from various industries, including online services and scientific computing. Users enjoy its auto-scaling and ease-of-management, and providers own more control to optimize their service. However, existing serverless platforms still require users to pre-define resource allocations for their functions, leading to frequent misconfiguration by inexperienced users in practice. Besides, functions' varying input data further escalate the gap between their dynamic resource demands and static allocations, leaving functions either over-provisioned or under-provisioned. This paper presents Libra, a safe and timely resource harvesting framework for multi-node serverless clusters. Libra makes precise harvesting decisions to accelerate function invocations with harvested resources and jointly improve resource utilization by profiling dynamic resource demands and availability proactively. Experiments on OpenWhisk clusters with real-world workloads show that Libra reduces response latency by 39% and achieves 3X resource utilization compared to state-of-the-art solutions.  © 2023 ACM.","resource harvesting; serverless computing","Dynamic resources; Infrastructure providers; Misconfigurations; On-line service; Resource demands; Resource harvesting; Resources allocation; Resources utilizations; Scalings; Serverless computing; Harvesting","Association for Computing Machinery, Inc","ACM SIGARCH; ACM SIGHPC","32nd International Symposium on High-Performance Parallel and Distributed Computing, HPDC 2023","16 June 2023 through 23 June 2023","Orlando","191522","Conference paper","Final","","Scopus","2-s2.0-85169604638"
"De Lama Sanchez N.; Haase P.; Roman D.; Prodan R.","De Lama Sanchez, Nuria (58243128800); Haase, Peter (7005272447); Roman, Dumitru (23467687800); Prodan, Radu (8858675500)","58243128800; 7005272447; 23467687800; 8858675500","Boosting the Impact of Extreme and Sustainable Graph Processing for Urgent Societal Challenges in Europe Graph-Massivizer: A Horizon Europe Project","2023","ICPE 2023 - Companion of the 2023 ACM/SPEC International Conference on Performance Engineering","","","","233","238","5","3","10.1145/3578245.3585334","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158859164&doi=10.1145%2f3578245.3585334&partnerID=40&md5=83daafe305649c9ecf04e9b7bef093d9","We explore the potential of the Graph-Massivizer project funded by the Horizon Europe research and innovation program of the European Union to boost the impact of extreme and sustainable graph processing for mitigating existing urgent societal challenges. Current graph processing platforms do not support diverse workloads, models, languages, and algebraic frameworks. Existing specialized platforms are difficult to use by non-experts and suffer from limited portability and interoperability, leading to redundant efforts and inefficient resource and energy consumption due to vendor and even platform lock-in. While synthetic data emerged as an invaluable resource overshadowing actual data for developing robust artificial intelligence analytics, graph generation remains a challenge due to extreme dimensionality and complexity. On the European scale, this practice is unsustainable and, thus, threatens the possibility of creating a climate-neutral and sustainable economy based on graph data. Making graph processing sustainable is essential but needs credible evidence. The grand vision of the Graph-Massivizer project is a technological solution, coupled with field experiments and experience-sharing, for a high-performance and sustainable graph processing of extreme data with a proper response for any need and organizational size by 2030. © 2023 Owner/Author.","extreme data; graph processing; serverless computing; sustainability","Data handling; 'current; European union; Extreme data; Graph processing; Innovation programs; Model languages; Processing platform; Research programs; Serverless computing; Workload model; Energy utilization","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC Research","14th Annual ACM/SPEC International Conference on Performance Engineering, ICPE 2023","15 April 2023 through 19 April 2023","Coimbra","187962","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85158859164"
"Hou K.; Lin S.; Chen Y.; Yegneswaran V.","Hou, Kaiyu (57195406810); Lin, Sen (57383902300); Chen, Yan (35228782000); Yegneswaran, Vinod (8307376800)","57195406810; 57383902300; 35228782000; 8307376800","QFaaS: Accelerating and Securing Serverless Cloud Networks with QUIC","2022","SoCC 2022 - Proceedings of the 13th Symposium on Cloud Computing","","","","240","256","16","5","10.1145/3542929.3563458","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143256650&doi=10.1145%2f3542929.3563458&partnerID=40&md5=2668fb7fa7f902a3d2c7c4196b1a3844","Serverless computing has greatly simplified cloud programming. It liberates cloud tenants from various system administration and resource management tasks, such as configuration and provisioning. Under this new cloud computing paradigm, a single monolithic application is divided into separate stateless functions, i.e., function-as-a-service (FaaS), which are then orchestrated together to support complex business logic. But there is a fundamental cost associated with this enhanced flexibility. Internal network connections between functions are now initiated frequently, to support serverless features such as agile autoscaling and function chains, raising communication latency. To alleviate this cost, current serverless providers sacrifice security for performance, keeping internal function communications unencrypted. We believe that the emerging QUIC protocol, which has secured and accelerated HTTP communications in the wide area, could proffer a solution to this challenge. We design a QUIC-based FaaS framework, called QFaaS, and implement it on the OpenFaaS platform. Our design explicitly ensures that existing serverless applications can directly benefit from QFaaS without any application code modification. Experiments on synthetic functions and real-world applications demonstrate that QFaaS can reduce communication latency for single functions and function chains by 28% and 40%, respectively, and save up to 50 ms in end-user response time.  © 2022 ACM.","cloud networking; QUIC protocol; serverless computing; serverless network performance and security","Cloud computing; Computation theory; HTTP; Hypertext systems; Cloud networkings; Cloud networks; Communication latency; Networks security; QUIC protocol; Resource management; Serverless computing; Serverless network performance and security; System administration; System resources; Network security","Association for Computing Machinery, Inc","ACM Special Interest Grou on Operating Systems (SIGOPS); ACM Special Interest Group on Management of Data (SIGMOD); EBay; et al.; FutureWei; Microsoft","13th Annual ACM Symposium on Cloud Computing, SoCC 2022","7 November 2022 through 11 November 2022","San Francisco","184095","Conference paper","Final","","Scopus","2-s2.0-85143256650"
"Larcher T.; Ristov S.","Larcher, Thomas (58728765700); Ristov, Sashko (49561774300)","58728765700; 49561774300","Scale Composite BaaS Services With AFCL Workflows","2023","ACM International Conference Proceeding Series","","","","2033","2041","8","2","10.1145/3624062.3624282","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178102584&doi=10.1145%2f3624062.3624282&partnerID=40&md5=841dab9795b1e320297916f827874af2","Due to various restrictions in serverless computing, developers face significant challenges to pipeline multiple Backend-as-a-Service (BaaS) services, which is restricted by the maximum size of the serverless function's deployment package, or by throughput and concurrency restrictions for functions and BaaS services. To bridge this gap, we introduce a methodology how to code scalable and composite BaaS services in form of serverless workflows. Using the Abstract Function Choreography Language (AFCL), we develop and characterize two scalable and composite BaaS services (i) pdf2SpeechDE, which translates a pdf file written in English and converts the extracted text into audio file in German; and (ii) Speech2SpeechDE, which translates audio files from English into a single audio file in German. We composed pdf2SpeechDE and speech2SpeechDE as a sequence of three BaaS services each, including split-merge functions for scalability. The characterization showed that there is no dominating provider for individual BaaS services. © 2023 Owner/Author.","Backend-as-a-Service; federation; Function-as-a-Service; serverless computing; workflows","Translation (languages); Audio files; Backend-as-a-service; Choreography languages; Federation; Function-as-a-service; PDF files; Serverless computing; Work-flows; Abstracting","Association for Computing Machinery","ACM's Special Interest Group on High Performance Computing (SIGHPC); Association for Computing Machinery; IEEE�s Technical Committee on High Performance Computing (TCHPC); Institute of Electrical and Electronics Engineers (IEEE) Computer Society","2023 International Conference on High Performance Computing, Network, Storage, and Analysis, SC Workshops 2023","12 November 2023 through 17 November 2023","Denver","194341","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85178102584"
"","","","SoCC 2023 - Proceedings of the 2023 ACM Symposium on Cloud Computing","2023","SoCC 2023 - Proceedings of the 2023 ACM Symposium on Cloud Computing","","","","","","620","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178512112&partnerID=40&md5=e35a101e29bcb64adc3ab519763fc1e2","The proceedings contain 40 papers. The topics discussed include: Plexus: optimizing join approximation for geo-distributed data analytics; carbon containers: a system-level facility for managing application-level carbon emissions; Golgi: performance-aware, resource-efficient function scheduling for serverless computing; oblivious Paxos: privacy-preserving consensus over secret-shares; not all resources are visible: exploiting fragmented shadow resources in shared-state scheduler architecture; dissecting overheads of service mesh sidecars; OneAdapt: fast adaptation for deep learning applications via backpropagation; multivariate anomaly detection with domain clustering; a comparison of end-to-end decision forest inference pipelines; and maximizing the utilization of GPUs used by cloud gaming through adaptive co-location with combo.","","","Association for Computing Machinery, Inc","ACM SIGMOD; ACM SIGOPS; ActiveLoop; Google; IBM; Microsoft","14th ACM Symposium on Cloud Computing, SoCC 2023","30 October 2023 through 1 November 2023","Santa Cruz","194526","Conference review","Final","","Scopus","2-s2.0-85178512112"
"Chadha M.; Pacyna V.; Jindal A.; Gu J.; Gerndt M.","Chadha, Mohak (57193381434); Pacyna, Victor (57933767100); Jindal, Anshul (57203162595); Gu, Jianfeng (57348828300); Gerndt, Michael (22333929300)","57193381434; 57933767100; 57203162595; 57348828300; 22333929300","Migrating from Microservices to Serverless: An IoT Platform Case Study","2022","WoSC 2022 - Proceedings of the 8th International Workshop on Serverless Computing, Part of Middleware 2022","","","","19","24","5","0","10.1145/3565382.3565881","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145557445&doi=10.1145%2f3565382.3565881&partnerID=40&md5=b23aab768c1cb70af98b192732e11f5f","Microservice architecture is the common choice for developing cloud applications these days since each individual microservice can be independently modified, replaced, and scaled. As a result, application development and operating cloud infrastructure were bundled together into what is now commonly called DevOps. However, with the increasing popularity of the serverless computing paradigm and its several advantages such as no infrastructure management, a pay-per-use billing policy, and on-demand fine-grained autoscaling, there is a growing interest in utilizing FaaS and server-less CaaS technologies for refactoring microservices-based applications. Towards this, we migrate a complex IoT platform application onto OpenWhisk (OW) and Google Cloud Run (GCR). We comprehensively evaluate the performance of the different deployment strategies, i.e., Google Kubernetes Engine (GKE)-Standard, OW, and GCR for the IoT platform using different load testing scenarios. Results from our experiments show that while GKE standard performs best for most scenarios, GCR is always cheaper wrt costs.  © 2022 ACM.","CaaS; container-as-a-service; FaaS; function-as-a-service; microservices; performance analysis; serverless","Load testing; Platform as a Service (PaaS); Caa; Case-studies; Cloud applications; Container-as-a-service; Faas; Function-as-a-service; Google+; Microservice; Performances analysis; Serverless; Internet of things","Association for Computing Machinery, Inc","ACM; ETS; Raytheon BBN; rti","8th International Workshop on Serverless Computing, WoSC 2022 - Part of Middleware 2022","7 November 2022","Quebec","184997","Conference paper","Final","","Scopus","2-s2.0-85145557445"
"Qi S.; Liu X.; Jin X.","Qi, Sheng (58697972100); Liu, Xuanzhe (22035687800); Jin, Xin (57189270771)","58697972100; 22035687800; 57189270771","Halfmoon: Log-Optimal Fault-Tolerant Stateful Serverless Computing","2023","SOSP 2023 - Proceedings of the 29th ACM Symposium on Operating Systems Principles","","","","314","330","16","9","10.1145/3600006.3613154","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176965806&doi=10.1145%2f3600006.3613154&partnerID=40&md5=4961634fe9b38d4d005ca62b28989732","Serverless computing separates function execution from state management. Simple retry-based fault tolerance might corrupt the shared state with duplicate updates. Existing solutions employ log-based fault tolerance to achieve exactlyonce semantics, where every single read or write to the external state is associated with a log for deterministic replay. However, logging is not a free lunch, which introduces considerable overhead to stateful serverless applications.We present Halfmoon, a serverless runtime system for fault-tolerant stateful serverless computing. Our key insight is that it is unnecessary to symmetrically log both reads and writes. Instead, it suffices to log either reads or writes, i.e., asymmetrically. We design two logging protocols that enforce exactly-once semantics while providing log-free reads and writes, which are suitable for read- and write-intensive workloads, respectively. We theoretically prove that the two protocols are log-optimal, i.e., no other protocols can achieve lower logging overhead than our protocols. We provide a criterion for choosing the right protocol for a given workload, and a pauseless switching mechanism to switch protocols for dynamic workloads. We implement a prototype of Halfmoon. Experiments show that Halfmoon achieves 20% - 40% lower latency and 1.5 - 4.0× lower logging overhead than the state-of-the-art solution Boki.  © 2023 ACM.","exactly-once semantics; FaaS; logging; serverless computing","Fault tolerance; Deterministic replay; Exactly-once semantic; Faas; Fault-tolerant; Run- time systems; Serverless computing; Shared state; Simple++; State management; Write-intensive workloads; Semantics","Association for Computing Machinery, Inc","ACM SIGOPS in cooperation with USENIX","29th ACM Symposium on Operating Systems Principles, SOSP 2023","23 October 2023 through 26 October 2023","Koblenz","193843","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85176965806"
"Cicconetti C.; Carlini E.; Paradell A.","Cicconetti, Claudio (22033885800); Carlini, Emanuele (35316758800); Paradell, Antonio (57204900609)","22033885800; 35316758800; 57204900609","EDGELESS Project: On the Road to Serverless Edge AI","2023","FRAME 2023 - Proceedings of the 3rd Workshop on Flexible Resource and Application Management on the Edge","","","","41","43","2","4","10.1145/3589010.3594890","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171453582&doi=10.1145%2f3589010.3594890&partnerID=40&md5=f346e6d755d74e22987115cadaab85ba","The EDGELESS project is set to efficiently operate serverless computing in extremely diverse computing environments, from resource-constrained edge devices to highly-virtualized cloud platforms. Automatic deployment and reconfiguration will leverage AI/ML techniques, resulting in a flexible horizontally-scalable computation solution able to fully use heterogeneous edge resources while preserving vertical integration with the cloud and the benefits of serverless and its companion programming model, i.e., Function-as-a-Service (FaaS). The system under design will be environmentally sustainable, as it will dynamically concentrate resources physically (e.g., by temporarily switching off far-edge devices) or logically (e.g., by dispatching tasks towards a specific set of nodes) at the expense of performance-tolerant applications. © 2023 ACM.","edge computing; internet of things; resource-constrained devices; serverless computing","Edge computing; Automatic deployments; Automatic reconfiguration; Cloud platforms; Computing environments; Edge computing; Edge resources; Programming models; Resourceconstrained devices; Serverless computing; Vertical integration; Internet of things","Association for Computing Machinery, Inc","ACM SIGARCH","3rd Workshop on Flexible Resource and Application Management on the Edge, FRAME 2023, held in conjunction with the HPDC 2023 and the FCRC 2023 Conferences","20 June 2023","Orlando","191790","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85171453582"
"Saxena D.; Ji T.; Singhvi A.; Khalid J.; Akella A.","Saxena, Divyanshu (57573564300); Ji, Tao (57573941100); Singhvi, Arjun (57200408847); Khalid, Junaid (25927179300); Akella, Aditya (8383144400)","57573564300; 57573941100; 57200408847; 25927179300; 8383144400","Memory Deduplication for Serverless Computing with Medes","2022","EuroSys 2022 - Proceedings of the 17th European Conference on Computer Systems","","","","714","729","15","32","10.1145/3492321.3524272","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128111309&doi=10.1145%2f3492321.3524272&partnerID=40&md5=641dc4eb78cd40808e0aa7b967e8ee22","Serverless platforms today impose rigid trade-offs between resource use and user-perceived performance. Limited controls, provided via toggling sandboxes between warm and cold states and keep-alives, force operators to sacrifice significant resources to achieve good performance. We present a serverless framework, Medes, that breaks the rigid trade-off and allows operators to navigate the trade-off space smoothly. Medes leverages the fact that the warm sandboxes running on serverless platforms have a high fraction of duplication in their memory footprints. We exploit these redundant chunks to develop a new sandbox state, called a dedup state, that is more memory-efficient than the warm state and faster to restore from than the cold state. We develop novel mechanisms to identify memory redundancy at minimal overhead while ensuring that the dedup containers' memory footprint is small. Finally, we develop a simple sandbox management policy that exposes a narrow, intuitive interface for operators to trade-off performance for memory by jointly controlling warm and dedup sandboxes. Detailed experiments with a prototype using real-world serverless workloads demonstrate that Medes can provide up to 1×-2.75× improvements in the end-to-end latencies. The benefits of Medes are enhanced in memory pressure situations, where Medes can provide up to 3.8× improvements in end-to-end latencies. Medes achieves this by reducing the number of cold starts incurred by 10-50% against the state-of-the-art baselines.  © 2022 Owner/Author.","Cloud computing; Memory deduplication; Serverless; Virtualization","Virtualization; Cloud-computing; Cold state; End to end latencies; Memory deduplication; Memory footprint; Performance; Resource use; Serverless; Trade off; Virtualizations; Economic and social effects","Association for Computing Machinery, Inc","ACM SIGOPS","17th European Conference on Computer Systems, EuroSys 2022","5 April 2022 through 8 April 2020","Rennes","178105","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85128111309"
"Alzayat M.; Mace J.; Druschel P.; Garg D.","Alzayat, Mohamed (57219526567); Mace, Jonathan (57113198400); Druschel, Peter (6701688597); Garg, Deepak (57215514499)","57219526567; 57113198400; 6701688597; 57215514499","Groundhog: Efficient Request Isolation in FaaS","2023","Proceedings of the 18th European Conference on Computer Systems, EuroSys 2023","","","","398","415","17","9","10.1145/3552326.3567503","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160206594&doi=10.1145%2f3552326.3567503&partnerID=40&md5=e0d3561cf6d0571988f9f4bcfec1b978","Security is a core responsibility for Function-as-a-Service (FaaS) providers. The prevailing approach isolates concurrent executions of functions in separate containers. However, successive invocations of the same function commonly reuse the runtime state of a previous invocation in order to avoid container cold-start delays. Although efficient, this container reuse has security implications for functions that are invoked on behalf of differently privileged users or administrative domains: bugs in a function’s implementation — or a third-party library/runtime it depends on — may leak private data from one invocation of the function to a subsequent one. Groundhog isolates sequential invocations of a function by efficiently reverting to a clean state, free from any private data, after each invocation. The system exploits two properties of typical FaaS platforms: each container executes at most one function at a time and legitimate functions do not retain state across invocations. This enables Groundhog to efficiently snapshot and restore function state between invocations in a manner that is independent of the programming language/runtime and does not require any changes to existing functions, libraries, language runtimes, or OS kernels. We describe the design and implementation of Groundhog and its integration with OpenWhisk, a popular production-grade open-source FaaS framework. On three existing benchmark suites, Groundhog isolates sequential invocations with modest overhead on end-to-end latency (median: 1.5%, 95p: 7%) and throughput (median: 2.5%, 95p: 49.6%), relative to an insecure baseline that reuses the container and runtime state. © 2023 Copyright held by the owner/author(s).","Checkpoint; FaaS; Request isolation; Restore; Rollback; Security; Serverless; Snapshot","Open source software; Restoration; Checkpoint; Function-as-a-service; Request isolation; Restore; Reuse; Rollback; Run-time state; Security; Serverless; Snapshot; Containers","Association for Computing Machinery, Inc","Ant Group Research; et al.; Google; Huawei; KAUST; Meta","18th European Conference on Computer Systems, EuroSys 2023","8 May 2023 through 12 May 2023","Rome","188495","Conference paper","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85160206594"
"Kuchler T.; Giardino M.; Roscoe T.; Klimovic A.","Kuchler, Tom (58577854900); Giardino, Michael (55816656600); Roscoe, Timothy (7003533216); Klimovic, Ana (56039431800)","58577854900; 55816656600; 7003533216; 56039431800","Function as a Function","2023","SoCC 2023 - Proceedings of the 2023 ACM Symposium on Cloud Computing","","","","81","92","11","9","10.1145/3620678.3624648","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178512307&doi=10.1145%2f3620678.3624648&partnerID=40&md5=13ffd1bc66c9596f92ca85bcfc6609b0","Function as a Service (FaaS) and the associated serverless computing paradigm alleviates users from resource management and allows cloud platforms to optimize system infrastructure under the hood. Despite significant advances, FaaS infrastructure still leaves much room to improve performance and resource efficiency. We argue that both higher performance and resource efficiency are possible — while maintaining secure isolation — if we are willing to revisit the FaaS programming model and system software design. We propose Dandelion, a clean-slate FaaS system that rethinks the programming model by treating serverless functions as pure functions, thereby explicitly separating computation and I/O. This new programming model enables a lightweight yet secure function execution system. It also makes functions more amenable to hardware acceleration and enables dataflow-aware function orchestration. Our initial prototype of Dandelion achieves 45× lower tail latency for cold starts compared to Firecracker. For 95% hot function invocations, Dandelion achieves 5× higher peak throughput. © 2023 Copyright held by the owner/author(s).","cloud computing; function as a service; serverless","Acceleration; Software design; Cloud platforms; Cloud-computing; Computing paradigm; Function as a service; Performance efficiency; Programming models; Resource efficiencies; Resource management; Serverless; System infrastructure; Efficiency","Association for Computing Machinery, Inc","ACM SIGMOD; ACM SIGOPS; ActiveLoop; Google; IBM; Microsoft","14th ACM Symposium on Cloud Computing, SoCC 2023","30 October 2023 through 1 November 2023","Santa Cruz","194526","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85178512307"
"","","","EdgeSys 2023 - Proceedings of the 6th International Workshop on Edge Systems, Analytics and Networking, Part of EuroSys 2023","2023","EdgeSys 2023 - Proceedings of the 6th International Workshop on Edge Systems, Analytics and Networking, Part of EuroSys 2023","","","","","","60","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159315619&partnerID=40&md5=4993287a4fc21d7a54ddcc9122f005a1","The proceedings contain 10 papers. The topics discussed include: S-Cache: function caching for serverless edge computing; an autonomous resource management model towards cloud morphing; latency-aware scheduling for real-time application support in edge computing; an evaluation of service mesh frameworks for edge systems; hot under the hood: an analysis of ambient temperature impact on heterogeneous edge platforms; lotus: serverless in-transit data processing for edge-based pub/sub; how to pipeline frame transfer and server inference in edge-assisted AR to optimize AR task accuracy?; cost-aware neural network splitting and dynamic rescheduling for edge intelligence; ESCEPE: early-exit network section-wise model compression using self-distillation and weight clustering; and an empirical study of resource-stressing faults in edge-computing applications.","","","Association for Computing Machinery, Inc","ACM SIGOPS","6th International Workshop on Edge Systems, Analytics and Networking, EdgeSys 2023, in conjunction with ACM EuroSys 2023","8 May 2023","Rome","188212","Conference review","Final","","Scopus","2-s2.0-85159315619"
"","","","EuroMLSys 2022 - Proceedings of the 2nd European Workshop on Machine Learning and Systems","2022","EuroMLSys 2022 - Proceedings of the 2nd European Workshop on Machine Learning and Systems","","","","","","120","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128370657&partnerID=40&md5=c5b855ff155bc9d8d101e66872256363","The proceedings contain 14 papers. The topics discussed include: empirical analysis of federated learning in heterogeneous environments; efficient multiclass classification with duet; reinforcement learning for resource management in multi-tenant serverless platforms; live video analytics as a service; deep learning on microcontrollers: a study on deployment costs and challenges; syslrn: learning what to monitor for efficient anomaly detection; data selection for efficient model update in federated learning; scaling knowledge graph embedding models; temporal shift reinforcement learning; Apache submarine: a unified machine learning platform made simple; and DyFiP: explainable ai-based dynamic filter pruning of convolutional neural networks.","","","Association for Computing Machinery, Inc","ACM SIGOPS","2nd European Workshop on Machine Learning and Systems, EuroMLSys 2022, in conjunction with ACM EuroSys 2022","5 April 2022 through 8 April 2022","Virtual, Online","178630","Conference review","Final","","Scopus","2-s2.0-85128370657"
"Yang Y.; Zhao L.; Li Y.; Zhang H.; Li J.; Zhao M.; Chen X.; Li K.","Yang, Yanan (57208671879); Zhao, Laiping (35243865000); Li, Yiming (57212459721); Zhang, Huanyu (57489072800); Li, Jie (56313960700); Zhao, Mingyang (58362525300); Chen, Xingzhen (57488969700); Li, Keqiu (57204189178)","57208671879; 35243865000; 57212459721; 57489072800; 56313960700; 58362525300; 57488969700; 57204189178","INFless: A native serverless system for low-latency, high-Throughput inference","2022","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","","","","768","781","13","97","10.1145/3503222.3507709","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126395137&doi=10.1145%2f3503222.3507709&partnerID=40&md5=77111f3b4bcd772e31db90137e9e59b9","Modern websites increasingly rely on machine learning (ML) to improve their business efficiency. Developing and maintaining ML services incurs high costs for developers. Although serverless systems are a promising solution to reduce costs, we find that the current general purpose serverless systems cannot meet the low latency, high throughput demands of ML services. While simply ""patching""general serverless systems does not resolve the problem completely, we propose that such a system should natively combine the features of inference with a serverless paradigm. We present INFless, the first ML domain-specific serverless platform. It provides a unified, heterogeneous resource abstraction between CPU and accelerators, and achieves high throughput using built-in batching and non-uniform scaling mechanisms. It also supports low latency through coordinated management of batch queuing time, execution time and coldstart rate. We evaluate INFless using both a local cluster testbed and a large-scale simulation. Experimental results show that INFless outperforms state-of-The-Art systems by 2×-5× on system throughput, meeting the latency goals of ML services.  © 2022 ACM.","Inference System; Machine Learning; Serverless Computing","'current; Business efficiency; High costs; High-throughput; Inference systems; Low latency; Machine-learning; Reduce costs; Serverless computing; Serverless systems; Machine learning","Association for Computing Machinery","ACM; ACM SIGARCH; ACM SIGOPS; ACM SIGPLAN","27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2022","28 February 2022 through 4 March 2022","Virtual, Online","177381","Conference paper","Final","","Scopus","2-s2.0-85126395137"
"Carver B.; Han R.; Zhang J.; Zheng M.; Cheng Y.","Carver, Benjamin (57215278740); Han, Runzhou (57219751944); Zhang, Jingyuan (57215280960); Zheng, Mai (57139384400); Cheng, Yue (56022559100)","57215278740; 57219751944; 57215280960; 57139384400; 56022559100","λfS: A Scalable and Elastic Distributed File System Metadata Service using Serverless Functions","2023","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","4","","","394","411","17","5","10.1145/3623278.3624765","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196880635&doi=10.1145%2f3623278.3624765&partnerID=40&md5=6472e1099e544f01c8c50b97733c83fc","The metadata service (MDS) sits on the critical path for distributed file system (DFS) operations, and therefore it is key to the overall performance of a large-scale DFS. Common ""serverful""MDS architectures, such as a single server or cluster of servers, have a significant shortcoming: either they are not scalable, or they make it difficult to achieve an optimal balance of performance, resource utilization, and cost. A modern MDS requires a novel architecture that addresses this shortcoming. To this end, we design and implement λFS, an elastic, highperformance metadata service for large-scale DFSes. λFS scales a DFS metadata cache elastically on a FaaS (Function-as-a-Service) platform and synthesizes a series of techniques to overcome the obstacles that are encountered when building large, stateful, and performance-sensitive applications on FaaS platforms. λFS takes full advantage of the unique benefits offered by FaaS.elastic scaling and massive parallelism.to realize a highly-optimized metadata service capable of sustaining up to 4.13× higher throughput, 90.40% lower latency, 85.99% lower cost, 3.33× better performance-per-cost, and better resource utilization and efficiency than a state-of-the-art DFS for an industrial workload. © 2023 Copyright held by the owner/author(s).","","Data as a service (DaaS); File editors; Resource allocation; Critical Paths; Distributed file systems; Large-scales; Metadata services; Performance; Resources utilizations; Service platforms; Services Architectures; Single server; Systems operation; Metadata","Association for Computing Machinery","ACM SIGARCH; ACM SIGOPS; ACM SIGPLAN","28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2023","25 March 2023 through 29 March 2023","Vancouver","201666","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85196880635"
"Bilal M.; Canini M.; Fonseca R.; Rodrigues R.","Bilal, Muhammad (57222277703); Canini, Marco (24469881600); Fonseca, Rodrigo (59847856900); Rodrigues, Rodrigo (13008425200)","57222277703; 24469881600; 59847856900; 13008425200","With Great Freedom Comes Great Opportunity: Rethinking Resource Allocation for Serverless Functions","2023","Proceedings of the 18th European Conference on Computer Systems, EuroSys 2023","","","","381","397","16","28","10.1145/3552326.3567506","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160202037&doi=10.1145%2f3552326.3567506&partnerID=40&md5=0b808795dad6511a42e267d19d22ed52","Current serverless offerings give users limited flexibility for configuring the resources allocated to their function invocations. This simplifies the interface for users to deploy serverless computations but creates deployments that are resource inefficient. In this paper, we take a principled approach to the problem of resource allocation for serverless functions, analyzing the effects of automating this choice in a way that leads to the best combination of performance and cost. In particular, we systematically explore the opportunities that come with decoupling memory and CPU resource allocations and also enabling the use of different VM types, and we find a rich trade-off space between performance and cost. The provider can use this in a number of ways, e.g., exposing all these parameters to the user; eliding preferences for performance and cost from users and simply offer the same performance with lower cost; or exposing a small number of choices for users to trade performance for cost. Our results show that, by decoupling memory and CPU allocation, there is the potential to have up to 40% lower execution cost than the preset coupled configurations that are the norm in current serverless offerings. Similarly, making the correct choice of VM instance type can provide up to 50% better execution time. Furthermore, we demonstrate that providers have the flexibility to choose different instance types for the same functions to maximize resource utilization while providing performance within 10-20% of the best resource configuration for each respective function. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Optimization; Resource Allocation; Serverless","Costs; Economic and social effects; 'current; CPU resources; Decouplings; Memory resources; Optimisations; Performance; Resources allocation; Serverless; Space between; Trade off; Resource allocation","Association for Computing Machinery, Inc","Ant Group Research; et al.; Google; Huawei; KAUST; Meta","18th European Conference on Computer Systems, EuroSys 2023","8 May 2023 through 12 May 2023","Rome","188495","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85160202037"
"Iosup A.; Prodan R.","Iosup, Alexandru (23392350500); Prodan, Radu (8858675500)","23392350500; 8858675500","ICPE'23 GraphSys Workshop Chairs Introduction (Welcome)","2023","ICPE 2023 - Companion of the 2023 ACM/SPEC International Conference on Performance Engineering","","","","207","208","1","0","10.1145/3578245.3585328","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158862387&doi=10.1145%2f3578245.3585328&partnerID=40&md5=e7529ceefad3c20ac52da2ef3cbfb45a","[No abstract available]","graph processing; graphsys serverless, extreme-scale, and sustainable graph processing systems; workshop","","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC Research","14th Annual ACM/SPEC International Conference on Performance Engineering, ICPE 2023","15 April 2023 through 19 April 2023","Coimbra","187962","Editorial","Final","","Scopus","2-s2.0-85158862387"
"Meladakis K.; Zeginis C.; Magoutis K.; Plexousakis D.","Meladakis, Kostas (58041949000); Zeginis, Chrysostomos (55345816500); Magoutis, Kostas (22035826800); Plexousakis, Dimitris (6602157883)","58041949000; 55345816500; 22035826800; 6602157883","Transferring transactional business processes to FaaS","2022","WoSC 2022 - Proceedings of the 8th International Workshop on Serverless Computing, Part of Middleware 2022","","","","25","30","5","3","10.1145/3565382.3565882","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145555314&doi=10.1145%2f3565382.3565882&partnerID=40&md5=35bdfced9355072b7473caa78f982a17","Function-as-a-Service (FaaS) is a modern cloud service model that has gained significant attention from the research and industry communities in recent years for its many benefits such as dynamic scaling, cost efficiency, faster programming, flexibility to microservices and containers technology. However, the building and deployment of serverless applications come with many challenges that need to be tackled, like workflow design complexity and migration of other applications. When transactions between different parties are involved, the workflow becomes knotty and the communication between participants and all properties of transactions have to be properly resolved. Transactions have widely been discussed in Business processes, so same practices might be adopted by serverless workflows. In this work we provide guidelines and mapping mechanisms for transforming transactional Business Process Modeling Notation 2.0 (BPMN2) applications to a serverless platform. We shed light on the current inability of function orchestrators to express workflow definitions, and deal with various architectural dilemmas that stem from the dissimilar nature of stateful BPMN vs. stateless serverless applications. We overcome the unbalanced capabilities between well-established BPMN notations and function orchestration definitions and illustrate how to exploit and combine cloud native services that comes with FaaS to create serverless applications.  © 2022 ACM.","BPMN2; FaaS; function orchestration; OpenWhisk; serverless workflow; transactions","Business Process; Business process modeling; Business process modeling notation 2.0; Function orchestration; Function-as-a-service; Modeling notation; Openwhisk; Serverless workflow; Transaction; Work-flows; Industrial research","Association for Computing Machinery, Inc","ACM; ETS; Raytheon BBN; rti","8th International Workshop on Serverless Computing, WoSC 2022 - Part of Middleware 2022","7 November 2022","Quebec","184997","Conference paper","Final","","Scopus","2-s2.0-85145555314"
"Habenicht D.; Kreutz K.; Becker S.; Bader J.; Thamsen L.; Kao O.","Habenicht, Daniel (58708904400); Kreutz, Kevin (57572919300); Becker, Soeren (57188812145); Bader, Jonathan (57221101513); Thamsen, Lauritz (55222001800); Kao, Odej (7004362997)","58708904400; 57572919300; 57188812145; 57221101513; 55222001800; 7004362997","SyncMesh: Improving Data Locality for Function-As-A-Service in Meshed Edge Networks","2022","EdgeSys 2022 - Proceedings of the 5th International Workshop on Edge Systems, Analytics and Networking, Part of EuroSys 2022","","","","55","60","5","5","10.1145/3517206.3526275","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128367528&doi=10.1145%2f3517206.3526275&partnerID=40&md5=91dd2c8e9a2ef0963054b0a079e644ce","The increasing use of Internet of Things devices coincides with more communication and data movement in networks, which can exceed existing network capabilities. These devices often process sensor or user information, where data privacy and latency are a major concern. Therefore, traditional approaches like cloud computing do not fit well, yet new architectures such as edge computing address this gap. In addition, the Function-As-A-Service (FaaS) paradigm gains in prevalence as a workload execution platform, however the decoupling of storage results in further challenges for highly distributed edge environments. To address this, we propose SyncMesh, a system to manage, query, and transform data in a scalable and stateless manner by leveraging the capabilities of Function-As-A-Service and at the same time enabling data locality. Furthermore, we provide a prototypical implementation and evaluate it against established centralized and decentralized systems in regard to traffic usage and request times. The preliminary results indicate that SyncMesh is able to exonerate the network layer and accelerate the transmission of data to clients, while simultaneously improving local data processing.  © 2022 ACM.","data locality; data management; edge computing; fog computing; function-As-A-service; mesh network","Data privacy; Digital storage; Fog computing; Information management; MESH networking; Network layers; Search engines; Data locality; Data movements; Edge computing; EDGE Networks; Function-as-A-service; In networks; MeshNetworks; Network capability; Process sensor; Sensor informations; Edge computing","Association for Computing Machinery, Inc","ACM SIGOPS","5th International Workshop on Edge Systems, Analytics and Networking, EdgeSys 2022, in conjunction with ACM EuroSys 2022","5 April 2022 through 8 April 2022","Virtual, Online","178625","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85128367528"
"Asheim T.; Khan T.A.; Kasicki B.; Kumar R.","Asheim, Truls (57215898608); Khan, Tanvir Ahmed (57188934366); Kasicki, Baris (58041985000); Kumar, Rakesh (57202403829)","57215898608; 57188934366; 58041985000; 57202403829","Impact of microarchitectural state reuse on serverless functions","2022","WoSC 2022 - Proceedings of the 8th International Workshop on Serverless Computing, Part of Middleware 2022","","","","7","12","5","3","10.1145/3565382.3565879","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145560340&doi=10.1145%2f3565382.3565879&partnerID=40&md5=86274dd235d99a487b95722a821e6525","Serverless computing has seen rapid growth in the past few years due to its seamless scalability and zero resource provisioning overhead for developers. In serverless, applications are composed of a set of very short-running functions which are invoked in response to events such as HTTP requests. For better resource utilization, cloud providers interleave the execution of thousands of serverless functions on a single server. Recent work argues that this interleaved execution and short run-times cause the serverless functions to perform poorly on modern processors. This is because interleaved execution thrashes the microarchitectural state of a function, thus forcing its subsequent execution to start from a cold state. Further, due to their short-running nature, serverless functions are unable to amortize the warm-up latency of microarchitectural structures, meaning that most the function execution happen from cold state. In this work, we analyze a function's performance sensitivity to microarchitectural state thrashing induced by interleaved execution. Unlike prior work, our analysis reveals that not all functions experience performance degradation because of microarchitectural state thrashing. The two dominating factors that dictate the impact of thrashing on function performance are function execution time and code footprint. For example, we observe that only the functions with short execution times (< 1 ms) show performance degradation due to thrashing and that this degradation is exacerbated for functions with large code footprints.  © 2022 Owner/Author.","FaaS; measurement; microarchitecture; serverless; top-down","Codes (symbols); Cold state; Faas; Micro architectures; Microarchitectural state; Performance degradation; Rapid growth; Resources utilizations; Reuse; Serverless; Topdown; Computer architecture","Association for Computing Machinery, Inc","ACM; ETS; Raytheon BBN; rti","8th International Workshop on Serverless Computing, WoSC 2022 - Part of Middleware 2022","7 November 2022","Quebec","184997","Conference paper","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85145560340"
"Du D.; Liu Q.; Jiang X.; Xia Y.; Zang B.; Chen H.","Du, Dong (57200438686); Liu, Qingyuan (57219510222); Jiang, Xueqiang (57238665400); Xia, Yubin (7403027696); Zang, Binyu (6701320221); Chen, Haibo (55743141500)","57200438686; 57219510222; 57238665400; 7403027696; 6701320221; 55743141500","Serverless Computing on Heterogeneous Computers","2022","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","","","","797","813","16","54","10.1145/3503222.3507732","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126390804&doi=10.1145%2f3503222.3507732&partnerID=40&md5=a81be85ab5dc2c279001c457591ba54e","Existing serverless computing platforms are built upon homogeneous computers, limiting the function density and restricting serverless computing to limited scenarios. We introduce Molecule, the first serverless computing system utilizing heterogeneous computers. Molecule enables both general-purpose devices (e.g., Nvidia DPU) and domain-specific accelerators (e.g., FPGA and GPU) for serverless applications that significantly improve function density (50% higher) and application performance (up to 34.6x). To achieve these results, we first propose XPU-Shim, a distributed shim to bridge the gap between underlying multi-OS systems (when using general-purpose devices) and our serverless runtime (i.e., Molecule). We further introduce vectorized sandbox, a sandbox abstraction to abstract hardware heterogeneity (when using domain-specific accelerators). Moreover, we also review state-of-The-Art serverless optimizations on startup and communication latency and overcome the challenges to implement them on heterogeneous computers. We have implemented Molecule on real platforms with Nvidia DPUs and Xilinx FPGAs and evaluate it using benchmarks and real-world applications.  © 2022 ACM.","Cloud computing; function-As-A-service; heterogeneous computers; operating system; serverless computing","Benchmarking; Computer operating systems; Field programmable gate arrays (FPGA); Application performance; Cloud-computing; Computing platform; Computing system; Domain specific; Function-as-A-service; Heterogeneous computers; Operating system; Runtimes; Serverless computing; Molecules","Association for Computing Machinery","ACM; ACM SIGARCH; ACM SIGOPS; ACM SIGPLAN","27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2022","28 February 2022 through 4 March 2022","Virtual, Online","177381","Conference paper","Final","","Scopus","2-s2.0-85126390804"
"Galimberti E.; Guindani B.; Filippini F.; Sedghani H.; Ardagna D.; Moltó G.; Caballer M.","Galimberti, Enrico (58241772100); Guindani, Bruno (57712327500); Filippini, Federica (57222342275); Sedghani, Hamta (57232110300); Ardagna, Danilo (6508040906); Moltó, Germán (8953108200); Caballer, Miguel (22733776500)","58241772100; 57712327500; 57222342275; 57232110300; 6508040906; 8953108200; 22733776500","OSCAR-P and aMLLibrary: Performance Profiling and Prediction of Computing Continua Applications","2023","ICPE 2023 - Companion of the 2023 ACM/SPEC International Conference on Performance Engineering","","","","139","146","7","9","10.1145/3578245.3584941","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158883808&doi=10.1145%2f3578245.3584941&partnerID=40&md5=8a719865ca3f15d482ca499b163d5c51","This paper proposes an auto-profiling tool for OSCAR, an open-source platform able to support serverless computing in cloud and edge environments. The tool, named OSCAR-P, is designed to automatically test a specified application workflow on different hardware and node combinations, obtaining relevant information on the execution time of the individual components. It then uses the collected data to build performance models using machine learning, making it possible to predict the performance of the application on unseen configurations. The preliminary evaluation of the performance models accuracy is promising, showing a mean absolute percentage error for extrapolation lower than 10%. © 2023 ACM.","edge computing; machine learning; performance profiling","Edge computing; Open systems; Automatically test; Edge computing; Individual components; Machine-learning; Open source platforms; Performance; Performance Modeling; Performance profiling; Profiling tools; Work-flows; Machine learning","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC Research","14th Annual ACM/SPEC International Conference on Performance Engineering, ICPE 2023","15 April 2023 through 19 April 2023","Coimbra","187962","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85158883808"
"Kousiouris G.; Ambroziak S.; Zarzycki B.; Costantino D.; Tsarsitalidis S.; Katevas V.; Mamelli A.; Stamati T.","Kousiouris, George (26031026200); Ambroziak, Szymon (57476220200); Zarzycki, Blazej (58242673000); Costantino, Domenico (57476070300); Tsarsitalidis, Stylianos (57209306915); Katevas, Vasileios (58242673100); Mamelli, Alessandro (23470097100); Stamati, Teta (6506964629)","26031026200; 57476220200; 58242673000; 57476070300; 57209306915; 58242673100; 23470097100; 6506964629","A Pattern-based Function and Workflow Visual Environment for FaaS Development across the Continuum","2023","ICPE 2023 - Companion of the 2023 ACM/SPEC International Conference on Performance Engineering","","","","165","172","7","5","10.1145/3578245.3584934","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158828920&doi=10.1145%2f3578245.3584934&partnerID=40&md5=a997be3ce0ca8e29fe3910c598ec139b","The ability to split applications across different locations in the continuum (edge/cloud) creates needs for application break down into smaller and more distributed chunks. In this realm the Function as a Service approach appears as a significant enabler in this process. The paper presents a visual function and workflow development environment for complex FaaS (Apache OpenwhisK) applications. The environment offers a library of pattern based and reusable nodes and flows while mitigating function orchestration limitations in the domain. Generation of the deployable artefacts, i.e. the functions, is performed through embedded DevOps pipelines. A range of annotations are available for dictating diverse options including QoS needs, function or data locality requirements, function affinity considerations etc. These are propagated to the deployment and operation stacks for supporting the cloud/edge interplay. The mechanism is evaluated functionally through creating, registering and executing functions and orchestrating workflows, adapting typical parallelization patterns and an edge data collection process. © 2023 Owner/Author.","function as a service; function orchestration; serverless computing; software development","Software design; Break down; Data locality; Development environment; Edge clouds; Function as a service; Function orchestration; Serverless computing; Visual environments; Visual functions; Work-flows; Computer software reusability","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC Research","14th Annual ACM/SPEC International Conference on Performance Engineering, ICPE 2023","15 April 2023 through 19 April 2023","Coimbra","187962","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85158828920"
"Straesser M.; Mathiasch J.; Bauer A.; Kounev S.","Straesser, Martin (57223022462); Mathiasch, Jonas (58237290400); Bauer, André (57194177145); Kounev, Samuel (23397538000)","57223022462; 58237290400; 57194177145; 23397538000","A Systematic Approach for Benchmarking of Container Orchestration Frameworks","2023","ICPE 2023 - Proceedings of the 2023 ACM/SPEC International Conference on Performance Engineering","","","","187","198","11","12","10.1145/3578244.3583726","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158141131&doi=10.1145%2f3578244.3583726&partnerID=40&md5=52d1fa94766edd9ad77fd724126b3724","Container orchestration frameworks play a critical role in modern cloud computing paradigms such as cloud-native or serverless computing. They significantly impact the quality and cost of service deployment as they manage many performance-critical tasks such as container provisioning, scheduling, scaling, and networking. Consequently, a comprehensive performance assessment of container orchestration frameworks is essential. However, until now, there is no benchmarking approach that covers the many different tasks implemented in such platforms and supports evaluating different technology stacks. In this paper, we present a systematic approach that enables benchmarking of container orchestrators. Based on a definition of container orchestration, we define the core requirements and benchmarking scope for such platforms. Each requirement is then linked to metrics and measurement methods, and a benchmark architecture is proposed. With COFFEE, we introduce a benchmarking tool supporting the definition of complex test campaigns for container orchestration frameworks. We demonstrate the potential of our approach with case studies of the frameworks Kubernetes and Nomad in a self-hosted environment and on the Google Cloud Platform. The presented case studies focus on container startup times, crash recovery, rolling updates, and more.  © 2023 ACM.","benchmarking; container orchestration; kubernetes; nomad; performance","Containers; Case-studies; Cloud-computing; Computing paradigm; Container orchestration; Cost of service; Kubernetes; Nomad; Performance; Quality-of-service; Service deployment; Benchmarking","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC Research","14th Annual ACM/SPEC International Conference on Performance Engineering, ICPE 2023","15 April 2023 through 19 April 2023","Coimbra","187953","Conference paper","Final","","Scopus","2-s2.0-85158141131"
"Qiu H.; Mao W.; Patke A.; Wang C.; Franke H.; Kalbarczyk Z.T.; Başar T.; Iyer R.K.","Qiu, Haoran (57219785413); Mao, Weichao (57204468822); Patke, Archit (57216790460); Wang, Chen (57157809200); Franke, Hubertus (7202800815); Kalbarczyk, Zbigniew T. (6603680588); Başar, Tamer (7102588845); Iyer, Ravishankar K. (35597694000)","57219785413; 57204468822; 57216790460; 57157809200; 7202800815; 6603680588; 7102588845; 35597694000","SIMPPO: A Scalable and Incremental Online Learning Framework for Serverless Resource Management","2022","SoCC 2022 - Proceedings of the 13th Symposium on Cloud Computing","","","","306","322","16","15","10.1145/3542929.3563475","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143252231&doi=10.1145%2f3542929.3563475&partnerID=40&md5=6f077189f6a7856fd77992376de27c1b","Serverless Function-as-a-Service (FaaS) offers improved programmability for customers, yet it is not server-""less""and comes at the cost of more complex infrastructure management (e.g., resource provisioning and scheduling) for cloud providers. To maintain service-level objectives (SLOs) and improve resource utilization efficiency, recent research has been focused on applying online learning algorithms such as reinforcement learning (RL) to manage resources. Despite the initial success of applying RL, we first show in this paper that the state-of-the-art single-agent RL algorithm (S-RL) suffers up to 4.8x higher p99 function latency degradation on multi-tenant serverless FaaS platforms compared to isolated environments and is unable to converge during training. We then design and implement a scalable and incremental multi-agent RL framework based on Proximal Policy Optimization (SIMPPO). Our experiments demonstrate that in multi-tenant environments, SIMPPO enables each RL agent to efficiently converge during training and provides online function latency performance comparable to that of S-RL trained in isolation with minor degradation (<9.2%). In addition, SIMPPO reduces the p99 function latency by 4.5x compared to S-RL in multi-tenant cases.  © 2022 ACM.","multi-agent; reinforcement learning; serverless computing","E-learning; Learning algorithms; Multi agent systems; Scheduling; Complex infrastructures; Learning frameworks; Multi agent; Multi tenants; Online learning; Programmability; Reinforcement learnings; Resource management; Serverless computing; Single-agent; Reinforcement learning","Association for Computing Machinery, Inc","ACM Special Interest Grou on Operating Systems (SIGOPS); ACM Special Interest Group on Management of Data (SIGMOD); EBay; et al.; FutureWei; Microsoft","13th Annual ACM Symposium on Cloud Computing, SoCC 2022","7 November 2022 through 11 November 2022","San Francisco","184095","Conference paper","Final","","Scopus","2-s2.0-85143252231"
"Mahéo A.; Sutra P.; Tarrant T.","Mahéo, Aurèle (55250328600); Sutra, Pierre (23467592200); Tarrant, Tristan (57219688857)","55250328600; 23467592200; 57219688857","The serverless shell","2021","Middleware 2021 Industry Track - Proceedings of the 2021 International Middleware Conference Industrial Track","","","","9","15","6","6","10.1145/3491084.3491426","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121449455&doi=10.1145%2f3491084.3491426&partnerID=40&md5=c587d02dec98a519a049e5c4d2902c55","Serverless computing is a recent paradigm to program the cloud. It allows to execute user-defined functions at scale, on demand, and in a pay-per-use manner. This paper reports on adapting the Unix shell for serverless. Our software, called the serverless shell (sshell), runs shell scripts on a serverless platform much like with a regular computer. It permits to reuse an existing code base while benefiting from the massive power of serverless and paying only for the resources used. sshell is built around a small set of components that includes a new inter-process communication layer for serverless. We evaluate it in AWS Lambda using several microbenchmarks and a large-scale application. Our results show that sshell achieves comparable or better performance than a high-end server. Moreover, it can be faster and more cost-efficient than a cluster-based solution to mine large datasets.  © 2021 ACM.","function-as-a-service; serverless computing; shell","Shells (structures); Communication layers; Function-as-a-service; Interprocess communication; On demands; Pay-per-use; Power; Reuse; Serverless computing; Sshell; User Defined Functions; Large dataset","Association for Computing Machinery, Inc","ACM; ETS; University Laval","22nd International Middleware Conference, Middleware 2021","6 December 2021 through 10 December 2021","Virtual, Online","175054","Conference paper","Final","","Scopus","2-s2.0-85121449455"
"Park S.; Choi J.; Lee K.","Park, Subin (57847633000); Choi, Jaeghang (57221607405); Lee, Kyungyong (57196193080)","57847633000; 57221607405; 57196193080","All-You-Can-Inference : Serverless DNN Model Inference Suite","2022","WoSC 2022 - Proceedings of the 8th International Workshop on Serverless Computing, Part of Middleware 2022","","","","1","6","5","6","10.1145/3565382.3565878","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145567418&doi=10.1145%2f3565382.3565878&partnerID=40&md5=2eb8cc2ccdb6baa8bc0b071153193d32","Serverless computing becomes prevalent and is widely adopted for various applications. Deep learning inference tasks are appropriate to be deployed using a serverless architecture due to the nature of fluctuating task arrival events. When serving a Deep Neural Net (DNN) model in a serverless computing environment, there exist many performance optimization opportunities, including various hardware support, model graph optimization, hardware-agnostic model compilation, memory size and batch size configurations, and many others. Although the serverless computing frees users from cloud resource management overhead, it is still very challenging to find an optimal serverless DNN inference environment among a very large optimization opportunities for the configurations. In this work, we propose All-You-Can-Inference (AYCI), which helps users to find an optimally operating DNN inference in a publicly available serverless computing environment. We have built the proposed system as a service using various fully-managed cloud services and open-sourced the system to help DNN application developers to build an optimal serving environment. The prototype implementation and initial experiment result present the difficulty of finding an optimal DNN inference environment with the varying performance.  © 2022 ACM.","DNN inference; serverless computing","Computer hardware; Open systems; Computing environments; Deep neural net inference; Deep neural nets; Graph optimization; Hardware supports; Model inference; Net model; Performance optimizations; Serverless architecture; Serverless computing; Deep neural networks","Association for Computing Machinery, Inc","ACM; ETS; Raytheon BBN; rti","8th International Workshop on Serverless Computing, WoSC 2022 - Part of Middleware 2022","7 November 2022","Quebec","184997","Conference paper","Final","","Scopus","2-s2.0-85145567418"
"Samanta A.; Stutsman R.","Samanta, Amit (56767642400); Stutsman, Ryan (14055061000)","56767642400; 14055061000","A Case of Multi-Resource Fairness for Serverless Workflows (Work In Progress Paper)","2023","ICPE 2023 - Companion of the 2023 ACM/SPEC International Conference on Performance Engineering","","","","45","50","5","6","10.1145/3578245.3585033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158846693&doi=10.1145%2f3578245.3585033&partnerID=40&md5=c449d5856e5ec59836a20af564be74d0","Serverless platforms have exploded in popularity in recent years, but, today, these platforms are still unsuitable for large classes of applications. They perform well for batch-oriented workloads that perform coarse transformations over data asynchronously, but their lack of clear service level agreements (SLAs), high per-invocation overheads, and interference make deploying online applications with stringent response time demands impractical. Our assertion is that beyond the glaring issues like cold start costs, a more fundamental shift is needed in how serverless function invocations are provisioned and scheduled in order to support these more demanding applications. Specifically, we propose a platform that leverages the observability and predictability of serverless functions to enforce multi-resource fairness. We explain why we believe interference across a spectrum of resources (CPU, network, and storage) contributes to lower resource utilization and poor response times for latency-sensitive and high-fanout serverless application patterns. Finally, we propose a new distributed and hierarchical function scheduling architecture that combines lessons from multi-resource fair scheduling, hierarchical scheduling, batch-analytics resource scheduling, and statistics to create an approach that we believe will enable tighter SLAs on serverless platforms than has been possible in the past. © 2023 ACM.","fairness; multi-resource management; serverless workflows; SLA","Network architecture; Coarse transformation; Fairness; Multi-resource; Multi-resource management; On-line applications; Resource management; Serverless workflow; Servicelevel agreement (SLA); Stringents; Work-flows; Digital storage","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC Research","14th Annual ACM/SPEC International Conference on Performance Engineering, ICPE 2023","15 April 2023 through 19 April 2023","Coimbra","187962","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85158846693"
"","","","ICPE 2022 - Companion of the 2022 ACM/SPEC International Conference on Performance Engineering","2022","ICPE 2022 - Companion of the 2022 ACM/SPEC International Conference on Performance Engineering","","","","","","156","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136007267&partnerID=40&md5=bcee935932e7926cbd31ee0c711b9967","The proceedings contain 26 papers. The topics discussed include: B-MEG: bottlenecked-microservices extraction using graph neural networks; SPEC research – introducing the predictive data analytics working group; MAPLE: model aggregation and prediction for learned ecosystem; SPEC efficiency benchmark development: how to contribute to the future of energy conservation; optimizing the performance of fog computing environments using AI and co-simulation; beware of the interactions of variability layers when reasoning about evolution of MongoDB; change point detection for MongoDB time series performance regression; beauty and the beast: a case study on performance prototyping of data-intensive containerized cloud applications; measuring baseline overheads in different orchestration mechanisms for large FaaS workflows; and TaskFlow: an energy- and makespan-aware task placement policy for workflow scheduling through delay management.","","","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC","13th Annual ACM/SPEC International Conference on Performance Engineering, ICPE 2022","9 April 2022 through 13 April 2022","Virtual, Online","181030","Conference review","Final","","Scopus","2-s2.0-85136007267"
"Eizaguirre G.T.; Sánchez-Artigas M.; García-López P.","Eizaguirre, Germán T. (57221861497); Sánchez-Artigas, Marc (10939155200); García-López, Pedro (24479469800)","57221861497; 10939155200; 24479469800","A milestone for FaaS pipelines; Object storage-vs VM-driven data exchange","2021","Middleware 2021 Demos and Posters - Proceedings of the 2021 International Middleware Conference Demos and Posters","","","","10","11","1","0","10.1145/3491086.3492472","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121423363&doi=10.1145%2f3491086.3492472&partnerID=40&md5=604959486fb86e43442d081c0ae1635f","Serverless functions provide high levels of parallelism, short startup times, and ""pay-as-you-go""billing. These attributes make them a natural substrate for data analytics workflows. However, the impossibility of direct communication between functions makes the execution of workflows challenging. The current practice to share intermediate data among functions is through remote object storage (e.g., IBM COS). Contrary to conventional wisdom, the performance of object storage is not well understood. For instance, object storage can even be superior to other simpler approaches like the execution of shuffle stages (e.g., GroupBy) inside powerful VMs to avoid all-to-all transfers between functions. Leveraging a genomics pipeline, we show that object storage is a reasonable choice for data passing when the appropriate number of functions is used in shuffling stages.  © 2021 ACM.","function-as-a-service; serverless computing","Digital storage; Electronic data interchange; Pipelines; Current practices; Data analytics; Direct communications; Function-as-a-service; Natural substrates; Object storages; Pay as you go; Serverless computing; Startup time; Work-flows; Data Analytics","Association for Computing Machinery, Inc","ACM; ETS; University Laval","22nd International Middleware Conference, Middleware 2021","6 December 2021 through 10 December 2021","Virtual, Online","175055","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85121423363"
"","","","ASPLOS 2023 - Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems","2023","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","4","","","","","426","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202197089&partnerID=40&md5=c409fcb10e60e71022219e2b73c72d04","The proceedings contain 23 papers. The topics discussed include: manticore: hardware-accelerated RTL simulation with static bulk-synchronous parallelism; exploiting the regular structure of modern quantum architectures for compiling and optimizing programs with permutable operators; MiniMalloc: a lightweight memory allocator for hardware-accelerated machine learning; DREAM: a dynamic scheduler for dynamic real-time multi-model ml workloads; supporting descendants in SIMD-accelerated JSONPath; DataFlower: exploiting the data-flow paradigm for serverless workflow orchestration; predict; don’t react for enabling efficient fine-grain DVFS in GPUs; LightRidge: an end-to-end agile design framework for diffractive optical neural networks; and Sleuth: a trace-based root cause analysis system for large-scale microservices with graph neural networks.","","","Association for Computing Machinery","ACM SIGARCH; ACM SIGOPS; ACM SIGPLAN","28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2023","25 March 2023 through 29 March 2023","Vancouver","201666","Conference review","Final","","Scopus","2-s2.0-85202197089"
"Moghimi A.; Hattori J.; Li A.; Chikha M.B.; Shahrad M.","Moghimi, Arshia (58743712600); Hattori, Joe (58041984900); Li, Alexander (58743601700); Chikha, Mehdi Ben (58743495100); Shahrad, Mohammad (56943394900)","58743712600; 58041984900; 58743601700; 58743495100; 56943394900","Parrotfish: Parametric Regression for Optimizing Serverless Functions","2023","SoCC 2023 - Proceedings of the 2023 ACM Symposium on Cloud Computing","","","","177","192","15","7","10.1145/3620678.3624654","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178519855&doi=10.1145%2f3620678.3624654&partnerID=40&md5=2b3a180ec25660d3d9df3fd35b2a2762","Serverless computing is a new paradigm that aims to remove the burdens of cloud management from developers. Yet rightsizing serverless functions remains a pain point for developers. Choosing the right memory configuration is necessary to ensure cost and/or performance optimality for serverless workloads. In this work, we identify that using parametric regression can significantly simplify function rightsizing compared to black-box optimization techniques currently available. With this insight, we build a tool, called Parrotfish, which finds optimal configurations through an online learning process. It also allows users to communicate constraints on execution time, or to relax cost optimality to gain performance. Parrotfish achieves substantially lower exploration costs (1.81-9.96×) compared with the state-of-the-art tools, while delivering similar or better recommendations. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Performance Modeling; Serverless Computing","Black-box optimization; Cloud managements; Cost performance; Memory configuration; Online learning; Optimality; Optimization techniques; Parametric regression; Performance Modeling; Serverless computing; Optimization","Association for Computing Machinery, Inc","ACM SIGMOD; ACM SIGOPS; ActiveLoop; Google; IBM; Microsoft","14th ACM Symposium on Cloud Computing, SoCC 2023","30 October 2023 through 1 November 2023","Santa Cruz","194526","Conference paper","Final","","Scopus","2-s2.0-85178519855"
"Yang Y.; Zhao L.; Li Y.; Wu S.; Hao Y.; Ma Y.; Li K.","Yang, Yanan (57208671879); Zhao, Laiping (35243865000); Li, Yiming (57212459721); Wu, Shihao (59300318900); Hao, Yuechan (56868028400); Ma, Yuchi (57832420700); Li, Keqiu (57204189178)","57208671879; 35243865000; 57212459721; 59300318900; 56868028400; 57832420700; 57204189178","Flame: A Centralized Cache Controller for Serverless Computing","2023","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","4","","","153","168","15","5","10.1145/3623278.3624769","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202195345&doi=10.1145%2f3623278.3624769&partnerID=40&md5=ad0ca66de4484398dd956a3e24001927","Caching function is a promising way to mitigate coldstart overhead in serverless computing. However, as caching also increases the resource cost significantly, how to make caching decisions is still challenging.We find that the prior ""local cache control""designs are insufficient to achieve high cache efficiency due to the workload skewness across servers. In this paper, inspired by the idea of software defined network management, we propose Flame, an efficient cache system to manage cached functions with a ""centralized cache control""design. By decoupling the cache control plane from local servers and setting up a separate centralized controller, Flame is able to make caching decisions considering a global viewof cluster status, enabling the optimized cache-hit ratio and resource efficiency. We evaluate Flame with real-world workloads and the evaluation results show that it can reduce the cache resource usage by 36% on average while improving the coldstart ratio by nearly 7× than the state-of-the-art method. © 2023 Copyright held by the owner/author(s).","Coldstart; Hotspot Function; Keep-alive; Serverless Computing","Cache memory; Decision making; Software defined networking; Cache control; Cache controller; Caching decisions; Centralised; Cold-start; Control design; Hotspot function; Hotspots; Keep-alive; Serverless computing; Efficiency","Association for Computing Machinery","ACM SIGARCH; ACM SIGOPS; ACM SIGPLAN","28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2023","25 March 2023 through 29 March 2023","Vancouver","201666","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85202195345"
"Wang J.; Lévai T.; Li Z.; Vieira M.A.M.; Govindan R.; Raghavan B.","Wang, Jianfeng (57220786013); Lévai, Tamás (57188649554); Li, Zhuojin (57195478257); Vieira, Marcos A. M. (34772211100); Govindan, Ramesh (35595955700); Raghavan, Barath (56249315600)","57220786013; 57188649554; 57195478257; 34772211100; 35595955700; 56249315600","Quadrant: A Cloud-Deployable NF Virtualization Platform","2022","SoCC 2022 - Proceedings of the 13th Symposium on Cloud Computing","","","","493","509","16","13","10.1145/3542929.3563471","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143251454&doi=10.1145%2f3542929.3563471&partnerID=40&md5=5621390a72a23bd5cd3c64ed3163e370","Network Functions (NFs) now process a significant fraction of Internet traffic. Software-based NF Virtualization (NFV) promised to enable rapid development of new NFs by vendors and leverage the power and economics of commodity computing infrastructure for NF deployment. To date, no cloud NFV systems achieve NF chaining, isolation, SLO-adherence, and scaling together with existing cloud computing infrastructure and abstractions, all while achieving generality, speed, and ease of deployment. These properties are taken for granted in other cloud contexts but unavailable for NF processing. We present Quadrant, an efficient and secure cloud-deployable NFV system, and show that Quadrant's approach of adapting existing cloud infrastructure to support packet processing can achieve NF chaining, isolation, generality, and performance in NFV. Quadrant reuses common cloud infrastructure such as Kubernetes, serverless, the Linux kernel, NIC hardware, and switches. It enables easy NFV deployment while delivering up to double the performance per core compared to the state of the art.  © 2022 Owner/Author.","network functions; service chain; virtualization","Computer operating systems; Transfer functions; Virtual reality; Cloud computing infrastructures; Cloud infrastructures; Computing infrastructures; Internet traffic; Network functions; Performance; Power; Scalings; Service chain; Virtualizations; Network function virtualization","Association for Computing Machinery, Inc","ACM Special Interest Grou on Operating Systems (SIGOPS); ACM Special Interest Group on Management of Data (SIGMOD); EBay; et al.; FutureWei; Microsoft","13th Annual ACM Symposium on Cloud Computing, SoCC 2022","7 November 2022 through 11 November 2022","San Francisco","184095","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85143251454"
"Wanninger N.C.; Bowden J.J.; Shetty K.; Garg A.; Hale K.C.","Wanninger, Nicholas C. (57223744377); Bowden, Joshua J. (57223751192); Shetty, Kirtankumar (57573935300); Garg, Ayush (57572981600); Hale, Kyle C. (55420868300)","57223744377; 57223751192; 57573935300; 57572981600; 55420868300","Isolating Functions at the Hardware Limit with Virtines","2022","EuroSys 2022 - Proceedings of the 17th European Conference on Computer Systems","","","","644","662","18","16","10.1145/3492321.3519553","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128046741&doi=10.1145%2f3492321.3519553&partnerID=40&md5=58d2b5d840dc374dd15d283889e2006c","An important class of applications, including programs that leverage third-party libraries, programs that use user-defined functions in databases, and serverless applications, benefit from isolating the execution of untrusted code at the granularity of individual functions or function invocations. However, existing isolation mechanisms were not designed for this use case; rather, they have been adapted to it. We introduce virtines, a new abstraction designed specifically for function granularity isolation, and describe how we build virtines from the ground up by pushing hardware virtualization to its limits. Virtines give developers fine-grained control in deciding which functions should run in isolated environments, and which should not. The virtine abstraction is a general one, and we demonstrate a prototype that adds extensions to the C language. We present a detailed analysis of the overheads of running individual functions in isolated VMs, and guided by those findings, we present Wasp, an embeddable hypervisor that allows programmers to easily use virtines. We describe several representative scenarios that employ individual function isolation, and demonstrate that virtines can be applied in these scenarios with only a few lines of changes to existing codebases and with acceptable slowdowns.  © 2022 ACM.","Isolation; Virtines; Virtualization","Abstracting; C (programming language); Computer hardware; Virtual reality; C language; Fine-grained control; Hardware virtualization; Isolation; Library projects; Third parties; Untrusted code; User Defined Functions; Virtine; Virtualizations; Application programs","Association for Computing Machinery, Inc","ACM SIGOPS","17th European Conference on Computer Systems, EuroSys 2022","5 April 2022 through 8 April 2020","Rennes","178105","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85128046741"
"Lyu X.; Cherkasova L.; Aitken R.; Parmer G.; Wood T.","Lyu, Xiaosu (57216978433); Cherkasova, Ludmila (7005553676); Aitken, Robert (7201960210); Parmer, Gabriel (8431909100); Wood, Timothy (36026983100)","57216978433; 7005553676; 7201960210; 8431909100; 36026983100","Towards Efficient Processing of Latency-Sensitive Serverless DAGs at the Edge","2022","EdgeSys 2022 - Proceedings of the 5th International Workshop on Edge Systems, Analytics and Networking, Part of EuroSys 2022","","","","49","54","5","12","10.1145/3517206.3526274","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128345078&doi=10.1145%2f3517206.3526274&partnerID=40&md5=d709748b6af718f37dcb6ca2bc7b1af7","Many emerging novel applications expect ""near real-Time""processing and responses, which can not be guaranteed by today's Cloud and would require processing at the Edge. Serverless computing is a particularly promising architecture for edge environments since it offers to improve efficiency by precisely scaling resources to meet application needs. As the edge applications grow more complex and get composed from a subset of simpler functions or microservices, there is a need to support more complicated function topologies which can be represented as directed acyclic graphs (DAGs). However, running DAG functions on a serverless platform poses new challenges related to interconnecting, instantiating, and scheduling function sandboxes. In this paper1, we explore how Sledge, a Wasm-based serverless runtime, can be extended to support DAG functions. Sledge's unique design allows for extremely lightweight sandbox instantiation-a new sandbox can be started for each function invocation in under 30μsec-which mitigates the cold start problems that can be especially detrimental to DAGs. Rather than relying on expensive coordination via shared storage, the enhanced Sledge framework provides a fast memory communication channel to propagate data through the DAG. We consider the DAGs with service level objectives, defined by their execution deadlines. To ensure the DAGs meet their performance requirements, we consider, analyze, and compare two deadline-Aware pluggable schedulers (that we implemented in Sledge) on a variety of realistic workloads.  © 2022 ACM.","DAGs; edge computing; serverless; SLOs; WebAssembly","Digital storage; Edge computing; Scheduling; Complicated functions; Edge computing; Near-real-time processing; Novel applications; Real time response; Scalings; Serverless; Simple++; SLO; Webassembly; Directed graphs","Association for Computing Machinery, Inc","ACM SIGOPS","5th International Workshop on Edge Systems, Analytics and Networking, EdgeSys 2022, in conjunction with ACM EuroSys 2022","5 April 2022 through 8 April 2022","Virtual, Online","178625","Conference paper","Final","","Scopus","2-s2.0-85128345078"
"Naranjo Delgado D.M.; Contreras M.; Moltó G.; Risco S.; Blanquer I.; Prades J.; Silla F.","Naranjo Delgado, Diana M. (57210020692); Contreras, Manuel (58242676000); Moltó, Germán (8953108200); Risco, Sebastián (57211030526); Blanquer, Ignacio (21734509800); Prades, Javier (55513273400); Silla, Federico (6701737007)","57210020692; 58242676000; 8953108200; 57211030526; 21734509800; 55513273400; 6701737007","On the Acceleration of FaaS Using Remote GPU Virtualization","2023","ICPE 2023 - Companion of the 2023 ACM/SPEC International Conference on Performance Engineering","","","","157","164","7","1","10.1145/3578245.3584933","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158848937&doi=10.1145%2f3578245.3584933&partnerID=40&md5=dbac4a9c68990f21250d251af45ac8b5","Serverless computing and, in particular, Function as a Service (FaaS) has introduced novel computational approaches with its highly-elastic capabilities, per-millisecond billing and scale-to-zero capacities, thus being of interest for the computing continuum. Services such as AWS Lambda allow efficient execution of event-driven short-lived bursty applications, even if there are limitations in terms of the amount of memory and the lack of GPU support for accelerated execution. To this aim, this paper analyses the suitability of including GPU support in AWS Lambda through the rCUDA middleware, which provides CUDA applications with remote GPU execution capabilities. A reference architecture for data-driven accelerated processing is introduced, based on elastic queues and event-driven object storage systems to manage resource contention and GPU scheduling. The benefits and limitations are assessed through a use case of sequence alignment. The results indicate that, for certain scenarios, the usage of remote GPUs in AWS Lambda represents a viable approach to reduce the execution time. © 2023 ACM.","AWS lambda; CUDA; FAAS; GPU; serverless","Data handling; Digital storage; Information management; Middleware; Program processors; Virtualization; AWS lambda; Computational approach; CUDA; Event-driven; FAAS; Lambda's; Paper analysis; Reference architecture; Serverless; Virtualizations; Graphics processing unit","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC Research","14th Annual ACM/SPEC International Conference on Performance Engineering, ICPE 2023","15 April 2023 through 19 April 2023","Coimbra","187962","Conference paper","Final","","Scopus","2-s2.0-85158848937"
"Zhao Z.; Wu M.; Tang J.; Zang B.; Wang Z.; Chen H.","Zhao, Ziming (57201619659); Wu, Mingyu (57193513326); Tang, Jiawei (58096807700); Zang, Binyu (6701320221); Wang, Zhaoguo (37040023500); Chen, Haibo (55743141500)","57201619659; 57193513326; 58096807700; 6701320221; 37040023500; 55743141500","BeeHive: Sub-second Elasticity for Web Services with Semi-FaaS Execution","2023","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","2","","","74","87","13","9","10.1145/3575693.3575752","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147736223&doi=10.1145%2f3575693.3575752&partnerID=40&md5=077acbd5d011945faa55d46139e43070","Function-as-a-service (FaaS), an emerging cloud computing paradigm, is expected to provide strong elasticity due to its promise to auto-scale fine-grained functions rapidly. Although appealing for applications with good parallelism and dynamic workload, this paper shows that it is non-trivial to adapt existing monolithic applications (like web services) to FaaS due to their complexity. To bridge the gap between complicated web services and FaaS, this paper proposes a runtime-based Semi-FaaS execution model, which dynamically extracts time-consuming code snippets (closures) from applications and offloads them to FaaS platforms for execution. It further proposes BeeHive, an offloading framework for Semi-FaaS, which relies on the managed runtime to provide a fallback-based execution model and addresses the performance issues in traditional offloading mechanisms for FaaS. Meanwhile, the runtime system of BeeHive selects offloading candidates in a user-transparent way and supports efficient object sharing, memory management, and failure recovery in a distributed environment. The evaluation using various web applications suggests that the Semi-FaaS execution supported by BeeHive can reach sub-second resource provisioning on commercialized FaaS platforms like AWS Lambda, which is up to two orders of magnitude better than other alternative scaling approaches in cloud computing.  © 2023 ACM.","Cloud Computing; Function-as-a-Service; Java Virtual Machine","Computer architecture; Elasticity; Java programming language; Virtual machine; Web services; Websites; Auto-scale; Cloud-computing; Computing paradigm; Execution modeling; Function-as-a-service; Java virtual machines; Runtimes; Service execution; Service platforms; Webs services; Cloud computing","Association for Computing Machinery","ACM; ACM SIGARCH; ACM SIGOPS; ACM SIGPLAN","28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2023","25 March 2023 through 29 March 2023","Vancouver","186350","Conference paper","Final","","Scopus","2-s2.0-85147736223"
"Liu Q.; Du D.; Xia Y.; Zhang P.; Chen H.","Liu, Qingyuan (57219510222); Du, Dong (57200438686); Xia, Yubin (7403027696); Zhang, Ping (58743503200); Chen, Haibo (55743141500)","57219510222; 57200438686; 7403027696; 58743503200; 55743141500","The Gap Between Serverless Research and Real-world Systems","2023","SoCC 2023 - Proceedings of the 2023 ACM Symposium on Cloud Computing","","","","475","485","10","10","10.1145/3620678.3624785","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178507784&doi=10.1145%2f3620678.3624785&partnerID=40&md5=d217ce4c5625c6d6a93eddc139a1861c","With the emergence of the serverless computing paradigm in the cloud, researchers have explored many challenges of serverless systems and proposed solutions such as snapshot-based booting. However, we have noticed that some of these optimizations are based on oversimplified assumptions that lead to infeasibility and hide real-world issues. This paper aims to analyze the gap between current serverless research and real-world systems from a perspective of industry, and present new observations, challenges, opportunities, and insights that may address the discrepancies. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.","cloud computing; scheduling; serverless; sidecar","'current; Cloud-computing; Computing paradigm; Optimisations; Real-world; Real-world system; Serverless; Serverless systems; Sidecar; Computer programming","Association for Computing Machinery, Inc","ACM SIGMOD; ACM SIGOPS; ActiveLoop; Google; IBM; Microsoft","14th ACM Symposium on Cloud Computing, SoCC 2023","30 October 2023 through 1 November 2023","Santa Cruz","194526","Conference paper","Final","","Scopus","2-s2.0-85178507784"
"Basu Roy R.; Patel T.; Liew R.; Babuji Y.N.; Chard R.; Tiwari D.","Basu Roy, Rohan (57219249946); Patel, Tirthak (57200205359); Liew, Richmond (58560110000); Babuji, Yadu Nand (57193607019); Chard, Ryan (55588066400); Tiwari, Devesh (23467777300)","57219249946; 57200205359; 58560110000; 57193607019; 55588066400; 23467777300","ProPack: Executing Concurrent Serverless Functions Faster and Cheaper","2023","HPDC 2023 - Proceedings of the 32nd International Symposium on High-Performance Parallel and Distributed Computing","","","","211","224","13","10","10.1145/3588195.3592988","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169582523&doi=10.1145%2f3588195.3592988&partnerID=40&md5=aeccc5453b204e9a616ee0b62f098a32","The serverless computing model has been on the rise in recent years due to a lower barrier to entry and elastic scalability. However, our experimental evidence suggests that multiple serverless computing platforms suffer from serious performance inefficiencies when a high number of concurrent function instances are invoked, which is a desirable capability for parallel applications. To mitigate this challenge, this paper introduces ProPack, a novel solution that provides higher performance and yields cost savings for end users running applications with high concurrency. ProPack leverages insights obtained from experimental study to build a simple and effective analytical model that mitigates the scalability bottleneck. Our evaluation on multiple serverless platforms including AWS Lambda and Google confirms that ProPack can improve average performance by 85% and save cost by 66%. ProPack provides significant improvement (over 50%) over the state-of-the-art serverless workload manager such as Pywren, and is also, effective at mitigating the concurrency bottleneck for FuncX, a recent on-premise serverless execution platform for parallel applications.  © 2023 ACM.","cloud computing; scalability; serverless computing","Cloud-computing; Computing model; Computing platform; Cost saving; Experimental evidence; Higher yield; Novel solutions; Parallel application; Performance; Serverless computing; Scalability","Association for Computing Machinery, Inc","ACM SIGARCH; ACM SIGHPC","32nd International Symposium on High-Performance Parallel and Distributed Computing, HPDC 2023","16 June 2023 through 23 June 2023","Orlando","191522","Conference paper","Final","","Scopus","2-s2.0-85169582523"
"Lei Z.; Shi X.; Lv C.; Yu X.; Zhao X.","Lei, Zhengyu (58025038600); Shi, Xiao (57205328361); Lv, Cunchi (58024239300); Yu, Xiaobing (58743332000); Zhao, Xiaofang (56022060800)","58025038600; 57205328361; 58024239300; 58743332000; 56022060800","Chitu: Accelerating Serverless Workflows with Asynchronous State Replication Pipelines","2023","SoCC 2023 - Proceedings of the 2023 ACM Symposium on Cloud Computing","","","","597","610","13","2","10.1145/3620678.3624794","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178521039&doi=10.1145%2f3620678.3624794&partnerID=40&md5=5e345def84d6b1ee6bb03923a0d5a783","Serverless workflows are characterized as multi-stage computing, while downstream functions require accessing intermediate states or the output of upstream functions for running. The workflow’s performance can be easily affected due to the inefficiency of data access. Studies accelerate data access with various policies, such as direct and indirect methods. However, these methods may fail due to various limitations such as resource availability. In this paper, we propose asynchronous state replication pipelines (ASRP) to speed up workflows for general applications, replacing the sequential computing pattern of current workflows. Cℎitu is built based on the insight with three main points. First, differentiable data types (DDT) are provided at the programming model level to support incremental state sharing and computation. Second, ASRP works by continuously delivering changes of DDT objects in real-time so that downstream functions can consume the objects without waiting for the ending of upstream functions. Third, we make a systematic design to support DDT and ASRP in Cℎitu framework, including direct communication and change propagation. We implement Cℎitu atop OpenFaaS, compare it with popular serverless workflow frameworks, and evaluate it with three commonly seen cases. The results show that Cℎitu accelerates data transmission in general serverless workflows up to 1.7×, and speeds up end-to-end applications by up to 57%. © 2023 Association for Computing Machinery.","Asynchronous state replication; Differentiable data type; Pipeline; Serverless workflow","Asynchronoi state replication; Data access; Datatypes; Differentiable data type; Down-stream; Multi-stages; Serverless workflow; Speed up; State replications; Work-flows; Computer programming","Association for Computing Machinery, Inc","ACM SIGMOD; ACM SIGOPS; ActiveLoop; Google; IBM; Microsoft","14th ACM Symposium on Cloud Computing, SoCC 2023","30 October 2023 through 1 November 2023","Santa Cruz","194526","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85178521039"
"Gain G.; Soldani C.; Huici F.; Mathy L.","Gain, Gaulthier (57223222184); Soldani, Cyril (23978746600); Huici, Felipe (26423713000); Mathy, Laurent (6701789212)","57223222184; 23978746600; 26423713000; 6701789212","Want More Unikernels? Inflate Them!","2022","SoCC 2022 - Proceedings of the 13th Symposium on Cloud Computing","","","","510","525","15","5","10.1145/3542929.3563473","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143252530&doi=10.1145%2f3542929.3563473&partnerID=40&md5=c4db2cca4920b2b3082121f2c1b1216c","Unikernels are on the rise in the cloud. These lightweight virtual machines (VMs) specialized to a single application offer the same level of isolation as full-blown VMs, while providing performance superior to standard Linux-based VMs or even to containers. However, their inherent specialization renders memory deduplication ineffective, causing unikernels, in practice, to consume more memory than their small memory footprint would suggest. This makes them less advantageous when thousands of SaaS and/or FaaS unikernels instances have to run on the same server. In this paper we introduce a novel approach to build the next generation of networked services and lambda functions by improving unikernel's memory layout so that it is more likely to share identical pages with other unikernels deployed on the system. Our approach supports SaaS and FaaS architectures and can be used with ASLR. Our experiments show that our approach can reduce the amount of physical memory used by a set of unikernels running on the same server by as much as 3x, with next to no overhead on applications performance.  © 2022 ACM.","alignment; deduplication; hypervisor; memory; operating systems; unikernels; virtual machine; virtualization","Computer operating systems; Network security; Software as a service (SaaS); Deduplication; Hypervisors; Lightweight virtual machines; Memory deduplication; Operating system; Performance; Small memory footprint; Specialisation; Unikernel; Virtualizations; Virtual machine","Association for Computing Machinery, Inc","ACM Special Interest Grou on Operating Systems (SIGOPS); ACM Special Interest Group on Management of Data (SIGMOD); EBay; et al.; FutureWei; Microsoft","13th Annual ACM Symposium on Cloud Computing, SoCC 2022","7 November 2022 through 11 November 2022","San Francisco","184095","Conference paper","Final","","Scopus","2-s2.0-85143252530"
"","","","WoSC 2022 - Proceedings of the 8th International Workshop on Serverless Computing, Part of Middleware 2022","2022","WoSC 2022 - Proceedings of the 8th International Workshop on Serverless Computing, Part of Middleware 2022","","","","","","39","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145591123&partnerID=40&md5=8ea975580f5abbc5a43bd921dad8589e","The proceedings contain 5 papers. The topics discussed include: all-you-can-inference: serverless DNN model inference suite; impact of microarchitectural state reuse on serverless functions; sentinel: a fast and memory-efficient serverless architecture for lightweight applications; migrating from microservices to serverless: an IoT platform case study; and transferring transactional business processes to FaaS.","","","Association for Computing Machinery, Inc","ACM; ETS; Raytheon BBN; rti","8th International Workshop on Serverless Computing, WoSC 2022 - Part of Middleware 2022","7 November 2022","Quebec","184997","Conference review","Final","","Scopus","2-s2.0-85145591123"
"Hattori J.; Kato S.","Hattori, Joe (58041984900); Kato, Shinpei (14631997400)","58041984900; 14631997400","Sentinel: A Fast and Memory-Efficient Serverless Architecture for Lightweight Applications","2022","WoSC 2022 - Proceedings of the 8th International Workshop on Serverless Computing, Part of Middleware 2022","","","","13","18","5","0","10.1145/3565382.3565880","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145606504&doi=10.1145%2f3565382.3565880&partnerID=40&md5=f7a036db6bcca54a6cc8399144ae1ef6","Serverless computing is a new computing paradigm that enables application developers to focus on the core service logic of their applications. To protect the host kernel from attacks, serverless service providers need to ensure isolation between application sandboxes while keeping the startup latency and memory usage low. Existing architectures provide the warm start functionality to alleviate the startup latency. However, as warm starts are achieved at the cost of high memory usage, handling all requests as warm starts is virtually impossible. Therefore, mitigating the cold start overhead is key to improving startup latency. In the real world, serverless applications tend to be uncomplicated; they only issue simple system calls and do not modify the underlying filesystem. While existing architectures support full-fledged virtualization, serverless applications support many functionalities and data structures that are unnecessary for simple applications. This paper proposes Sentinel, a serverless architecture to target those simple applications. By stripping the unnecessary functionalities and providing the bare minimum OS virtualization needed to execute the targeted applications, Sentinel achieves drastic improvement compared to existing architectures; up to 10× shorter startup overhead, 8.13× shorter end-to-end execution latency, and 98% lower memory usage.  © 2022 ACM.","function-as-a-service; serverless computing; virtualization","Computation theory; Memory architecture; Virtual reality; Existing architectures; Function-as-a-service; Lightweight application; Memory efficient; Memory usage; Serverless architecture; Serverless computing; Simple++; Virtualizations; Warm start; Virtualization","Association for Computing Machinery, Inc","ACM; ETS; Raytheon BBN; rti","8th International Workshop on Serverless Computing, WoSC 2022 - Part of Middleware 2022","7 November 2022","Quebec","184997","Conference paper","Final","","Scopus","2-s2.0-85145606504"
"Narasimhamurthy S.; Lockwood G.","Narasimhamurthy, Sai (15064453700); Lockwood, Glenn (56310090300)","15064453700; 56310090300","HiPS2022: The 2nd Workshop on High Performance Serverless Computing","2022","HPDC 2022 - Proceedings of the 31st International Symposium on High-Performance Parallel and Distributed Computing","","","","292","293","1","0","10.1145/3502181.3535105","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134167297&doi=10.1145%2f3502181.3535105&partnerID=40&md5=cff5b76046b2e1e43e8e57dbb5c1860a","Exciting changes are coming to the fore in the world of Storage and I/O for High Performance and Data Intensive computing. Existing data access and data retrieval methods that suitably support the new generation of data intensive applications in the realm of HPC and AI are being re-assessed. These new applications need to support both evolutionary as well as revolutionary approaches to data access and storage. Open source is enabling the adoption of these new techniques. This workshop looks at cutting edge trends in storage systems and solutions for data intensive HPC and AI applications with specific focus primarily on community driven initiatives and commercial products that may inspire the community. © 2022 Owner/Author.","cloud computing; data intensive computing; exascale i/o; hpc storage","Cloud computing; Cloud-computing; Data access; Data retrieval; Data-intensive application; Data-intensive computing; Exascale; Exascale i/o; Hpc storage; Performance; Retrieval methods; Digital storage","Association for Computing Machinery, Inc","ACM SIGARCH","31st International Symposium on High-Performance Parallel and Distributed Computing, HPDC 2022","27 June 2022 through 30 June 2022","Virtual, Online","180402","Conference paper","Final","","Scopus","2-s2.0-85134167297"
"Przybylski B.; Pawlik M.; Zuk P.; Lagosz B.; Malawski M.; Rzadca K.","Przybylski, Bartlomiej (57190976634); Pawlik, MacIej (55055652500); Zuk, Pawel (57219792001); Lagosz, Bartlomiej (57973863500); Malawski, MacIej (22433325400); Rzadca, Krzysztof (8952117500)","57190976634; 55055652500; 57219792001; 57973863500; 22433325400; 8952117500","Using Unused: Non-Invasive Dynamic FaaS Infrastructure with HPC-Whisk","2022","International Conference for High Performance Computing, Networking, Storage and Analysis, SC","2022-November","","","","","","6","10.1109/SC41404.2022.00045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149281006&doi=10.1109%2fSC41404.2022.00045&partnerID=40&md5=5688fee3090df9fa10eaeddc1fcccc31","Modern HPC workload managers and their careful tuning contribute to the high utilization of HPC clusters. However, due to inevitable uncertainty it is impossible to completely avoid node idleness. Although such idle slots are usually too short for any HPC job, they are too long to ignore them. Function-as-a-Service (FaaS) paradigm promisingly fills this gap, and can be a good match, as typical FaaS functions last seconds, not hours. Here we show how to build a FaaS infrastructure on idle nodes in an HPC cluster in such a way that it does not affect the performance of the HPC jobs significantly. We dynamically adapt to a changing set of idle physical machines, by integrating open-source software Slurm and OpenWhisk. We designed and implemented a prototype solution that allowed us to cover up to 90% of the idle time slots on a 50k-core cluster that runs production workloads. © 2022 IEEE.","FaaS; function as a service; high-performance computing; HPC; serverless; supercomputer","Open source software; Open systems; Dynamic functions; Function as a service; Function-as-a-service; High-performance computing; HPC; HPC clusters; Performance computing; Serverless; Services infrastructures; Workload managers; Supercomputers","IEEE Computer Society","ACM's Special Interest Group on High Performance Computing (SIGHPC); Association for Computing Machinery; IEEE Computer Society; IEEE's Technical Committee on High Performance Computing (TCHPC)","2022 International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2022","13 November 2022 through 18 November 2022","Dallas","186921","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85149281006"
"Arif M.; Assogba K.; Rafique M.M.","Arif, Moiz (57190034679); Assogba, Kevin (57201820908); Rafique, M. Mustafa (25655215700)","57190034679; 57201820908; 25655215700","Canary: Fault-Tolerant FaaS for Stateful Time-Sensitive Applications","2022","International Conference for High Performance Computing, Networking, Storage and Analysis, SC","2022-November","","","","","","6","10.1109/SC41404.2022.00046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149339277&doi=10.1109%2fSC41404.2022.00046&partnerID=40&md5=8f3395455c3e288ac4607f42a4e4b566","Function-as-a-Service (FaaS) platforms have recently gained rapid popularity. Many stateful applications have been migrated to FaaS platforms due to their ease of deployment, scalability, and minimal management overhead. However, failures in FaaS have not been thoroughly investigated, thus making these desirable platforms unreliable for guaranteeing function execution and ensuring performance requirements. In this paper, we propose Canary, a highly resilient and fault-tolerant framework for FaaS that mitigates the impact of failures and reduces the overhead of function restart. Canary utilizes replicated container runtimes and application-level checkpoints to reduce application recovery time over FaaS platforms. Our evaluations using representative stateful FaaS applications show that Canary reduces the application recovery time and dollar cost by up to 83% and 12%, respectively over the default retry-based strategy. Moreover, it improves application availability with an additional average execution time and cost overhead of 14% and 8%, respectively, as compared to the ideal failure-free execution. © 2022 IEEE.","Data Parallelism; Data-intensive Computing; Deep Learning; OpenWhisk; Serverless Computing; TensorFlow","Deep learning; Fault tolerance; Application recovery; Data parallelism; Data-intensive computing; Deep learning; Fault-tolerant; Openwhisk; Recovery time; Serverless computing; Service platforms; Tensorflow; Cost reduction","IEEE Computer Society","ACM's Special Interest Group on High Performance Computing (SIGHPC); Association for Computing Machinery; IEEE Computer Society; IEEE's Technical Committee on High Performance Computing (TCHPC)","2022 International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2022","13 November 2022 through 18 November 2022","Dallas","186921","Conference paper","Final","","Scopus","2-s2.0-85149339277"
"Li Y.; Zhao L.; Yang Y.; Qu W.","Li, Yiming (57212459721); Zhao, Laiping (35243865000); Yang, Yanan (57208671879); Qu, Wenyu (8917789900)","57212459721; 35243865000; 57208671879; 8917789900","Rethinking Deployment for Serverless Functions: A Performance-First Perspective","2023","Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2023","","","67","","","","0","10.1145/3581784.3613211","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179551792&doi=10.1145%2f3581784.3613211&partnerID=40&md5=8a892d843de1f3af20bf70b059723a25","Serverless computing commonly adopts strong isolation mechanisms for deploying functions, which may bring significant performance overhead because each function needs to run in a completely new environment (i.e., the ""one-to-one""model). To accelerate the function computation, prior work has proposed using sandbox sharing to reduce the overhead, i.e., the ""many-to-one""model. Nonetheless, either process-based true parallelism or thread-based pseudo-parallelism still causes high latency, preventing its adaptation for latency-sensitive web services.To achieve optimal performance and resource efficiency for serverless workflow, we argue an ""m-to-n""deployment model that manipulates multiple granularities of computing abstractions such as processes, threads, and sandboxes to amortize overhead. We propose wrap, a new deployment abstraction that balances the tradeoffs between interaction overhead, startup overhead and function execution. We further design Chiron, a wrap-based deployment manager that can automatically perform the orchestration of multiple computing abstractions based on performance prioritization. Our comprehensive evaluation indicates that Chiron outperforms state-of-the-art systems by 1.3×-21.8× on system throughput.  © 2023 ACM.","deployment model; graph partition; serverless workflows","Abstracting; Deployment models; Function computations; Graph partition; Many-to-one; Optimal performance; Performance; Process-based; Serverless workflow; Webs services; Work-flows; Web services","Association for Computing Machinery, Inc","ACM's Special Interest Group on High Performance Computing (SIGHPC); Association for Computing Machinery; IEEE�s Technical Committee on High Performance Computing (TCHPC); Institute of Electrical and Electronics Engineers (IEEE) Computer Society","2023 International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2023","12 November 2023 through 17 November 2023","Denver","194664","Conference paper","Final","","Scopus","2-s2.0-85179551792"
"Pei Q.; Yuan Y.; Hu H.; Chen Q.; Liu F.","Pei, Qiangyu (57488996600); Yuan, Yongjie (57488978700); Hu, Haichuan (57814699700); Chen, Qiong (57203010887); Liu, Fangming (55717312800)","57488996600; 57488978700; 57814699700; 57203010887; 55717312800","AsyFunc: A High-Performance and Resource-Efficient Serverless Inference System via Asymmetric Functions","2023","SoCC 2023 - Proceedings of the 2023 ACM Symposium on Cloud Computing","","","","324","340","16","21","10.1145/3620678.3624664","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178510257&doi=10.1145%2f3620678.3624664&partnerID=40&md5=85d34e767c25bc37be3a91fd6f60d8d9","Recent advances in deep learning (DL) have spawned various intelligent cloud services with well-trained DL models. Nevertheless, it is nontrivial to maintain the desired end-to-end latency under bursty workloads, raising critical challenges on high-performance while resource-efficient inference services. To handle burstiness, some inference services have migrated to the serverless paradigm for its rapid elasticity. However, they neglect the impact of the time-consuming and resource-hungry model-loading process when scaling out function instances, leading to considerable resource inefficiency for maintaining high performance under burstiness. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.","","Burstiness; Bursty workloads; Cloud services; Critical challenges; End to end latencies; Inference systems; Learning models; Loading process; Performance; Resource-efficient; Deep learning","Association for Computing Machinery, Inc","ACM SIGMOD; ACM SIGOPS; ActiveLoop; Google; IBM; Microsoft","14th ACM Symposium on Cloud Computing, SoCC 2023","30 October 2023 through 1 November 2023","Santa Cruz","194526","Conference paper","Final","","Scopus","2-s2.0-85178510257"
"Bhasi V.M.; Gunasekaran J.R.; Sharma A.; Kandemir M.T.; Das C.","Bhasi, Vivek M. (57343857600); Gunasekaran, Jashwant Raj (55440506600); Sharma, Aakash (57225853993); Kandemir, Mahmut Taylan (35549787100); Das, Chita (7201851990)","57343857600; 55440506600; 57225853993; 35549787100; 7201851990","Cypress: Input size Sensitive Container Provisioning and Request Scheduling for Serverless Platforms","2022","SoCC 2022 - Proceedings of the 13th Symposium on Cloud Computing","","","","257","272","15","22","10.1145/3542929.3563464","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143251604&doi=10.1145%2f3542929.3563464&partnerID=40&md5=1ea63345a05c008781a0c175818a621d","The growing popularity of the serverless platform has seen an increase in the number and variety of applications (apps) being deployed on it. The majority of these apps process user-provided input to produce the desired results. Existing work in the area of input-sensitive profiling has empirically shown that many such apps have input size-dependent execution times which can be determined through modelling techniques. Nevertheless, existing serverless resource management frameworks are agnostic to the input size-sensitive nature of these apps. We demonstrate in this paper that this can potentially lead to container over-provisioning and/or end-to-end Service Level Objective (SLO) violations. To address this, we propose Cypress, an input size-sensitive resource management framework, that minimizes the containers provisioned for apps, while ensuring a high degree of SLO compliance. We perform an extensive evaluation of Cypress on top of a Kubernetes-managed cluster using 5 apps from the AWS Serverless Application Repository and/or Open-FaaS Function Store with real-world traces and varied input size distributions. Our experimental results show that Cypress spawns up to 66% fewer containers, thereby, improving container utilization and saving cluster-wide energy by up to 2.95X and 23%, respectively, versus state-of-the-art frameworks, while remaining highly SLO-compliant (up to 99.99%).  © 2022 ACM.","input size; resource-management; scheduling; serverless","Containers; Natural resources management; Resource allocation; Application process; Input size; Modelling techniques; Over provisioning; Request scheduling; Resource management; Resource management framework; Serverless; Service level objective; Size dependent; Scheduling","Association for Computing Machinery, Inc","ACM Special Interest Grou on Operating Systems (SIGOPS); ACM Special Interest Group on Management of Data (SIGMOD); EBay; et al.; FutureWei; Microsoft","13th Annual ACM Symposium on Cloud Computing, SoCC 2022","7 November 2022 through 11 November 2022","San Francisco","184095","Conference paper","Final","","Scopus","2-s2.0-85143251604"
"Simonetti D.; Tovar B.; Thain D.","Simonetti, David (57795275200); Tovar, Ben (59049368900); Thain, Douglas (8900976600)","57795275200; 59049368900; 8900976600","Mixed Modality Workflows in TaskVine","2023","HPDC 2023 - Proceedings of the 32nd International Symposium on High-Performance Parallel and Distributed Computing","","","","331","332","1","0","10.1145/3588195.3595953","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169591512&doi=10.1145%2f3588195.3595953&partnerID=40&md5=49b80067ed163d53c8d35a6113932e65","Modern scientific workflows desire to mix several different computing modalities: self-contained computational tasks, data-intensive transformations, and serverless function calls. To date, these modalities have required distinct system architectures with different scheduling objectives and constraints. In this paper, we describe how TaskVine, a new workflow execution platform, combines these modalities into an execution platform with shared abstractions. We demonstrate results of the system executing a machine learning workflow with combined standalone tasks and serverless functions.  © 2023 Owner/Author.","serverless; workflows","Computational task; Data intensive; Execution platforms; Function calls; Machine-learning; Scientific workflows; Serverless; Systems architecture; Work-flows; Workflow execution","Association for Computing Machinery, Inc","ACM SIGARCH; ACM SIGHPC","32nd International Symposium on High-Performance Parallel and Distributed Computing, HPDC 2023","16 June 2023 through 23 June 2023","Orlando","191522","Conference paper","Final","","Scopus","2-s2.0-85169591512"
"Abdi M.; Ginzburg S.; Lin C.; Faleiro J.M.; Goiri Í.; Chaudhry G.; Bianchini R.; Berger D.S.; Fonseca R.","Abdi, Mania (57205024778); Ginzburg, Samuel (57221613382); Lin, Charles (58288340000); Faleiro, José M. (23974170800); Goiri, Íñigo (24801831000); Chaudhry, Gohar (57219259486); Bianchini, Ricardo (7006939544); Berger, Daniel S. (56270528200); Fonseca, Rodrigo (59847856900)","57205024778; 57221613382; 58288340000; 23974170800; 24801831000; 57219259486; 7006939544; 56270528200; 59847856900","Palette Load Balancing: Locality Hints for Serverless Functions","2023","Proceedings of the 18th European Conference on Computer Systems, EuroSys 2023","","","","365","380","15","29","10.1145/3552326.3567496","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160205636&doi=10.1145%2f3552326.3567496&partnerID=40&md5=99f620f2edf044d9fedfb14bb5dfe644","Function-as-a-Service (FaaS) serverless computing enables a simple programming model with almost unbounded elasticity. Unfortunately, current FaaS platforms achieve this flexibility at the cost of lower performance for data-intensive applications compared to a serverful deployment. The ability to have computation close to data is a key missing feature. We introduce Palette load balancing, which offers FaaS applications a simple mechanism to express locality to the platform, through hints we term “colors”. Palette maintains the serverless nature of the service – users are still not allocating resources – while allowing the platform to place successive invocations related to each other on the same executing node. We compare a prototype of the Palette load balancer to a state-of-the-art locality-oblivious load balancer on representative examples of three applications. For a serverless web application with a local cache, Palette improves the hit ratio by 6x. For a serverless version of Dask, Palette improves run times by 46% and 40% on Task Bench and TPC-H, respectively. On a serverless version of NumS, Palette improves run times by 37%. These improvements largely bridge the gap to serverful implementation of the same systems. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Caching; Cloud Computing; Data-parallel processing; Serverless Computing","Balancing; Platform as a Service (PaaS); Caching; Cloud-computing; Data parallel; Data-parallel processing; Load balancer; Load-Balancing; Parallel processing; Runtimes; Serverless computing; Simple++; Data handling","Association for Computing Machinery, Inc","Ant Group Research; et al.; Google; Huawei; KAUST; Meta","18th European Conference on Computer Systems, EuroSys 2023","8 May 2023 through 12 May 2023","Rome","188495","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85160205636"
"Wang M.; Schirmer T.; Pfandzelter T.; Bermbach D.","Wang, Minghe (58159216200); Schirmer, Trever (57555715400); Pfandzelter, Tobias (57208737730); Bermbach, David (51461094200)","58159216200; 57555715400; 57208737730; 51461094200","Lotus: Serverless In-Transit Data Processing for Edge-based Pub/Sub","2023","EdgeSys 2023 - Proceedings of the 6th International Workshop on Edge Systems, Analytics and Networking, Part of EuroSys 2023","","","","31","35","4","2","10.1145/3578354.3592869","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159350758&doi=10.1145%2f3578354.3592869&partnerID=40&md5=14135eb41b4786bf5281410e89eaff04","Publish-subscribe systems are a popular approach for edge-based IoT use cases: Heterogeneous, constrained edge devices can be integrated easily, with message routing logic offloaded to edge message brokers. Message processing, however, is still done on constrained edge devices. Complex content-based filtering, the transformation between data representations, or message extraction place a considerable load on these systems, and resulting superfluous message transfers strain the network.In this paper, we propose Lotus, adding in-Transit data processing to an edge publish-subscribe middleware in order to offload basic message processing from edge devices to brokers. Specifically, we leverage the Function-As-A-Service paradigm, which offers support for efficient multi-Tenancy, scale-To-zero, and real-Time processing. With a proof-of-concept prototype of Lotus, we validate its feasibility and demonstrate how it can be used to offload sensor data transformation to the publish-subscribe messaging middleware.  © 2023 ACM.","edge communication; function-As-A-service (FaaS); publish/subscribe","Message passing; Metadata; Publishing; Content based filtering; Edge communication; Edge-based; Function-as-A-service; Message brokers; Message processing; Message routing; Pub/sub; Publish/subscribe; Publish/Subscribe system; Middleware","Association for Computing Machinery, Inc","ACM SIGOPS","6th International Workshop on Edge Systems, Analytics and Networking, EdgeSys 2023, in conjunction with ACM EuroSys 2023","8 May 2023","Rome","188212","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85159350758"
"Kousiouris G.; Giannakos C.; Tserpes K.; Stamati T.","Kousiouris, George (26031026200); Giannakos, Chris (57847000100); Tserpes, Konstantinos (6506348972); Stamati, Teta (6506964629)","26031026200; 57847000100; 6506348972; 6506964629","Measuring Baseline Overheads in Different Orchestration Mechanisms for Large FaaS Workflows","2022","ICPE 2022 - Companion of the 2022 ACM/SPEC International Conference on Performance Engineering","","","","61","68","7","2","10.1145/3491204.3527467","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136058597&doi=10.1145%2f3491204.3527467&partnerID=40&md5=43d467da215748619c45aa3552112342","Serverless environments have attracted significant attention in recent years as a result of their agility in execution as well as inherent scaling capabilities as a cloud-native execution model. While extensive analysis has been performed in various critical performance aspects of these environments, such as cold start times, the aspect of workflow orchestration delays has been neglected. Given that this paradigm has become more mature in recent years and application complexity has started to rise from a few functions to more complex application structures, the issue of delays in orchestrating these functions may become severe. In this work, one of the main open source FaaS platforms, Openwhisk, is utilized in order to measure and investigate its orchestration delays for the main sequence operator of the platform. These are compared to delays included in orchestration of functions through two alternative means, including the execution of orchestrator logic functions in supporting runtimes based on Node-RED. The delays inserted by each different orchestration mode are measured and modeled, while boundary points of selection between each mode are presented, based on the number and expected delay of the functions that constitute the workflow. It is indicative that in certain cases, the orchestration overheads might range from 0.29% to 235% compared to the beneficial computational time needed for the workflow functions. The results can extend simulation and estimation mechanisms with information on the orchestration overheads.  © 2022 ACM.","faas; openwhisk; orchestration; overhead; performance; serverless","Execution modeling; Faas; Openwhisk; Orchestration; Overhead; Performance; Performance aspects; Scaling capability; Serverless; Work-flows; Computer programming","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC","13th Annual ACM/SPEC International Conference on Performance Engineering, ICPE 2022","9 April 2022 through 13 April 2022","Virtual, Online","181030","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85136058597"
"Fuerst A.; Novakovic S.; Goiri I.; Chaudhry G.I.; Sharma P.; Arya K.; Broas K.; Bak E.; Iyigun M.; Bianchini R.","Fuerst, Alexander (57218225304); Novakovic, Stanko (56103917700); Goiri, Inigo (24801831000); Chaudhry, Gohar Irfan (57219259486); Sharma, Prateek (58273575200); Arya, Kapil (35177461700); Broas, Kevin (57489035900); Bak, Eugene (57488966600); Iyigun, Mehmet (57489051500); Bianchini, Ricardo (7006939544)","57218225304; 56103917700; 24801831000; 57219259486; 58273575200; 35177461700; 57489035900; 57488966600; 57489051500; 7006939544","Memory-harvesting vms in cloud platforms","2022","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","","","","583","594","11","38","10.1145/3503222.3507725","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126394637&doi=10.1145%2f3503222.3507725&partnerID=40&md5=fcb35f5a504b587096e26046ef2da86a","loud platforms monetize their spare capacity by renting ""Spot""virtual machines (VMs) that can be evicted in favor of higher-priority VMs. Recent work has shown that resource-harvesting VMs are more effective at exploiting spare capacity than Spot VMs, while also reducing the number of evictions. However, the prior work focused on harvesting CPU cores while keeping memory size fixed. This wastes a substantial monetization opportunity and may even limit the ability of harvesting VMs to leverage spare cores. Thus, in this paper, we explore memory harvesting and its challenges in real cloud platforms, namely its impact on VM creation time, NUMA spanning, and page fragmentation. We start by characterizing the amount and dynamics of the spare memory in Azure. We then design and implement memory-harvesting VMs (MHVMs), introducing new techniques for memory buffering, batching, and pre-reclamation. To demonstrate the use of MHVMs, we also extend a popular cluster scheduling framework (Hadoop) and a FaaS platform to adapt to them. Our main results show that (1) there is plenty of scope for memory harvesting in real platforms; (2) MHVMs are effective at mitigating the negative impacts of harvesting; and (3) our extensions of Hadoop and FaaS successfully hide the MHVMs' varying memory size from the users' data-processing jobs and functions. We conclude that memory harvesting has great potential for practical deployment and users can save up to 93% of their costs when running workloads on MHVMs.  © 2022 ACM.","Cloud computing; memory management; resource harvesting","Cloud computing; Data handling; Cloud platforms; Cloud-computing; Cluster scheduling; CPU cores; Design and implements; Memory size; Memory-management; Page fragmentation; Resource harvesting; Spare capacity; Harvesting","Association for Computing Machinery","ACM; ACM SIGARCH; ACM SIGOPS; ACM SIGPLAN","27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2022","28 February 2022 through 4 March 2022","Virtual, Online","177381","Conference paper","Final","","Scopus","2-s2.0-85126394637"
"Roy R.B.; Patel T.; Tiwari D.","Roy, Rohan Basu (57219249946); Patel, Tirthak (57200205359); Tiwari, Devesh (23467777300)","57219249946; 57200205359; 23467777300","DayDream: Executing Dynamic Scientific Workflows on Serverless Platforms with Hot Starts","2022","International Conference for High Performance Computing, Networking, Storage and Analysis, SC","2022-November","","","","","","22","10.1109/SC41404.2022.00027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149267350&doi=10.1109%2fSC41404.2022.00027&partnerID=40&md5=96f62aa83fafcb0650cc805a287e024b","HPC applications are increasingly being designed as dynamic workflows for the ease of development and scaling. This work demonstrates how the serverless computing model can be leveraged for efficient execution of complex, real-world scientific workflows, although serverless computing was not originally designed for executing scientific workflows. This work characterizes, quantifies, and improves the execution of three real-world, complex, dynamic scientific workflows: ExaFEL (workflow for investigating the molecular structures via X-Ray diffraction), Cosmoscout-Vr(workflow for large scale virtual reality simulation), and Core Cosmology Library (a cosmology workflow for investigating dark matter). The proposed technique, DayDream, employs the hot start mechanism for warming up the components of the workflows by decoupling the runtime environment from the component function code to mitigate cold start overhead. DayDream optimizes the service time and service cost jointly to reduce the service time by 45% and service cost by 23% over the state-of-the-art HPC workload manager. © 2022 IEEE.","Cloud Computing; HPC Workflows; Serverless Computing","Cosmology; Virtual reality; Cloud-computing; Dynamic workflow; HPC workflow; Real-world; Scalings; Scientific workflows; Serverless computing; Service costs; Service time; Work-flows; Digital libraries","IEEE Computer Society","ACM's Special Interest Group on High Performance Computing (SIGHPC); Association for Computing Machinery; IEEE Computer Society; IEEE's Technical Committee on High Performance Computing (TCHPC)","2022 International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2022","13 November 2022 through 18 November 2022","Dallas","186921","Conference paper","Final","","Scopus","2-s2.0-85149267350"
"","","","SoCC 2022 - Proceedings of the 13th Symposium on Cloud Computing","2022","SoCC 2022 - Proceedings of the 13th Symposium on Cloud Computing","","","","","","565","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143266415&partnerID=40&md5=3fd2e7f9ed7c5d1f4f7217fde2cf1b6d","The proceedings contain 38 papers. The topics discussed include: Arax: a runtime framework for decoupling applications from heterogeneous accelerators; DeepScaling: microservices AutoScaling for stable CPU utilization in large scale cloud systems; Demeter: QoS-aware CPU scheduling to reduce power consumption of multiple black-box workloads; network resource management as a database problem; cloud-native workflow scheduling using a hybrid priority rule and dynamic task parallelism; Owl: performance-aware scheduling for resource-efficient function-as-a-service cloud; characterizing and orchestrating VM reservation in geo-distributed clouds to improve the resource efficiency; how to fight production incidents? an empirical study on a large-scale cloud service; and workload consolidation in Alibaba clusters: the good, the bad, and the ugly.","","","Association for Computing Machinery, Inc","ACM Special Interest Grou on Operating Systems (SIGOPS); ACM Special Interest Group on Management of Data (SIGMOD); EBay; et al.; FutureWei; Microsoft","13th Annual ACM Symposium on Cloud Computing, SoCC 2022","7 November 2022 through 11 November 2022","San Francisco","184095","Conference review","Final","","Scopus","2-s2.0-85143266415"
"Li S.; Wang W.; Yang J.; Chen G.; Lu D.","Li, Suyi (57221639733); Wang, Wei (57234263600); Yang, Jun (58743076700); Chen, Guangzhen (58743712700); Lu, Daohe (57200422702)","57221639733; 57234263600; 58743076700; 58743712700; 57200422702","Golgi: Performance-Aware, Resource-Efficient Function Scheduling for Serverless Computing","2023","SoCC 2023 - Proceedings of the 2023 ACM Symposium on Cloud Computing","","","","32","47","15","16","10.1145/3620678.3624645","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178521027&doi=10.1145%2f3620678.3624645&partnerID=40&md5=4c9fc9a89953ba0b9ffe2d11a31c8ac5","This paper introduces Golgi, a novel scheduling system designed for serverless functions, with the goal of minimizing resource provisioning costs while meeting the function latency requirements. To achieve this, Golgi judiciously overcommits functions based on their past resource usage. To ensure overcommitment does not cause significant performance degradation, Golgi identifies nine low-level metrics to capture the runtime performance of functions, encompassing factors like request load, resource allocation, and contention on shared resources. These metrics enable accurate prediction of function performance using the Mondrian Forest, a classification model that is continuously updated in real-time for optimal accuracy without extensive offline training. Golgi employs a conservative exploration-exploitation strategy for request routing. By default, it routes requests to non-overcommitted instances to ensure satisfactory performance. However, it actively explores opportunities for using more resource-efficient overcommitted instances, while maintaining the specified latency SLOs. Golgi also performs vertical scaling to dynamically adjust the concurrency of overcommitted instances, maximizing request throughput and enhancing system robustness to prediction errors. We have prototyped Golgi and evaluated it in both EC2 cluster and a small production cluster. The results show that Golgi can meet the SLOs while reducing the resource provisioning cost by 42% (30%) in EC2 cluster (our production cluster). © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Resource Management; Scheduling; Serverless Computing","Performance; Performance degradation; Resource management; Resource usage; Resource-efficient; Resources allocation; Runtime performance; Scheduling; Scheduling systems; Serverless computing; Resource allocation","Association for Computing Machinery, Inc","ACM SIGMOD; ACM SIGOPS; ActiveLoop; Google; IBM; Microsoft","14th ACM Symposium on Cloud Computing, SoCC 2023","30 October 2023 through 1 November 2023","Santa Cruz","194526","Conference paper","Final","","Scopus","2-s2.0-85178521027"
"","","","Middleware 2021 - Proceedings of the 22nd International Middleware Conference","2021","Middleware 2021 - Proceedings of the 22nd International Middleware Conference","","","","","","109","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117564497&partnerID=40&md5=7d78ffeb8c134460f7f7e36f96879b09","The proceedings contain 8 papers. The topics discussed include: precursor: a fast, client-centric and trusted key-value store using RDMA and Intel SGX; Prox: efficient privacy for recommendation-as-a-service; FW-KV: improving read guarantees in psi; towards optimal placement and scheduling of DNN operations with Pesto; prosecutor: an efficient BFT consensus algorithm with behavior-aware penalization against byzantine attacks; SeBS: a serverless benchmark suite for function-as-a-service computing; magic-pipe: self-optimizing video analytics pipelines; and experience paper: sgx-dl: dynamic loading and hot-patching for secure applications.","","","Association for Computing Machinery, Inc","ACM","22nd International Middleware Conference, Middleware 2021","6 December 2021 through 10 December 2021","Virtual, Online","172202","Conference review","Final","","Scopus","2-s2.0-85117564497"
"","","","Proceedings of SC 2021: The International Conference for High Performance Computing, Networking, Storage and Analysis: Science and Beyond","2021","International Conference for High Performance Computing, Networking, Storage and Analysis, SC","","","","","","1493","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120002454&partnerID=40&md5=9b59ba943fbf53aa19b59ff22000195e","The proceedings contain 105 papers. The topics discussed include: tensor processing primitives: a programming abstraction for efficiency and portability in deep learning workloads; enable simultaneous DNN services based on deterministic operator overlap and precise latency prediction; preparing an incompressible-flow fluid dynamics code for exascale-class wind energy simulations; a next-generation discontinuous Galerkin fluid dynamics solver with application to high-resolution lung airflow simulations; understanding, predicting and scheduling serverless workloads under partial interference; the hidden cost of the edge: a performance comparison of edge and cloud latencies; ribbon: cost-effective and QoS-aware deep learning model inference using a diverse pool of cloud computing instances; and chimera: efficiently training large-scale neural networks with bidirectional pipelines.","","","IEEE Computer Society","ACM�s Special Interest Group SIGHPC; Association for Computing Machinery (ACM); IEEE Computer Society; IEEE�s Technical Committee on High Performance Computing (TCHPC)","33rd International Conference for High Performance Computing, Networking, Storage and Analysis: Science and Beyond, SC 2021","14 November 2021 through 19 November 2021","Virtual, Online","174017","Conference review","Final","","Scopus","2-s2.0-85120002454"
"Copik M.; Kwasniewski G.; Besta M.; Podstawski M.; Hoefler T.","Copik, Marcin (57194605130); Kwasniewski, Grzegorz (56278375300); Besta, MacIej (56145464900); Podstawski, Michal (57195152522); Hoefler, Torsten (14018121700)","57194605130; 56278375300; 56145464900; 57195152522; 14018121700","SeBS: A serverless benchmark suite for function-as-a-service computing","2021","Middleware 2021 - Proceedings of the 22nd International Middleware Conference","","","","64","78","14","105","10.1145/3464298.3476133","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117609008&doi=10.1145%2f3464298.3476133&partnerID=40&md5=2a8e7418f27a9e41e36fda10c0216425","Function-as-a-Service (FaaS) is one of the most promising directions for the future of cloud services, and serverless functions have immediately become a new middleware for building scalable and cost-efficient microservices and appli cations. However, the quickly moving technology hinders reproducibility, and the lack of a standardized benchmarking suite leads to ad-hoc solutions and microbenchmarks being used in serverless research, further complicating meta-analysis and comparison of research solutions. To address this challenge, we propose the Serverless Benchmark Suite: the first benchmark for FaaS computing that systematically covers a wide spectrum of cloud resources and applications. Our benchmark consists of the specification of representative workloads, the accompanying implementation and evaluation infrastructure, and the evaluation methodology that facilitates reproducibility and enables interpretability. We demonstrate that the abstract model of a FaaS execution environment ensures the applicability of our benchmark to multiple commercial providers such as AWS, Azure, and Google Cloud. Our work facilities experimental evaluation of serverless systems, and delivers a standardized, reliable and evolving evaluation methodology of performance, efficiency, scalability and reliability of middleware FaaS platforms.  © 2021 ACM.","benchmark; FaaS; function-as-a-service; serverless","Benchmarking; Function evaluation; Benchmark; Benchmark suites; Cloud services; Cost-efficient; Evaluation methodologies; Function-as-a-service; Reproducibilities; Serverless; Service computing; Middleware","Association for Computing Machinery, Inc","ACM","22nd International Middleware Conference, Middleware 2021","6 December 2021 through 10 December 2021","Virtual, Online","172202","Conference paper","Final","","Scopus","2-s2.0-85117609008"
"Gadepalli P.K.; McBride S.; Peach G.; Cherkasova L.; Parmer G.","Gadepalli, Phani Kishore (57201876148); McBride, Sean (57221200888); Peach, Gregor (57205081861); Cherkasova, Ludmila (7005553676); Parmer, Gabriel (8431909100)","57201876148; 57221200888; 57205081861; 7005553676; 8431909100","Sledge: A Serverless-first, Light-weight Wasm Runtime for the Edge","2020","Middleware 2020 - Proceedings of the 2020 21st International Middleware Conference","","","","265","279","14","71","10.1145/3423211.3425680","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098498610&doi=10.1145%2f3423211.3425680&partnerID=40&md5=dafc6f9c18a3d7b554ab3c88788f85ed","Emerging IoT applications with real-time latency constraints require new data processing systems operating at the Edge. Serverless computing offers a new compelling paradigm, where a user can execute a small application without handling the operational issues of server provisioning and resource management. Despite a variety of existing commercial and open source serverless platforms (utilizing VMs and containers), these solutions are too heavy-weight for a resource-constrained Edge systems (due to large memory footprint and high invocation time). Moreover, serverless workloads that focus on per-client, short-running computations are not an ideal fit for existing general purpose computing systems. In this paper, we present the design and implementation of Sledge - a novel and efficient WebAssembly-based serverless framework for the Edge. Sledge is optimized for supporting unique properties of serverless workloads: the need for high density multi-tenancy, low startup time, bursty client request rates, and short-lived computations. Sledge is designed for these constraints by offering (i) optimized scheduling policies and efficient work-distribution for short-lived computations, and (ii) a light-weight function isolation model implemented using our own WebAssembly-based software fault isolation infrastructure. These lightweight sandboxes are designed to support high-density computation: with fast startup and teardown times to handle high client request rates. An extensive evaluation of Sledge with varying workloads and real-world serverless applications demonstrates the effectiveness of the designed serverless-first runtime for the Edge. Sledge supports up to 4 times higher throughput and 4 times lower latencies compared to Nuclio, one of the fastest open-source serverless frameworks. © 2020 Association for Computing Machinery.","Edge computing; IoT; Serverless; WebAssembly","Data handling; Middleware; Open source software; Open systems; Data processing systems; Design and implementations; General-purpose computing; High density computation; Operational issues; Optimized scheduling; Resource management; Software fault isolations; Real time systems","Association for Computing Machinery, Inc","Association for Computing Machinery (ACM); IFIP; TU Delft; USENIX","21st International Middleware Conference, Middleware 2020","7 December 2020 through 11 December 2020","Virtual, Online","165666","Conference paper","Final","","Scopus","2-s2.0-85098498610"
"Tariq A.; Pahl A.; Nimmagadda S.; Rozner E.; Lanka S.","Tariq, Ali (57219773187); Pahl, Austin (57219776765); Nimmagadda, Sharat (57219774343); Rozner, Eric (24281956100); Lanka, Siddharth (57219777508)","57219773187; 57219776765; 57219774343; 24281956100; 57219777508","Sequoia: Enabling quality-of-service in serverless computing","2020","SoCC 2020 - Proceedings of the 2020 ACM Symposium on Cloud Computing","","","","311","327","16","87","10.1145/3419111.3421306","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095439090&doi=10.1145%2f3419111.3421306&partnerID=40&md5=04b3111d11f1056b8d800b2feb8089c4","Serverless computing is a rapidly growing paradigm that easily harnesses the power of the cloud. With serverless computing, developers simply provide an event-driven function to cloud providers, and the provider seamlessly scales function invocations to meet demands as event-triggers occur. As current and future serverless offerings support a wide variety of serverless applications, effective techniques to manage serverless workloads becomes an important issue. This work examines current management and scheduling practices in cloud providers, uncovering many issues including inflated application run times, function drops, inefficient allocations, and other undocumented and unexpected behavior. To fix these issues, a new quality-of-service function scheduling and allocation framework, called Sequoia, is designed. Sequoia allows developers or administrators to easily def ne how serverless functions and applications should be deployed, capped, prioritized, or altered based on easily configured, flexible policies. Results with controlled and realistic workloads show Sequoia seamlessly adapts to policies, eliminates mid-chain drops, reduces queuing times by up to 6.4X, enforces tight chain-level fairness, and improves run-time performance up to 25X.  © 2020 ACM.","measurement; quality-of-service; serverless computing","Cloud computing; Drops; Scheduling; Cloud providers; Event trigger; Event-driven; Run-time performance; Quality of service","Association for Computing Machinery, Inc","ACM SIGMOD; ACM SIGOPS; Alibaba Group; aws; et al.; Microsoft","11th ACM Symposium on Cloud Computing, SoCC 2020","19 October 2020 through 21 October 2020","Virtual, Online","163871","Conference paper","Final","","Scopus","2-s2.0-85095439090"
"Carver B.; Zhang J.; Wang A.; Anwar A.; Wu P.; Cheng Y.","Carver, Benjamin (57215278740); Zhang, Jingyuan (57215280960); Wang, Ao (57215285001); Anwar, Ali (57197590735); Wu, Panruo (55277613300); Cheng, Yue (56022559100)","57215278740; 57215280960; 57215285001; 57197590735; 55277613300; 56022559100","Wukong: A scalable and locality-enhanced framework for serverless parallel computing","2020","SoCC 2020 - Proceedings of the 2020 ACM Symposium on Cloud Computing","","","","1","15","14","95","10.1145/3419111.3421286","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095448759&doi=10.1145%2f3419111.3421286&partnerID=40&md5=2fc6b280089ad7bb8470b5e1fba5980d","Executing complex, burst-parallel, directed acyclic graph (DAG) jobs poses a major challenge for serverless execution frameworks, which will need to rapidly scale and schedule tasks at high throughput, while minimizing data movement across tasks. We demonstrate that, for serverless parallel computations, decentralized scheduling enables scheduling to be distributed across Lambda executors that can schedule tasks in parallel, and brings multiple benefits, including enhanced data locality, reduced network I/Os, automatic resource elasticity, and improved cost effectiveness. We describe the implementation and deployment of our new serverless parallel framework, called Wukong, on AWS Lambda. We show that Wukong achieves near-ideal scalability, executes parallel computation jobs up to 68.17X faster, reduces network I/O by multiple orders of magnitude, and achieves 92.96% tenant-side cost savings compared to numpywren.  © 2020 ACM.","","Cloud computing; Cost effectiveness; Directed graphs; Scheduling; Data movements; Decentralized scheduling; Directed acyclic graph (DAG); Execution framework; High throughput; Multiple orders; Parallel Computation; Parallel framework; Cost reduction","Association for Computing Machinery, Inc","ACM SIGMOD; ACM SIGOPS; Alibaba Group; aws; et al.; Microsoft","11th ACM Symposium on Cloud Computing, SoCC 2020","19 October 2020 through 21 October 2020","Virtual, Online","163871","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85095448759"
"Baarzi A.F.; Kesidis G.; Joe-Wong C.; Shahrad M.","Baarzi, Ataollah Fatahi (57218214304); Kesidis, George (7003540724); Joe-Wong, Carlee (47962256600); Shahrad, Mohammad (56943394900)","57218214304; 7003540724; 47962256600; 56943394900","On merits and viability of multi-cloud serverless","2021","SoCC 2021 - Proceedings of the 2021 ACM Symposium on Cloud Computing","","","","600","608","8","25","10.1145/3472883.3487002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119250537&doi=10.1145%2f3472883.3487002&partnerID=40&md5=429acd6f9ce91039dd3a4e858182bf06","Serverless computing is a rapidly growing paradigm in the cloud industry that envisions functions as the computational building blocks of an application. Instead of forcing the application developer to provision cloud resources for their application, the cloud provider provisions the required resources for each function ""under the hood.""In this work, we envision virtual serverless providers (VSPs) to aggregate serverless offerings. In doing so, VSPs allow developers (and businesses) to get rid of vendor lock-in problems and exploit pricing and performance variation across providers by adaptively utilizing the best provider at each time, forcing the providers to compete to offer cheaper and superior services. We discuss the merits of a VSP and show that serverless systems are well-suited to cross-provider aggregation, compared to virtual machines. We propose a VSP system architecture and implement an initial version. Using experimental evaluations, our preliminary results show that a VSP can improve maximum sustained throughput by 1.2x to 4.2x, reduces SLO violations by 98.8%, and improves the total invocations' costs by 54%.  © 2021 Association for Computing Machinery.","Cloud computing; Multi-cloud; Serverless","Application developers; Building blockes; Cloud providers; Cloud-computing; Forcings; Lock-in; Multi-clouds; Performance variations; Serverless; Serverless systems; Cloud computing","Association for Computing Machinery, Inc","ACM Special Interest Groups on Management of Data (ACM SIGMOD); ACM Special Interest Groups on Operating Systems (ACM SIGOPS); et al.; Exotanium; Microsoft; Salesforce","12th Annual ACM Symposium on Cloud Computing, SoCC 2021","1 November 2021 through 4 November 2021","Virtual, Online","173200","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85119250537"
"Cordingly R.; Heydari N.; Yu H.; Hoang V.; Sadeghi Z.; Lloyd W.","Cordingly, Robert (57220806485); Heydari, Navid (57223141628); Yu, Hanfei (57220804031); Hoang, Varik (57220804100); Sadeghi, Zohreh (57220805982); Lloyd, Wes (8537012400)","57220806485; 57223141628; 57220804031; 57220804100; 57220805982; 8537012400","Enhancing observability of serverless computing with the serverless application analytics framework","2021","ICPE 2021 - Companion of the ACM/SPEC International Conference on Performance Engineering","","","","161","164","3","10","10.1145/3447545.3451173","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104969464&doi=10.1145%2f3447545.3451173&partnerID=40&md5=e3b56b9331b79f7e3e75dd8fcfced694","To improve the observability of workload performance, resource utilization, and infrastructure underlying serverless Function-as-a-Service (FaaS) platforms, we have developed the Serverless Application Analytics Framework (SAAF). SAAF provides a reusable framework supporting multiple programming languages that developers can leverage to inspect performance, resource utilization, scalability, and infrastructure metrics of function deployments to commercial and open-source FaaS platforms. To automate reproducible FaaS performance experiments, we provide the FaaS Runner as a multithreaded FaaS client. FaaS Runner provides a programmable client that can orchestrate over one thousand concurrent FaaS function calls. The ReportGenerator is then used to aggregate experiment output into CSV files for consumption by popular data analytics tools. SAAF and its supporting tools combined can assess forty-eight distinct metrics to enhance observability of serverless software deployments. In this tutorial paper, we describe SAAF and its supporting tools and provide examples of observability insights that can be derived. © 2021 Association for Computing Machinery.","","Computer software reusability; Data Analytics; Infrastructure as a service (IaaS); Open source software; Platform as a Service (PaaS); Analytics tools; Function calls; Multithreaded; Open sources; Performance experiment; Resource utilizations; Software deployment; Supporting tool; Observability","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC Research","2021 ACM/SPEC International Conference on Performance Engineering, ICPE 2021","19 April 2021 through 21 April 2021","Virtual, Online","168447","Conference paper","Final","","Scopus","2-s2.0-85104969464"
"Suresh A.; Gandhi A.","Suresh, Amoghvarsha (56161438700); Gandhi, Anshul (56744293000)","56161438700; 56744293000","FNSched: An efficient scheduler for serverless functions","2019","WOSC 2019 - Proceedings of the 2019 5th International Workshop on Serverless Computing, Part of Middleware 2019","","","","19","24","5","67","10.1145/3366623.3368136","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078922442&doi=10.1145%2f3366623.3368136&partnerID=40&md5=68847dba229c4bab2960be8b3fe1f625","An imminent challenge in the serverless computing landscape is the escalating cost of infrastructure needed to handle the growing traffic at scale. This work presents FnSched, a function-level scheduler designed to minimize provider resource costs while meeting customer performance requirements. FnSched works by carefully regulating the resource usage of colocated functions on each invoker, and autoscaling capacity by concentrating load on few invokers in response to varying traffic. We implement a prototype of FnSched and show that, compared to existing baselines, FnSched significantly improves resource efficiency, by as much as 36%–55%, while providing acceptable application latency. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","","Middleware; Autoscaling; Co-located; Escalating costs; Function levels; Performance requirements; Resource costs; Resource efficiencies; Resource usage; Scheduling","Association for Computing Machinery, Inc","ACM","5th International Workshop on Serverless Computing, WOSC 2019 - Part of Middleware 2019","9 December 2019 through 13 December 2019","Davis","156757","Conference paper","Final","","Scopus","2-s2.0-85078922442"
"Jia Z.; Witchel E.","Jia, Zhipeng (57209220014); Witchel, Emmett (22836994600)","57209220014; 22836994600","Boki: Stateful Serverless Computing with Shared Logs","2021","SOSP 2021 - Proceedings of the 28th ACM Symposium on Operating Systems Principles","","","","691","707","16","75","10.1145/3477132.3483541","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119095022&doi=10.1145%2f3477132.3483541&partnerID=40&md5=0a523cc72994678f5b7a27032ba434e6","Boki is a new serverless runtime that exports a shared log API to serverless functions. Boki shared logs enable stateful serverless applications to manage their state with durability, consistency, and fault tolerance. Boki shared logs achieve high throughput and low latency. The key enabler is the metalog, a novel mechanism that allows Boki to address ordering, consistency and fault tolerance independently. The metalog orders shared log records with high throughput and it provides read consistency while allowing service providers to optimize the write and read path of the shared log in different ways. To demonstrate the value of shared logs for stateful serverless applications, we build Boki support libraries that implement fault-tolerant workflows, durable object storage, and message queues. Our evaluation shows that shared logs can speed up important serverless workloads by up to 4.7x. © 2021 Owner/Author.","consistency; function-as-a-service; Serverless computing; shared log","Consistency; Function-as-a-service; High-low; High-throughput; Low latency; Runtimes; Serverless computing; Service provider; Shared log; Support libraries; Fault tolerance","Association for Computing Machinery, Inc","ACM SIGOPS in cooperation with USENIX","28th ACM Symposium on Operating Systems Principles, SOSP 2021","26 October 2021 through 29 October 2021","Virtual, Online","173064","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85119095022"
"Goltzsche D.; Nieke M.; Knauth T.; Kapitza R.","Goltzsche, David (57194219156); Nieke, Manuel (57184241600); Knauth, Thomas (34868426600); Kapitza, Rüdiger (13408600800)","57194219156; 57184241600; 34868426600; 13408600800","Acctee: A WebAssembly-based Two-way Sandbox for Trusted Resource Accounting","2019","Middleware 2019 - Proceedings of the 2019 20th International Middleware Conference","","","","123","135","12","41","10.1145/3361525.3361541","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078057952&doi=10.1145%2f3361525.3361541&partnerID=40&md5=aefcfb2e5a6e2b1f1c0494e7aacf8378","Remote computation has numerous use cases such as cloud computing, client-side web applications or volunteer computing. Typically, these computations are executed inside a sandboxed environment for two reasons: first, to isolate the execution in order to protect the host environment from unauthorised access, and second to control and restrict resource usage. Often, there is mutual distrust between entities providing the code and the ones executing it, owing to concerns over three potential problems: (i) loss of control over code and data by the providing entity, (ii) uncertainty of the integrity of the execution environment for customers, and (iii) a missing mutually trusted accounting of resource usage. In this paper we present AccTEE, a two-way sandbox that offers remote computation with resource accounting trusted by consumers and providers. AccTEE leverages two recent technologies: hardware-protected trusted execution environments, and WebAssembly, a novel platform independent byte-code format. We show how AccTEE uses automated code instrumentation for fine-grained resource accounting while maintaining confidentiality and integrity of code and data. Our evaluation of AccTEE in three scenarios – volunteer computing, serverless computing, and pay-by-computation for the web – shows a maximum accounting overhead of 10%. © 2019 Association for Computing Machinery.","Intel SGX; Resource Accounting; Sandbox; Trusted Computing; WebAssembly","Middleware; Trusted computing; Execution environments; Intel SGX; Platform independent; Remote computations; Resource accountings; Sandbox; Trusted execution environments; WebAssembly; Codes (symbols)","Association for Computing Machinery, Inc","et al.; Hewlett-Packard Enterprise; NSF; Raytheon BBN Technologies; rti; SIEMENS","20th ACM/IFIP/USENIX Middleware Conference, Middleware 2019","9 December 2019 through 13 December 2019","Davis","156194","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85078057952"
"Silva P.; Fireman D.; Pereira T.E.","Silva, Paulo (57225842239); Fireman, Daniel (15135510600); Pereira, Thiago Emmanuel (56048383500)","57225842239; 15135510600; 56048383500","Prebaking functions to warm the serverless cold start","2020","Middleware 2020 - Proceedings of the 2020 21st International Middleware Conference","","","","1","13","12","90","10.1145/3423211.3425682","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098482026&doi=10.1145%2f3423211.3425682&partnerID=40&md5=a504f228433628dd70e0d3a74365fdba","Function-as-service (FaaS) platforms promise a simpler programming model for cloud computing, in which the developers concentrate on writing its applications. In contrast, platform providers take care of resource management and administration. As FaaS users are billed based on the execution of the functions, platform providers have a natural incentive not to keep idle resources running at the platform's expense. However, this strategy may lead to the cold start issue, in which the execution of a function is delayed because there is no ready resource to host the execution. Cold starts can take hundreds of milliseconds to seconds and have been a prohibitive and painful disadvantage for some applications. This work describes and evaluates a technique to start functions, which restores snapshots from previously executed function processes. We developed a prototype of this technique based on the CRIU process checkpoint/restore Linux tool. We evaluate this prototype by running experiments that compare its start-up time against the standard Unix process creation/start-up procedure. We analyze the following three functions: i) a ""do-nothing"" function, ii) an Image Resizer function, and iii) a function that renders Markdown files. The results attained indicate that the technique can improve the start-up time of function replicas by 40% (in the worst case of a ""do-nothing"" function) and up to 71% for the Image Resizer one. Further analysis indicates that the runtime initialization is a key factor, and we confirmed it by performing a sensitivity analysis based on synthetically generated functions of different code sizes. These experiments demonstrate that it is critical to decide when to create a snapshot of a function. When one creates the snapshots of warm functions, the speed-up achieved by the prebaking technique is even higher: the speed-up increases from 127.45% to 403.96%, for a small, synthetic function; and for a bigger, synthetic function, this ratio increases from 121.07% to 1932.49%. © 2020 Association for Computing Machinery.","Cloud; Faas; Performance evaluation; Serverless","Computer operating systems; Image enhancement; Sensitivity analysis; Cold start; Generated function; ITS applications; Key factors; Process creation; Programming models; Resource management; Startup time; Middleware","Association for Computing Machinery, Inc","Association for Computing Machinery (ACM); IFIP; TU Delft; USENIX","21st International Middleware Conference, Middleware 2020","7 December 2020 through 11 December 2020","Virtual, Online","165666","Conference paper","Final","","Scopus","2-s2.0-85098482026"
"Hunhoff E.; Irshad S.; Thurimella V.; Tariq A.; Rozner E.","Hunhoff, Erika (57221159427); Irshad, Shazal (57215839238); Thurimella, Vijay (57221157850); Tariq, Ali (57219773187); Rozner, Eric (24281956100)","57221159427; 57215839238; 57221157850; 57219773187; 24281956100","Proactive Serverless Function Resource Management","2020","WOSC 2020 - Proceedings of the 2020 6th International Workshop on Serverless Computing, Part of Middleware 2020","","","","61","66","5","14","10.1145/3429880.3430102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099584821&doi=10.1145%2f3429880.3430102&partnerID=40&md5=e10db9600fb066f60e1c86182e293942","This paper introduces a new primitive to serverless language runtimes called freshen. With freshen, developers or providers specify functionality to perform before a given function executes. This proactive technique allows for overheads associated with serverless functions to be mitigated at execution time, which improves function responsiveness. We show various predictive opportunities exist to run freshen within reasonable time windows. A high-level design and implementation are described, along with preliminary results to show the potential benefits of our scheme.  © 2020 ACM.","Resource Management; Serverless Computing","Natural resources management; Design and implementations; High-level design; Language runtimes; Potential benefits; Resource management; Serverless computing; Time windows; Resource allocation","Association for Computing Machinery, Inc","ACM","6th International Workshop on Serverless Computing, WOSC 2020 - Part of Middleware 2020","7 December 2020 through 11 December 2020","Virtual, Online","166193","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85099584821"
"Yadav P.; Salwala D.; Sudharsan B.; Curry E.","Yadav, Piyush (57216831316); Salwala, Dhaval (57215659410); Sudharsan, Bharath (57215272471); Curry, Edward (12790805000)","57216831316; 57215659410; 57215272471; 12790805000","GNOSIS-query-driven multimodal event processing for unstructured data streams","2021","Middleware 2021 Demos and Posters - Proceedings of the 2021 International Middleware Conference Demos and Posters","","","","16","17","1","0","10.1145/3491086.3492475","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121423280&doi=10.1145%2f3491086.3492475&partnerID=40&md5=c8c63be2af6069c82bafffeefcf221e4","This paper presents GNOSIS, an event processing engine to detect complex event patterns over multimodal data streams. GNOSIS follows a query-driven approach where users can write complex event queries using Multimodal Event Processing Language (MEPL). The system models incoming multimodal data into an evolving Multimodal Event Knowledge Graph (MEKG) using an ensemble of deep neural network (DNN) and machine learning (ML) models and applies a neuro-symbolic approach for event matching. GNOSIS follows a serverless paradigm where its different components act as independent microservices and can be deployed across different nodes with optimized edge support. The paper demonstrates two multimodal use case queries from Occupational Health and Safety and Accessibility domain.  © 2021 Owner/Author.","event processing; event queries; multimodal","Complex networks; Data handling; Industrial hygiene; Knowledge graph; Complex events; Data stream; Event pattern; Event Processing; Event query; Event-processing engine; Multi-modal; Multimodal data streams; Query-driven approach; Unstructured data; Deep neural networks","Association for Computing Machinery, Inc","ACM; ETS; University Laval","22nd International Middleware Conference, Middleware 2021","6 December 2021 through 10 December 2021","Virtual, Online","175055","Conference paper","Final","","Scopus","2-s2.0-85121423280"
"Ginzburg S.; Freedman M.J.","Ginzburg, Samuel (57221613382); Freedman, Michael J. (8655839500)","57221613382; 8655839500","Serverless Isn't Server-Less: Measuring and Exploiting Resource Variability on Cloud FaaS Platforms","2020","WOSC 2020 - Proceedings of the 2020 6th International Workshop on Serverless Computing, Part of Middleware 2020","","","","43","48","5","21","10.1145/3429880.3430099","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099607225&doi=10.1145%2f3429880.3430099&partnerID=40&md5=9f92f24b55a5d8c444b53772261f0413","Serverless computing in the cloud, or functions as a service (FaaS), poses new and unique systems design challenges. Serverless offers improved programmability for customers, yet at the cost of increased design complexity for cloud providers. One such challenge is effective and consistent resource management for serverless platforms, the implications of which we explore in this paper. In this paper, we conduct one of the first detailed in situ measurement studies of performance variability in AWS Lambda. We show that the observed variations in performance are not only significant, but stable enough to exploit. We then design and evaluate an end-to-end system that takes advantage of this resource variability to exploit the FaaS consumption-based pricing model, in which functions are charged based on their fine-grain execution time rather than actual low-level resource consumption. By using both light-weight resource probing and function execution times to identify attractive servers in serverless platforms, customers of FaaS services can cause their functions to execute on better performing servers and realize a cost savings of up to 13% in the same AWS region.  © 2020 ACM.","","Cloud providers; Design challenges; Design complexity; In-situ measurement; Lambda's; Measurement study; Performance; Performance variability; Programmability; Resource management; Costs","Association for Computing Machinery, Inc","ACM","6th International Workshop on Serverless Computing, WOSC 2020 - Part of Middleware 2020","7 December 2020 through 11 December 2020","Virtual, Online","166193","Conference paper","Final","","Scopus","2-s2.0-85099607225"
"Ustiugov D.; Petrov P.; Kogias M.; Bugnion E.; Grot B.","Ustiugov, Dmitrii (57211122881); Petrov, Plamen (57219259797); Kogias, Marios (57200567467); Bugnion, Edouard (6602775973); Grot, Boris (25926414600)","57211122881; 57219259797; 57200567467; 6602775973; 25926414600","Benchmarking, analysis, and optimization of serverless function snapshots","2021","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","","","","559","572","13","143","10.1145/3445814.3446714","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104742254&doi=10.1145%2f3445814.3446714&partnerID=40&md5=800ccb44f1a173c0eaa41a3d8025deee","Serverless computing has seen rapid adoption due to its high scalability and flexible, pay-as-you-go billing model. In serverless, developers structure their services as a collection of functions, sporadically invoked by various events like clicks. High inter-arrival time variability of function invocations motivates the providers to start new function instances upon each invocation, leading to significant cold-start delays that degrade user experience. To reduce cold-start latency, the industry has turned to snapshotting, whereby an image of a fully-booted function is stored on disk, enabling a faster invocation compared to booting a function from scratch. This work introduces vHive, an open-source framework for serverless experimentation with the goal of enabling researchers to study and innovate across the entire serverless stack. Using vHive, we characterize a state-of-the-art snapshot-based serverless infrastructure, based on industry-leading Containerd orchestration framework and Firecracker hypervisor technologies. We find that the execution time of a function started from a snapshot is 95% higher, on average, than when the same function is memory-resident. We show that the high latency is attributable to frequent page faults as the function's state is brought from disk into guest memory one page at a time. Our analysis further reveals that functions access the same stable working set of pages across different invocations of the same function. By leveraging this insight, we build REAP, a light-weight software mechanism for serverless hosts that records functions' stable working set of guest memory pages and proactively prefetches it from disk into memory. Compared to baseline snapshotting, REAP slashes the cold-start delays by 3.7x, on average. © 2021 ACM.","cloud computing; datacenters; serverless; snapshots; virtualization","User experience; High scalabilities; Inter-arrival time; Memory-resident; New functions; Open source frameworks; Pay as you go; Software mechanisms; State of the art; Open source software","Association for Computing Machinery","ACM SIGARCH; ACM SIGOPS; ACM SIGPLAN","26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2021","19 April 2021 through 23 April 2021","Virtual, Online","168445","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85104742254"
"Spillner J.","Spillner, Josef (14042825100)","14042825100","Resource Management for Cloud Functions with Memory Tracing, Profiling and Autotuning","2020","WOSC 2020 - Proceedings of the 2020 6th International Workshop on Serverless Computing, Part of Middleware 2020","","","","13","18","5","10","10.1145/3429880.3430094","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099592865&doi=10.1145%2f3429880.3430094&partnerID=40&md5=67a2e9d1b2e934f020fae7c97da37f70","Application software provisioning evolved from monolithic designs towards differently designed abstractions including serverless applications. The promise of that abstraction is that developers are free from infrastructural concerns such as instance activation and autoscaling. Today's serverless architectures based on FaaS are however still exposing developers to explicit low-level decisions about the amount of memory to allocate for the respective cloud functions. In many cases, guesswork and ad-hoc decisions determine the values a developer will put into the configuration. We contribute tools to measure the memory consumption of a function in various Docker, OpenFaaS and GCF/GCR configurations over time and to create trace profiles that advanced FaaS engines can use to autotune memory dynamically. Moreover, we explain how pricing forecasts can be performed by connecting these traces with a FaaS characteristics knowledge base.  © 2020 ACM.","models; serverless computing; vertical scaling","Abstracting; Economics; Knowledge based systems; Architecture-based; Autoscaling; Autotuning; Memory consumption; Memory tracing; Monolithic design; Resource management; Serverless architecture; Serverless computing; Vertical scaling; Application programs","Association for Computing Machinery, Inc","ACM","6th International Workshop on Serverless Computing, WOSC 2020 - Part of Middleware 2020","7 December 2020 through 11 December 2020","Virtual, Online","166193","Conference paper","Final","","Scopus","2-s2.0-85099592865"
"Nikolos O.L.; Papazafeiropoulos K.; Psomadakis S.; Nanos A.; Koziris N.","Nikolos, Orestis Lagkas (57214123698); Papazafeiropoulos, Konstantinos (57195336784); Psomadakis, Stratos (55830537900); Nanos, Anastassios (24766961900); Koziris, Nectarios (6701648998)","57214123698; 57195336784; 55830537900; 24766961900; 6701648998","Extending storage support for unikernel containers","2019","WOSC 2019 - Proceedings of the 2019 5th International Workshop on Serverless Computing, Part of Middleware 2019","","","","31","36","5","3","10.1145/3366623.3368138","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078887217&doi=10.1145%2f3366623.3368138&partnerID=40&md5=06feb9009d9d8b5e34ba53709ff236c1","In recent years, the rapid adoption of the serverless computing paradigm has led to the proliferation of Function-as-a-Service computing frameworks. The majority of these frameworks utilize containers, a lightweight operating system virtualization technique, to ensure isolated function execution. Unikernels, which package applications within a single-address space library operating system, have been proposed as an alternative function isolation mechanism, which offers stronger isolation guarantees without suffering the performance penalties of full hardware virtualization. However, due to different storage semantics between containers and unikernels, the state-of-the-art approaches for using unikernels in place of containers result in decreased performance, inefficient resource utilization and limited functionality. In this paper we bridge the storage gap between containers and unikernels in the context of serverless computing. First, we examine and categorize the storage requirements for building and running functions based on unikernels. Based on these requirements, we design and prototype a framework, which extends the Docker storage layer to support unikernel images. Our framework enables the sharing of common read-only unikernel image layers between functions and moves the unikernel image building overhead away from the critical path of function execution. We show that our framework improves function instantiation times while reducing storage space overhead. © 2019 Association for Computing Machinery.","Cloud computing; Container storage; Containers; Unikernels; Virtualization","Cloud computing; Middleware; Semantics; Virtual reality; Virtualization; Computing paradigm; Hardware virtualization; Performance penalties; Resource utilizations; State-of-the-art approach; Storage requirements; System virtualization; Unikernels; Containers","Association for Computing Machinery, Inc","ACM","5th International Workshop on Serverless Computing, WOSC 2019 - Part of Middleware 2019","9 December 2019 through 13 December 2019","Davis","156757","Conference paper","Final","","Scopus","2-s2.0-85078887217"
"Skluzacek T.J.; Chard R.; Wong R.; Li Z.; Babuji Y.N.; Ward L.; Blaiszik B.; Chard K.; Foster I.","Skluzacek, Tyler J. (57193628652); Chard, Ryan (55588066400); Wong, Ryan (57215290441); Li, Zhuozhao (56275944000); Babuji, Yadu N. (57193607019); Ward, Logan (55556371700); Blaiszik, Ben (15043808100); Chard, Kyle (9132950200); Foster, Ian (35572232000)","57193628652; 55588066400; 57215290441; 56275944000; 57193607019; 55556371700; 15043808100; 9132950200; 35572232000","Serverless workflows for indexing large scientific data","2019","WOSC 2019 - Proceedings of the 2019 5th International Workshop on Serverless Computing, Part of Middleware 2019","","","","43","48","5","14","10.1145/3366623.3368140","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078230642&doi=10.1145%2f3366623.3368140&partnerID=40&md5=714ad3b53fde2970078a438aa5afbcc0","The use and reuse of scientific data is ultimately dependent on the ability to understand what those data represent, how they were captured, and how they can be used. In many ways, data are only as useful as the metadata available to describe them. Unfortunately, due to growing data volumes, large and distributed collaborations, and a desire to store data for long periods of time, scientific “data lakes” quickly become disorganized and lack the metadata necessary to be useful to researchers. New automated approaches are needed to derive metadata from scientific files and to use these metadata for organization and discovery. Here we describe one such system, Xtract, a service capable of processing vast collections of scientific files and automatically extracting metadata from diverse file types. Xtract relies on function as a service models to enable scalable metadata extraction by orchestrating the execution of many, short-running extractor functions. To reduce data transfer costs, Xtract can be configured to deploy extractors centrally or near to the data (i.e., at the edge). We present a prototype implementation of Xtract and demonstrate that it can derive metadata from a 7 TB scientific data repository. © 2019 Association for Computing Machinery.","Data lakes; File systems; Materials science; Metadata extraction; Serverless","Data transfer; Extraction; Lakes; Materials science; Metadata; Middleware; Automated approach; Distributed collaboration; File systems; Meta-data extractions; Prototype implementations; Scientific data; Serverless; Service Model; Data mining","Association for Computing Machinery, Inc","ACM","5th International Workshop on Serverless Computing, WOSC 2019 - Part of Middleware 2019","9 December 2019 through 13 December 2019","Davis","156757","Conference paper","Final","","Scopus","2-s2.0-85078230642"
"Skluzacek T.J.","Skluzacek, Tyler J. (57193628652)","57193628652","Dredging a data lake: Decentralized metadata extraction","2019","Middleware 2019 - Proceedings of the 2019 20th International Middleware Conference Doctoral Symposium, Part of Middleware 2019","","","","51","53","2","6","10.1145/3366624.3368170","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078240684&doi=10.1145%2f3366624.3368170&partnerID=40&md5=f3b50623ca2e06a8367bf17c84df9aa7","The rapid generation of data from distributed IoT devices, scientific instruments, and compute clusters presents unique data management challenges. The influx of large, heterogeneous, and complex data causes repositories to become siloed or generally unsearchable-both problems not currently well-addressed by distributed file systems. In this work, we propose Xtract, a serverless middleware to extract metadata from files spread across heterogeneous edge computing resources. In my future work, we intend to study how Xtract can automatically construct file extraction workflows subject to users' cost, time, security, and compute allocation constraints. To this end, Xtract will enable the creation of a searchable centralized index across distributed data collections. © 2019 Association for Computing Machinery.","Data lakes; File systems; Metadata extraction; Serverless","Extraction; File organization; Information management; Lakes; Metadata; Middleware; Computing resource; Data management challenges; Distributed file systems; File systems; Meta-data extractions; Rapid generations; Scientific instrument; Serverless; Data mining","Association for Computing Machinery, Inc","ACM","20th International Middleware Conference Doctoral Symposium, Middleware 2019, Part of Middleware 2019","9 December 2019 through 13 December 2019","Davis","156196","Conference paper","Final","","Scopus","2-s2.0-85078240684"
"Zhao L.; Yang Y.; Li Y.; Zhou X.; Li K.","Zhao, Laiping (35243865000); Yang, Yanan (57208671879); Li, Yiming (57212459721); Zhou, Xian (57808643600); Li, Keqiu (57204189178)","35243865000; 57208671879; 57212459721; 57808643600; 57204189178","Understanding, Predicting and Scheduling Serverless Workloads under Partial Interference","2021","International Conference for High Performance Computing, Networking, Storage and Analysis, SC","","","","","","","43","10.1145/3458817.3476215","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119983834&doi=10.1145%2f3458817.3476215&partnerID=40&md5=a3a5c8924fa68b297736b61c71f45ae0","Interference among distributed cloud applications can be classified into three types: full, partial and zero. While prior research merely focused on full interference, the partial interference that occurs at parts of applications is far more common yet still lacks in-depth study. Serverless computing that structures applications into small-sized, short-lived functions further exacerbate partial interference. We characterize the features of partial interference in serverless as exhibiting high volatility, spatial-Temporal variation, and propagation. Given these observations, we propose an incremental learning predictor, named Gsight, which can achieve high precision by harnessing the spatial-Temporal overlap codes and profiles of functions via an end-To-end call path. Experimental results show that Gsight can achieve an average error of 1.71%. Its convergence speed is at least 3× faster than that in a serverful system. A scheduling case study shows that the proposed method can improve function density by 18.79% while guaranteeing the quality of service (QoS). © 2021 IEEE Computer Society. All rights reserved.","Partial Interference; Performance Prediction; Resource Utilization; Serverless","Quality of service; Scheduling; Classifieds; Cloud applications; Distributed clouds; In-depth study; Partial interference; Performance prediction; Predicting and scheduling; Resources utilizations; Serverless; Spatial temporals; Distributed cloud","IEEE Computer Society","ACM�s Special Interest Group SIGHPC; Association for Computing Machinery (ACM); IEEE Computer Society; IEEE�s Technical Committee on High Performance Computing (TCHPC)","33rd International Conference for High Performance Computing, Networking, Storage and Analysis: Science and Beyond, SC 2021","14 November 2021 through 19 November 2021","Virtual, Online","174017","Conference paper","Final","","Scopus","2-s2.0-85119983834"
"Uta A.; Duplyakin D.; Abad C.; Herbst N.; Iosup A.","Uta, Alexandru (56440094500); Duplyakin, Dmitry (55791259100); Abad, Cristina (22957218500); Herbst, Nikolas (55747080400); Iosup, Alexandru (23392350500)","56440094500; 55791259100; 22957218500; 55747080400; 23392350500","3rd workshop on hot topics in cloud computing performance (HotCloudPerf'20): Performance variability","2020","ICPE 2020 - Proceedings of the ACM/SPEC International Conference on Performance Engineering","","","","301","302","1","0","10.1145/3358960.3383768","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085917769&doi=10.1145%2f3358960.3383768&partnerID=40&md5=73bdc70066343c11dce3ed9a8c3e9042","The organizers of the Third Workshop on Hot Topics in Cloud Computing Performance (HotCloudPerf 2020) are delighted to welcome you to the workshop proceedings as part of the ICPE conference companion. The HotCloudPerf 2020 workshop is a full-day workshop on Tuesday, April 21, taking place jointly with WOSP-C as part of the ICPE conference week in Edmonton, Canada. Each year, the workshop chooses a focus theme to explore; for 2020, the theme is ""Performance variability of cloud datacenters and the implications of such phenomena on application performance"" Cloud computing is emerging as one of the most profound changes in the way we build and use IT. The use of global services in public clouds is increasing, and the lucrative and rapidly. © 2020 Owner/Author.","Cloud; Performance; Performance variability; Serverless","Engineering; Industrial engineering; Application performance; Data centers; Edmonton , Canada; Global services; Hot topics; Performance variability; Public clouds; Cloud computing","Association for Computing Machinery, Inc","ACM Special Interest Group on Measurement and Evaluation (SIGMETRICS); ACM Special Interest Group on Software Engineering (SIGSOFT)","11th ACM/SPEC International Conference on Performance Engineering, ICPE 2020","20 April 2020 through 24 April 2020","Edmonton","160106","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85085917769"
"Barcelona-Pons D.; Sánchez-Artigas M.; París G.; Sutra P.; García-López P.","Barcelona-Pons, Daniel (57206657129); Sánchez-Artigas, Marc (10939155200); París, Gerard (24721553300); Sutra, Pierre (23467592200); García-López, Pedro (24479469800)","57206657129; 10939155200; 24721553300; 23467592200; 24479469800","On the FaaS track: Building stateful distributed applications with serverless architectures","2019","Middleware 2019 - Proceedings of the 2019 20th International Middleware Conference","","","","41","54","13","88","10.1145/3361525.3361535","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078068295&doi=10.1145%2f3361525.3361535&partnerID=40&md5=bd5b0656720a5f31e36721ba3faf29d3","Serverless computing is an emerging paradigm that greatly simplifies the usage of cloud resources and suits well to many tasks. Most notably, Function-as-a-Service (FaaS) enables programmers to develop cloud applications as individual functions that can run and scale independently. Yet, due to the disaggregation of storage and compute resources in FaaS, applications that require fine-grained support for mutable state and synchronization, such as machine learning and scientific computing, are hard to build. In this work, we present Crucial, a system to program highly-concurrent stateful applications with serverless architectures. Its programming model keeps the simplicity of FaaS and allows to port effortlessly multi-threaded algorithms to this new environment. Crucial is built upon the key insight that FaaS resembles to concurrent programming at the scale of a data center. As a consequence, a distributed shared memory layer is the right answer to the need for fine-grained state management and coordination in serverless. We validate our system with the help of micro-benchmarks and various applications. In particular, we implement two common machine learning algorithms: k-means clustering and logistic regression. For both cases, Crucial obtains superior or comparable performance to an equivalent Spark cluster. © 2019 Association for Computing Machinery.","FaaS; In-memory; Serverless; Stateful; Synchronization","Application programs; Computer programming; Digital storage; K-means clustering; Learning algorithms; Machine learning; Memory architecture; Middleware; Synchronization; Concurrent programming; Distributed applications; Distributed shared memory; FaaS; Multi-threaded algorithms; Serverless; Serverless architecture; Stateful; Benchmarking","Association for Computing Machinery, Inc","et al.; Hewlett-Packard Enterprise; NSF; Raytheon BBN Technologies; rti; SIEMENS","20th ACM/IFIP/USENIX Middleware Conference, Middleware 2019","9 December 2019 through 13 December 2019","Davis","156194","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85078068295"
"Singhvi A.; Khalid J.; Akella A.; Banerjee S.","Singhvi, Arjun (57200408847); Khalid, Junaid (25927179300); Akella, Aditya (8383144400); Banerjee, Sujata (7404544979)","57200408847; 25927179300; 8383144400; 7404544979","SNF: Serverless network functions","2020","SoCC 2020 - Proceedings of the 2020 ACM Symposium on Cloud Computing","","","","296","310","14","20","10.1145/3419111.3421295","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095455718&doi=10.1145%2f3419111.3421295&partnerID=40&md5=53a36270204a6b6c59c7abc63f46c86a","Our work addresses how a cloud provider can offer Network Functions (NF) as a Service, or NFaaS, using the emerging serverless computing paradigm. Serverless computing has the right NFaaS building blocks - usage-based billing, event-driven programming model and elastic scaling. But we identify two core limitations of existing serverless platforms that undermine support for NFaaS - coupling of the billing and work assignment granularities, and state sharing via an external store. Our framework, SNF, overcomes these limitations via two ideas. SNF allocates work at the granularity of flowlets observed in network traffic, whereas billing and programming occur at a finer level. SNF embellishes serverless platforms with ephemeral local state that lasts for the flowlet duration and supports high performance state operations. We demonstrate that our SNF prototype matches utilization closely with demand and reduces tail packet processing latency substantially compared to alternatives.  © 2020 ACM.","flowlets; network functions; serverless computing","Cloud computing; Building blockes; Cloud providers; Computing paradigm; Event-driven programming model; Network functions; Packet processing; Performance States; Work assignments; Transfer functions","Association for Computing Machinery, Inc","ACM SIGMOD; ACM SIGOPS; Alibaba Group; aws; et al.; Microsoft","11th ACM Symposium on Cloud Computing, SoCC 2020","19 October 2020 through 21 October 2020","Virtual, Online","163871","Conference paper","Final","","Scopus","2-s2.0-85095455718"
"Ali A.; Pinciroli R.; Yan F.; Smirni E.","Ali, Ahsan (57207417176); Pinciroli, Riccardo (56938727000); Yan, Feng (50862222500); Smirni, Evgenia (6603711126)","57207417176; 56938727000; 50862222500; 6603711126","Batch: Machine learning inference serving on serverless platforms with adaptive batching","2020","International Conference for High Performance Computing, Networking, Storage and Analysis, SC","2020-November","","9355312","","","","134","10.1109/SC41405.2020.00073","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102384238&doi=10.1109%2fSC41405.2020.00073&partnerID=40&md5=2a6bdf4bc5184242c3ce4f5f1de5066e","Serverless computing is a new pay-per-use cloud service paradigm that automates resource scaling for stateless functions and can potentially facilitate bursty machine learning serving. Batching is critical for latency performance and cost-effectiveness of machine learning inference, but unfortunately it is not supported by existing serverless platforms due to their stateless design. Our experiments show that without batching, machine learning serving cannot reap the benefits of serverless computing. In this paper, we present BATCH, a framework for supporting efficient machine learning serving on serverless platforms. BATCH uses an optimizer to provide inference tail latency guarantees and cost optimization and to enable adaptive batching support. We prototype BATCH atop of AWS Lambda and popular machine learning inference systems. The evaluation verifies the accuracy of the analytic optimizer and demonstrates performance and cost advantages over the state-of-the-art method MArk and the state-of-the-practice tool SageMaker. © 2020 IEEE.","Batching; Cloud; Cost-effective; Inference; Machine-learning-as-a-service (MLaaS); Modeling; Optimization; Prediction; Serverless; Service Level Objective (SLO); Serving","Cost effectiveness; Cloud services; Cost advantages; Cost optimization; Inference systems; Latency performance; Pay-per-use; State of the practice; State-of-the-art methods; Machine learning","IEEE Computer Society","ACM's Special Interest Group on High Performance Computing (SIGHPC); Association for Computing Machinery; IEEE Computer Society; IEEE's Technical Committee on High Performance Computing (TCHPC)","2020 International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2020","9 November 2020 through 19 November 2020","Virtual, Atlanta","167380","Conference paper","Final","","Scopus","2-s2.0-85102384238"
"Smirnov F.; Pourmohseni B.; Fahringer T.","Smirnov, Fedor (57190064460); Pourmohseni, Behnaz (56039584800); Fahringer, Thomas (7004334921)","57190064460; 56039584800; 7004334921","Apollo: Modular and Distributed Runtime System for Serverless Function Compositions on Cloud, Edge, and IoT Resources","2021","HiPS 2021 - Proceedings of the 1st Workshop on High Performance Serverless Computing, co-located with HPDC 2021","","","","5","8","3","3","10.1145/3452413.3464793","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110779829&doi=10.1145%2f3452413.3464793&partnerID=40&md5=eadcc4158b4d0a370d31e7b87685ce0b","This paper provides a first presentation of Apollo, a runtime system for serverless function compositions distributed across the cloud-edge-IoT continuum. Apollo's modular design enables a fine-grained decomposition of the runtime implementation(scheduling, data transmission, etc.) of the application, so that each of the numerous implementation decisions can be optimized separately, fully exploiting the potential for the optimization of the overall performance and costs. Apollo features (a) a flexible model of the application and the available resources and (b) an implementation process based on a large set of independent agents. This flexible structure enables distributing not only the processing, but the implementation process itself across a large number of resources, each running an independent Apollo instance. The ability to flexibly determine the placement of implementation actions opens up new optimization opportunities, while at the same time providing access to greater computing power for optimizing challenging decisions such as task scheduling and the placement and routing of data.  © 2020 ACM.","event-based; iot; orchestration; serverless; workflows","Flexible structures; Computing power; Distributed runtime; Implementation process; Independent agents; Modular designs; Placement and routing; Runtime systems; Task-scheduling; Internet of things","Association for Computing Machinery, Inc","ACM SIGARCH; University of Arizona","1st Workshop on High Performance Serverless Computing, HiPS 2021 - Co-located with HPDC 2021","25 June 2021","Virtual, Online","169886","Conference paper","Final","","Scopus","2-s2.0-85110779829"
"Cadden J.; Unger T.; Awad Y.; Dong H.; Krieger O.; Appavoo J.","Cadden, James (57209225293); Unger, Thomas (57217096525); Awad, Yara (57203371918); Dong, Han (57212533954); Krieger, Orran (57207594975); Appavoo, Jonathan (6506363364)","57209225293; 57217096525; 57203371918; 57212533954; 57207594975; 6506363364","SEUSS: Skip redundant paths to make serverless fast","2020","Proceedings of the 15th European Conference on Computer Systems, EuroSys 2020","","","","","","","121","10.1145/3342195.3392698","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087107959&doi=10.1145%2f3342195.3392698&partnerID=40&md5=7c393950590d0b7cd76c31891e4148d7","This paper presents a system-level method for achieving the rapid deployment and high-density caching of serverless functions in a FaaS environment. For reduced start times, functions are deployed from unikernel snapshots, bypassing expensive initialization steps. To reduce the memory footprint of snapshots we apply page-level sharing across the entire software stack that is required to run a function. We demonstrate the effects of our techniques by replacing Linux on the compute node of a FaaS platform architecture. With our prototype OS, the deployment time of a function drops from 100s of milliseconds to under 10 ms. Platform throughput improves by 51x on workload composed entirely of new functions. We are able to cache over 50,000 function instances in memory as opposed to 3,000 using standard OS techniques. In combination, these improvements give the FaaS platform a new ability to handle large-scale bursts of requests. © 2020 ACM.","","Computer operating systems; Deployment time; Initialization step; Memory footprint; New functions; Platform architecture; Rapid deployments; Redundant paths; Software stacks; Cache memory","Association for Computing Machinery, Inc","ACM Special Interest Group on Operating Systems (SIGOPS)","15th European Conference on Computer Systems, EuroSys 2020","27 April 2020 through 30 April 2020","Heraklion","160104","Conference paper","Final","","Scopus","2-s2.0-85087107959"
"Romero F.; Chaudhry G.I.; Goiri Í.; Gopa P.; Batum P.; Yadwadkar N.J.; Fonseca R.; Kozyrakis C.; Bianchini R.","Romero, Francisco (57205890307); Chaudhry, Gohar Irfan (57219259486); Goiri, Íñigo (24801831000); Gopa, Pragna (57223749879); Batum, Paul (57219260740); Yadwadkar, Neeraja J. (56429519900); Fonseca, Rodrigo (59847856900); Kozyrakis, Christos (6602525246); Bianchini, Ricardo (7006939544)","57205890307; 57219259486; 24801831000; 57223749879; 57219260740; 56429519900; 59847856900; 6602525246; 7006939544","Faa$T: A transparent auto-scaling cache for serverless applications","2021","SoCC 2021 - Proceedings of the 2021 ACM Symposium on Cloud Computing","","","","122","137","15","93","10.1145/3472883.3486974","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119252418&doi=10.1145%2f3472883.3486974&partnerID=40&md5=c647877b49c497d928908b3fae011918","Function-as-a-Service (FaaS) has become an increasingly popular way for users to deploy their applications without the burden of managing the underlying infrastructure. However, existing FaaS platforms rely on remote storage to maintain state, limiting the set of applications that can be run efficiently. Recent caching work for FaaS platforms has tried to address this problem, but has fallen short: it disregards the widely different characteristics of FaaS applications, does not scale the cache based on data access patterns, or requires changes to applications. To address these limitations, we present Faa$T, a transparent auto-scaling distributed cache for serverless applications. Each application gets its own cache. After a function executes and the application becomes inactive, the cache is unloaded from memory with the application. Upon reloading for the next invocation, Faa$T pre-warms the cache with objects likely to be accessed. In addition to traditional compute-based scaling, Faa$T scales based on working set and object sizes to manage cache space and I/O bandwidth. We motivate our design with a comprehensive study of data access patterns on Azure Functions. We implement Faa$T for Azure Functions, and show that Faa$T can improve performance by up to 92% (57% on average) for challenging applications, and reduce cost for most users compared to state-of-the-art caching systems, i.e. the cost of having to stand up additional serverful resources.  © 2021 Association for Computing Machinery.","Caching; Cloud computing; Function as a service (FaaS); Serverless computing","Cloud computing; Caching; Cloud-computing; Data access patterns; Distributed cache; Function as a service; Reloadings; Remote storage; Scalings; Serverless computing; Services applications; Digital storage","Association for Computing Machinery, Inc","ACM Special Interest Groups on Management of Data (ACM SIGMOD); ACM Special Interest Groups on Operating Systems (ACM SIGOPS); et al.; Exotanium; Microsoft; Salesforce","12th Annual ACM Symposium on Cloud Computing, SoCC 2021","1 November 2021 through 4 November 2021","Virtual, Online","173200","Conference paper","Final","","Scopus","2-s2.0-85119252418"
"Shankar V.; Krauth K.; Vodrahalli K.; Pu Q.; Recht B.; Stoica I.; Ragan-Kelley J.; Jonas E.; Venkataraman S.","Shankar, Vaishaal (56597606100); Krauth, Karl (57196034655); Vodrahalli, Kailas (57210640380); Pu, Qifan (55841232900); Recht, Benjamin (6602593952); Stoica, Ion (7007009125); Ragan-Kelley, Jonathan (18435174100); Jonas, Eric (56640967500); Venkataraman, Shivaram (55312380900)","56597606100; 57196034655; 57210640380; 55841232900; 6602593952; 7007009125; 18435174100; 56640967500; 55312380900","Serverless linear algebra","2020","SoCC 2020 - Proceedings of the 2020 ACM Symposium on Cloud Computing","","","","281","295","14","71","10.1145/3419111.3421287","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095420600&doi=10.1145%2f3419111.3421287&partnerID=40&md5=e32907103777b10205b93f832ad00aa3","Datacenter disaggregation provides numerous benefits to both the datacenter operator and the application designer. However switching from the server-centric model to a disaggregated model requires developing new programming abstractions that can achieve high performance while benefiting from the greater elasticity. To explore the limits of datacenter disaggregation, we study an application area that near-maximally benefits from current server-centric datacenters: dense linear algebra. We build NumPyWren, a system for linear algebra built on a disaggregated serverless programming model, and LAmbdaPACK, a companion domain-specific language designed for serverless execution of highly parallel linear algebra algorithms. We show that, for a number of linear algebra algorithms such as matrix multiply, singular value decomposition, Cholesky decomposition, and QR decomposition, NumPyWren's performance (completion time) is within a factor of 2 of optimized server-centric MPI implementations, and has up to 15% greater compute efficiency (total CPU-hours), while providing fault tolerance.  © 2020 Owner/Author.","","Cloud computing; Fault tolerance; Problem oriented languages; Cholesky decomposition; Dense linear algebra; Domain specific languages; Linear algebra algorithms; Mpi implementations; Programming abstractions; Programming models; Q R decomposition; Singular value decomposition","Association for Computing Machinery, Inc","ACM SIGMOD; ACM SIGOPS; Alibaba Group; aws; et al.; Microsoft","11th ACM Symposium on Cloud Computing, SoCC 2020","19 October 2020 through 21 October 2020","Virtual, Online","163871","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85095420600"
"","","","Middleware Industry 2020 - Proceedings of the 2020 21st International Middleware Conference Industrial Track, Part of Middleware 2020","2020","Middleware Industry 2020 - Proceedings of the 2020 21st International Middleware Conference Industrial Track, Part of Middleware 2020","","","","","","50","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100468197&partnerID=40&md5=90a7781196999394bf3e0b584bb7dc74","The proceedings contain 6 papers. The topics discussed include: mmFilter: language-guided video analytics at the edge; blockNDP: block-storage near data processing; DiagSys: network and third-party web-service monitoring from the browser’s perspective (industry track); HopsFS-S3: extending object stores with POSIX-like semantics and more (industry track); Primula: a practical shuﬄe/sort operator for serverless computing; and secure volume hot-plugging for containers (industry track).","","","Association for Computing Machinery, Inc","ACM","21st International Middleware Conference Industrial Track, Middleware Industry 2020 - Part of Middleware 2020","7 December 2020 through 11 December 2020","Virtual, Online","166461","Conference review","Final","","Scopus","2-s2.0-85100468197"
"","","","Middleware 2020 - Proceedings of the 2020 21st International Middleware Conference","2020","Middleware 2020 - Proceedings of the 2020 21st International Middleware Conference","","","","","","405","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098508770&partnerID=40&md5=6518a33c6272193709133a88ecf281b6","The proceedings contain 30 papers. The topics discussed include: prebaking functions to warm the serverless cold start; resilient cloud-based replication with low latency; practical active revocation; secureTF: a secure tensorflow framework; on delivery guarantees in distributed content-based publish/subscribe systems; MATCH: a decentralized middleware for fair matchmaking in peer-to-peer markets; PipeTune: pipeline parallelism of hyper and system parameters tuning for deep learning clusters; fast training of deep learning models over multiple GPUs; and FLeet: online federated learning via staleness awareness and performance prediction.","","","Association for Computing Machinery, Inc","Association for Computing Machinery (ACM); IFIP; TU Delft; USENIX","21st International Middleware Conference, Middleware 2020","7 December 2020 through 11 December 2020","Virtual, Online","165666","Conference review","Final","","Scopus","2-s2.0-85098508770"
"Jain A.; Urgaonkar B.; Baarzi A.F.; Alfares N.; Kesidis G.; Kandemir M.","Jain, Aman (57205024886); Urgaonkar, Bhuvan (14020908300); Baarzi, Ata F. (57218214304); Alfares, Nader (57219230684); Kesidis, George (7003540724); Kandemir, Mahmut (35549787100)","57205024886; 14020908300; 57218214304; 57219230684; 7003540724; 35549787100","SplitServe: Efficiently splitting Apache spark jobs across Faas and IaAs","2020","Middleware 2020 - Proceedings of the 2020 21st International Middleware Conference","","","","236","250","14","11","10.1145/3423211.3425695","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098494085&doi=10.1145%2f3423211.3425695&partnerID=40&md5=0ee9e934e8037fce282c9fa3812ff5f7","Due to their lower startup latencies and finer-grain pricing than virtual machines (VMs), Amazon Lambdas and other cloud functions (CFs) have been identified as ideal candidates for handling unexpected spikes in simple, stateless workloads. However, it is not immediately clear if CFs would be similarly effective in autoscaling complex workloads involving significant state transfer across distributed application components. We have found that, through careful design, currently available CFs can indeed be useful even for complex workloads. To demonstrate this, we design and implement SplitServe, an enhancement of Apache Spark. If not enough executors on existing VMs are available for a newly arriving latency-sensitive job, SplitServe is able to use CFs to quickly bridge this shortfall in VMs, so avoiding the startup latencies of newly requested VMs. If desirable in terms of performance or cost, when newly requested VMs, or executors on existing VMs, do become available, SplitServe is able to move ongoing work from CFs to them. Our experimental evaluation of SplitServe using four different workloads (either on a mixture of VM-based executors and CFs or just CFs) shows that it improves execution time by up to (a) 55% for workloads with small to modest amount of shuffling, and (b) 31% in workloads with large amounts of shuffling, when compared to only VM-based autoscaling. © 2020 Association for Computing Machinery.","Apache Spark; Cloud Computing; Cloud Functions; Virtual Machines","Virtual machine; Autoscaling; Design and implements; Distributed applications; Experimental evaluation; Finer grains; Large amounts; State transfer; Middleware","Association for Computing Machinery, Inc","Association for Computing Machinery (ACM); IFIP; TU Delft; USENIX","21st International Middleware Conference, Middleware 2020","7 December 2020 through 11 December 2020","Virtual, Online","165666","Conference paper","Final","","Scopus","2-s2.0-85098494085"
"Barcelona-Pons D.; García-López P.; Ruiz Á.; Gómez-Gómez A.; París G.; Sánchez-Artigas M.","Barcelona-Pons, Daniel (57206657129); García-López, Pedro (24479469800); Ruiz, Álvaro (57214676704); Gómez-Gómez, Amanda (57214675580); París, Gerard (24721553300); Sánchez-Artigas, Marc (10939155200)","57206657129; 24479469800; 57214676704; 57214675580; 24721553300; 10939155200","FaAS orchestration of parallel workloads","2019","WOSC 2019 - Proceedings of the 2019 5th International Workshop on Serverless Computing, Part of Middleware 2019","","","","25","30","5","23","10.1145/3366623.3368137","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078887296&doi=10.1145%2f3366623.3368137&partnerID=40&md5=c7d88215d3763a675892b5ae1c8fd168","Function as a Service (FaaS) is based on a reactive programming model where functions are activated by triggers in response to cloud events (e.g., objects added to an object store). The inherent elasticity and the pay-per-use model of serverless functions make them very appropriate for embarrassingly parallel tasks like data preprocessing, or even the execution of MapReduce jobs in the cloud. But current Serverless orchestration systems are not designed for managing parallel fork-join workflows in a scalable and efficient way. We demonstrate in this paper that existing services like AWS Step Functions or Azure Durable Functions incur in considerable overheads, and only Composer at IBM Cloud provides suitable performance. Successively, we analyze the architecture of OpenWhisk as an open-source FaaS systems and its orchestration features (Composer). We outline its architecture problems and propose guidelines for orchestrating massively parallel workloads using serverless functions. © 2019 Association for Computing Machinery.","Event-based; FaaS; Orchestration; Serverless","Middleware; Open source software; Data preprocessing; Event-based; FaaS; Massively parallels; Orchestration; Parallel workloads; Reactive programming; Serverless; Open systems","Association for Computing Machinery, Inc","ACM","5th International Workshop on Serverless Computing, WOSC 2019 - Part of Middleware 2019","9 December 2019 through 13 December 2019","Davis","156757","Conference paper","Final","","Scopus","2-s2.0-85078887296"
"Cordingly R.; Yu H.; Hoang V.; Sadeghi Z.; Foster D.; Perez D.; Hatchett R.; Lloyd W.","Cordingly, Robert (57220806485); Yu, Hanfei (57220804031); Hoang, Varik (57220804100); Sadeghi, Zohreh (57220805982); Foster, David (57220804025); Perez, David (57219742288); Hatchett, Rashad (57220806487); Lloyd, Wes (8537012400)","57220806485; 57220804031; 57220804100; 57220805982; 57220804025; 57219742288; 57220806487; 8537012400","The Serverless Application Analytics Framework: Enabling Design Trade-off Evaluation for Serverless Software","2020","WOSC 2020 - Proceedings of the 2020 6th International Workshop on Serverless Computing, Part of Middleware 2020","","","","67","72","5","24","10.1145/3429880.3430103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099550918&doi=10.1145%2f3429880.3430103&partnerID=40&md5=ecd7e97af17c8305211d74f6b6643b41","To help better understand factors that impact performance on Function-as-a-Service (FaaS) platforms we have developed the Serverless Application Analytics Framework (SAAF). SAAF provides a reusable framework supporting multiple programming languages that developers can integrate into a function's package for deployment to multiple commercial and open source FaaS platforms. SAAF improves the observability of FaaS function deployments by collecting forty-eight distinct metrics to enable developers to profile CPU and memory utilization, monitor infrastructure state, and observe platform scalability. In this paper, we describe SAAF in detail and introduce supporting tools highlighting important features and how to use them. Our client application, FaaS Runner, provides a tool to orchestrate workloads and automate the process of conducting experiments across FaaS platforms. We provide a case study demonstrating the integration of SAAF into an existing open source image processing pipeline built for AWS Lambda. Using FaaS Runner, we automate experiments and acquire metrics from SAAF to profile each function of the pipeline to evaluate performance implications. Finally, we summarize contributions using our tools to evaluate implications of different programming languages for serverless data processing, and to build performance models to predict runtime for serverless workloads.  © 2020 ACM.","Frameworks; Function-as-a-Service; Performance Evaluation; Programming Languages; Serverless Computing","Application programs; Data handling; Economic and social effects; Function evaluation; Image processing; Open source software; Pipeline processing systems; Pipelines; Commercial sources; Design tradeoff; Framework; Function-as-a-service; Impact performance; Open-source; Performances evaluation; Programming language; Serverless computing; Source functions; Computer software reusability","Association for Computing Machinery, Inc","ACM","6th International Workshop on Serverless Computing, WOSC 2020 - Part of Middleware 2020","7 December 2020 through 11 December 2020","Virtual, Online","166193","Conference paper","Final","","Scopus","2-s2.0-85099550918"
"Choi J.; Lee K.","Choi, Jaeghang (57221607405); Lee, Kyungyong (57196193080)","57221607405; 57196193080","Evaluation of Network File System as a Shared Data Storage in Serverless Computing","2020","WOSC 2020 - Proceedings of the 2020 6th International Workshop on Serverless Computing, Part of Middleware 2020","","","","25","30","5","4","10.1145/3429880.3430096","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099566413&doi=10.1145%2f3429880.3430096&partnerID=40&md5=cee17f510537b4759ba5aa634cc2e796","Fully-managed cloud and Function-as-a-Service (FaaS) services allow the wide adoption of serverless computing for various cloud-native applications. Despite the many advantages that serverless computing provides, no direct connection support exists between function run-times, and it is a barrier for data-intensive applications. To overcome this limitation, the leading cloud computing vendor Amazon Web Services (AWS) has started to support mounting the network file system (NFS) across different function run-times. This paper quantitatively evaluates the performance of accessing NFS storage from multiple function run-times and compares the performance with other methods of sharing data among function run-times. Despite the great qualitative benefits of the approach, the limited I/O bandwidth of NFS storage can become a bottleneck, especially when the number of concurrent access from function run-times increases.  © 2020 ACM.","","File organization; Function evaluation; Web services; Accessing network; Amazon web services; Cloud-computing; Data storage; Data-intensive application; Multiple function; Network file system; Performance; Runtimes; Shared data; Digital storage","Association for Computing Machinery, Inc","ACM","6th International Workshop on Serverless Computing, WOSC 2020 - Part of Middleware 2020","7 December 2020 through 11 December 2020","Virtual, Online","166193","Conference paper","Final","","Scopus","2-s2.0-85099566413"
"Fuerst A.; Sharma P.","Fuerst, Alexander (57218225304); Sharma, Prateek (58273575200)","57218225304; 58273575200","FaasCache: Keeping serverless computing alive with greedy-dual caching","2021","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","","","","386","400","14","167","10.1145/3445814.3446757","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104696796&doi=10.1145%2f3445814.3446757&partnerID=40&md5=56e522595267899cb351310a7376301c","Functions as a Service (also called serverless computing) promises to revolutionize how applications use cloud resources. However, functions suffer from cold-start problems due to the overhead of initializing their code and data dependencies before they can start executing. Keeping functions alive and warm after they have finished execution can alleviate the cold-start overhead. Keep-alive policies must keep functions alive based on their resource and usage characteristics, which is challenging due to the diversity in FaaS workloads. Our insight is that keep-alive is analogous to caching. Our caching-inspired Greedy-Dual keep-alive policy can be effective in reducing the cold-start overhead by more than 3× compared to current approaches. Caching concepts such as reuse distances and hit-ratio curves can also be used for auto-scaled server resource provisioning, which can reduce the resource requirement of FaaS providers by 30% for real-world dynamic workloads. We implement caching-based keep-alive and resource provisioning policies in our FaasCache system, which is based on OpenWhisk. We hope that our caching analogy opens the door to more principled and optimized keep-alive and resource provisioning techniques for future FaaS workloads and platforms. © 2021 ACM.","Caching; Cloud Computing; Functions as a Service; Serverless Computing","Cold start problems; Data dependencies; Keep-alive; Provisioning policies; Real-world; Resource requirements; Reuse distance; Server resources","Association for Computing Machinery","ACM SIGARCH; ACM SIGOPS; ACM SIGPLAN","26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2021","19 April 2021 through 23 April 2021","Virtual, Online","168445","Conference paper","Final","","Scopus","2-s2.0-85104696796"
"Romero F.; Zhao M.; Yadwadkar N.J.; Kozyrakis C.","Romero, Francisco (57205890307); Zhao, Mark (57203242519); Yadwadkar, Neeraja J. (56429519900); Kozyrakis, Christos (6602525246)","57205890307; 57203242519; 56429519900; 6602525246","Llama: A heterogeneous & serverless framework for auto-tuning video analytics pipelines","2021","SoCC 2021 - Proceedings of the 2021 ACM Symposium on Cloud Computing","","","","1","17","16","64","10.1145/3472883.3486972","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119289056&doi=10.1145%2f3472883.3486972&partnerID=40&md5=f8735fb975595a831878bde9be782619","The proliferation of camera-enabled devices and large video repositories has led to a diverse set of video analytics applications. These applications rely on video pipelines, represented as DAGs of operations, to transform videos, process extracted metadata, and answer questions like, ""Is this intersection congested?""The latency and resource efficiency of pipelines can be optimized using configurable knobs for each operation (e.g., sampling rate, batch size, or type of hardware used). However, determining efficient configurations is challenging because (a) the configuration search space is exponentially large, and (b) the optimal configuration depends on users' desired latency and cost targets, (c) input video contents may exercise different paths in the DAG and produce a variable amount intermediate results. Existing video analytics and processing systems leave it to the users to manually configure operations and select hardware resources. We present Llama: a heterogeneous and serverless framework for auto-tuning video pipelines. Given an end-to-end latency target, Llama optimizes for cost efficiency by (a) calculating a latency target for each operation invocation, and (b) dynamically running a cost-based optimizer to assign configurations across heterogeneous hardware that best meet the calculated per-invocation latency target. This makes the problem of auto-tuning large video pipelines tractable and allows us to handle input-dependent behavior, conditional branches in the DAG, and execution variability. We describe the algorithms in Llama and evaluate it on a cloud platform using serverless CPU and GPU resources. We show that compared to state-of-the-art cluster and serverless video analytics and processing systems, Llama achieves 7.8x lower latency and 16x cost reduction on average.  © 2021 Association for Computing Machinery.","Distributed systems; Heterogeneous; Scheduling; Serverless computing; Video analytics","Computer hardware; Cost reduction; Efficiency; Pipeline processing systems; Video signal processing; Analytics systems; Autotuning; Heterogeneous; Processing systems; Resource efficiencies; Serverless computing; Video analytics; Video process; Video processing; Video repository; Pipelines","Association for Computing Machinery, Inc","ACM Special Interest Groups on Management of Data (ACM SIGMOD); ACM Special Interest Groups on Operating Systems (ACM SIGOPS); et al.; Exotanium; Microsoft; Salesforce","12th Annual ACM Symposium on Cloud Computing, SoCC 2021","1 November 2021 through 4 November 2021","Virtual, Online","173200","Conference paper","Final","","Scopus","2-s2.0-85119289056"
"Gunasekaran J.R.; Thinakaran P.; Nachiappan N.C.; Kandemir M.T.; Das C.R.","Gunasekaran, Jashwant Raj (55440506600); Thinakaran, Prashanth (55428420900); Nachiappan, Nachiappan C. (55427549300); Kandemir, Mahmut Taylan (35549787100); Das, Chita R. (7201851990)","55440506600; 55428420900; 55427549300; 35549787100; 7201851990","Fifer: Tackling resource underutilization in the serverless era","2020","Middleware 2020 - Proceedings of the 2020 21st International Middleware Conference","","","","280","295","15","71","10.1145/3423211.3425683","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098501448&doi=10.1145%2f3423211.3425683&partnerID=40&md5=5b7c61544291abbc88128e4805963129","Datacenters are witnessing a rapid surge in the adoption of serverless functions for microservices-based applications. A vast majority of these microservices typically span less than a second, have strict SLO requirements, and are chained together as per the requirements of an application. The aforementioned characteristics introduce a new set of challenges, especially in terms of container provisioning and management, as the state-of-the-art resource management frameworks, employed in serverless platforms, tend to look at microservice-based applications similar to conventional monolithic applications. Hence, these frameworks suffer from microservice agnostic scheduling and colossal container over-provisioning, especially during workload fluctuations, thereby resulting in poor resource utilization. In this work, we quantify the above shortcomings using a variety of workloads on a multi-node cluster managed by the Kubernetes and Brigade serverless framework. To address them, we propose Fifer Ð an adaptive resource management framework to efficiently manage function-chains on serverless platforms. The key idea is to make Fifer (i) utilization conscious by efficiently bin packing jobs to fewer containers using function-aware container scaling and intelligent request batching, and (ii) at the same time, SLO-compliant by proactively spawning containers to avoid cold-starts, thus minimizing the overall response latency. Combining these benefits, Fifer improves container utilization and cluster-wide energy consumption by 4× and 31%, respectively, without compromising on SLO's, when compared to the state-of-the-art schedulers employed by serverless platforms. © 2020 Association for Computing Machinery.","Queuing; Resource-management; Scheduling; Serverless","Energy utilization; Middleware; Natural resources management; Resource allocation; Scheduling; Adaptive Resource Management; Bin packing; Data centers; Multi-nodes; Over provisioning; Resource management framework; Resource utilizations; State of the art; Bins","Association for Computing Machinery, Inc","Association for Computing Machinery (ACM); IFIP; TU Delft; USENIX","21st International Middleware Conference, Middleware 2020","7 December 2020 through 11 December 2020","Virtual, Online","165666","Conference paper","Final","","Scopus","2-s2.0-85098501448"
"Huber F.; Körber N.; Mock M.","Huber, Florian (57214690838); Körber, Nikolai (57479121200); Mock, Markus (57211063567)","57214690838; 57479121200; 57211063567","Selena: A serverless energy management system","2019","WOSC 2019 - Proceedings of the 2019 5th International Workshop on Serverless Computing, Part of Middleware 2019","","","","7","12","5","4","10.1145/3366623.3368134","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078915912&doi=10.1145%2f3366623.3368134&partnerID=40&md5=0d7a4c1fac7a3e8442a3ac7fd6bdeda0","Reduction of CO2 emissions has become a significant challenge faced by humanity today. Energy management systems try to contribute to addressing this challenge by enabling an intelligent use and combination of different energy sources by capturing and visualizing energy usage and production data to enable energy efficiency improvement measures. In this paper, we present Selena, a prototypical energy management system that is implemented using the serverless computing paradigm. Essential design goals for Selena are both extensibility, so that many different data sources and providers (e.g., measurement systems) can be integrated easily, as well as efficient scalability, so that the system can be used from small (e.g., one building) to large installations (potentially entire neighborhoods) with deployment cost commensurate with the installation size. Our initial experiences with Selena indicate that the serverless paradigm is very well suited to capture and process energy-related data reliably and has excellent scaling properties due to the elastic compute platform that it is built upon. Copyright © 2019 ACM","AWS Lambda; Energy management systems; FaaS; IoT; Microservices; Serverless computing","Energy efficiency; Energy management; Information management; Middleware; AWS Lambda; Different energy sources; Energy efficiency improvements; FaaS; Measurement system; Microservices; Scaling properties; Serverless computing; Energy management systems","Association for Computing Machinery, Inc","ACM","5th International Workshop on Serverless Computing, WOSC 2019 - Part of Middleware 2019","9 December 2019 through 13 December 2019","Davis","156757","Conference paper","Final","","Scopus","2-s2.0-85078915912"
"Nguyen H.D.; Zhang C.; Xiao Z.; Chien A.A.","Nguyen, Hai Duc (57214666053); Zhang, Chaojie (59800231500); Xiao, Zhujun (57200006971); Chien, Andrew A. (7004570702)","57214666053; 59800231500; 57200006971; 7004570702","Real-time Serverless: Enabling application performance guarantees","2019","WOSC 2019 - Proceedings of the 2019 5th International Workshop on Serverless Computing, Part of Middleware 2019","","","","1","6","5","36","10.1145/3366623.3368133","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078892114&doi=10.1145%2f3366623.3368133&partnerID=40&md5=9e990a14c52b81e703e0d69fb5407763","Today’s serverless provides “function-as-a-service” with dynamic scaling and fine-grained resource charging, enabling new cloud applications. Serverless functions are invoked as a best-effort service. We propose an extension to serverless, called real-time serverless that provides an invocation rate guarantee, a service-level objective (SLO) specified by the application, and delivered by the underlying implementation. Real-time serverless allows applications to guarantee real-time performance. We study real-time serverless behavior analytically and empirically to characterize its ability to support bursty, realtime cloud and edge applications efficiently. Finally, we use a case study, traffic monitoring, to illustrate the use and benefits of real-time serverless, on our prototype implementation. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Bursty; Interface; Real-time; Serverless","Computer programming; Computer science; Interfaces (materials); Application performance; Best effort services; Bursty; Prototype implementations; Real time; Real time performance; Serverless; Service level objective; Middleware","Association for Computing Machinery, Inc","ACM","5th International Workshop on Serverless Computing, WOSC 2019 - Part of Middleware 2019","9 December 2019 through 13 December 2019","Davis","156757","Conference paper","Final","","Scopus","2-s2.0-85078892114"
"Zhang Y.; Goiri I.; Chaudhry G.I.; Fonseca R.; Elnikety S.; Delimitrou C.; Bianchini R.","Zhang, Yanqi (57208405061); Goiri, Íñigo (24801831000); Chaudhry, Gohar Irfan (57219259486); Fonseca, Rodrigo (59847856900); Elnikety, Sameh (8366429300); Delimitrou, Christina (38361424300); Bianchini, Ricardo (7006939544)","57208405061; 24801831000; 57219259486; 59847856900; 8366429300; 38361424300; 7006939544","Faster and Cheaper Serverless Computing on Harvested Resources","2021","SOSP 2021 - Proceedings of the 28th ACM Symposium on Operating Systems Principles","","","","724","739","15","109","10.1145/3477132.3483580","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119096405&doi=10.1145%2f3477132.3483580&partnerID=40&md5=124cc67eb5b3353b43aef46ff8274c82","Serverless computing is becoming increasingly popular due to its ease of programming, fast elasticity, and fine-grained billing. However, the serverless provider still needs to provision, manage, and pay the IaaS provider for the virtual machines (VMs) hosting its platform. This ties the cost of the serverless platform to the cost of the underlying VMs. One way to significantly reduce cost is to use spare resources, which cloud providers rent at a massive discount. Harvest VMs offer such cheap resources: they grow and shrink to harvest all the unallocated CPU cores in their host servers, but may be evicted to make room for more expensive VMs. Thus, using Harvest VMs to run the serverless platform comes with two main challenges that must be carefully managed: VM evictions and dynamically varying resources in each VM. In this work, we explore the challenges and benefits of hosting serverless (Function as a Service or simply FaaS) platforms on Harvest VMs. We characterize the serverless workloads and Harvest VMs of Microsoft Azure, and design a serverless load balancer that is aware of evictions and resource variations in Harvest VMs. We modify OpenWhisk, a widely-used open-source serverless platform, to monitor harvested resources and balance the load accordingly, and evaluate it experimentally. Our results show that adopting harvested resources improves efficiency and reduces cost. Under the same cost budget, running serverless platforms on harvested resources achieves 2.2x to 9.0x higher throughput compared to using dedicated resources. When using the same amount of resources, running serverless platforms on harvested resources achieves 48% to 89% cost savings with lower latency due to better load balancing. © 2021 ACM.","harvested resources; Serverless computing","Budget control; Harvesting; Open source software; Virtual machine; Windows operating system; Cheap resources; Cloud providers; CPU cores; Dynamically varying resource; Fine grained; Harvested resource; Host servers; MicroSoft; Reduce costs; Serverless computing; Cost reduction","Association for Computing Machinery, Inc","ACM SIGOPS in cooperation with USENIX","28th ACM Symposium on Operating Systems Principles, SOSP 2021","26 October 2021 through 29 October 2021","Virtual, Online","173064","Conference paper","Final","","Scopus","2-s2.0-85119096405"
"Chahal D.; Palepu S.; Mishra M.; Singhal R.","Chahal, Dheeraj (57147492400); Palepu, Surya (57223141130); Mishra, Mayank (57205914416); Singhal, Rekha (36069730400)","57147492400; 57223141130; 57205914416; 36069730400","SLA-Aware Workload Scheduling Using Hybrid Cloud Services","2021","HiPS 2021 - Proceedings of the 1st Workshop on High Performance Serverless Computing, co-located with HPDC 2021","","","","1","4","3","8","10.1145/3452413.3464789","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110570400&doi=10.1145%2f3452413.3464789&partnerID=40&md5=097797f9d8b5e3315ee7deb3b19e9085","Cloud services have an auto-scaling feature for load balancing to meet the performance requirements of an application. Existing auto-scaling techniques are based on upscaling and downscaling cloud resources to distribute the dynamically varying workloads. However, bursty workloads pose many challenges for auto-scaling and sometimes result in Service Level Agreement (SLA) violations. Furthermore, over-provisioning or under-provisioning cloud resources to address dynamically evolving workloads results in performance degradation and cost escalation. In this work, we present a workload characterization based approach for scheduling the bursty workload on a highly scalable serverless architecture in conjunction with a machine learning (ML) platform. We present the use of Amazon Web Services (AWS) ML platform SageMaker and serverless computing platform Lambda for load balancing the inference workload to avoid SLA violations. We evaluate our approach using a recommender system that is based on a deep learning model for inference.  © 2020 ACM.","aws lambda, serverless architecture; cloud; load balancing; ml platform; sagemaker; sla-Aware","Balancing; Deep learning; Learning systems; Scheduling; Aws lambda, serverless architecture; Cloud services; Lambda's; Load-Balancing; Ml platform; Sagemaker; Scalings; Serverless architecture; Servicelevel agreement (SLA); Sla-aware; Web services","Association for Computing Machinery, Inc","ACM SIGARCH; University of Arizona","1st Workshop on High Performance Serverless Computing, HiPS 2021 - Co-located with HPDC 2021","25 June 2021","Virtual, Online","169886","Conference paper","Final","","Scopus","2-s2.0-85110570400"
"Zhao K.; Gong S.; Fonseca P.","Zhao, Kaiyang (58551726700); Gong, Sishuai (57223248797); Fonseca, Pedro (56343089800)","58551726700; 57223248797; 56343089800","On-demand-fork: A microsecond fork for memory-intensive and latency-sensitive applications","2021","EuroSys 2021 - Proceedings of the 16th European Conference on Computer Systems","","","3456258","540","555","15","27","10.1145/3447786.3456258","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105250256&doi=10.1145%2f3447786.3456258&partnerID=40&md5=972fdc788e67620c7d7c01d8dce0bbe4","Fork has long been the process creation system call for Unix. At its inception, fork was hailed as an efficient system call due to its use of copy-on-write on memory shared between parent and child processes. However, application memory demand has increased drastically since the early days and the cost incurred by fork to simply set up virtual memory (e.g., copy page tables) is now a concern, even for applications that only require hundreds of MBs of memory. In practice, fork performance already holds back system efficiency and latency across a range of uses cases that fork large processes, such as fault-tolerant systems, serverless frameworks, and testing frameworks. This paper proposes On-demand-fork, a fast implementation of the fork system call specifically designed for applications with large memory footprints. On-demand-fork relies on the observation that copy-on-write can be generalized to page tables, even on commodity hardware. On-demand-fork executes faster than the traditional fork implementation by additionally sharing page tables between parent and child at fork time and selectively copying page tables in small chunks, on-demand, when handling page faults. On-demand-fork is a drop-in replacement for fork that requires no changes to applications or hardware. We evaluated On-demand-fork on a range of micro-benchmarks and real-world workloads. On-demand-fork significantly reduces the fork invocation time and has improved scalability. For processes with 1 GB of allocated memory, On-demand-fork has a 65× performance advantage over Fork. We also evaluated On-demand-fork on testing, fuzzing, and snapshotting workloads of well-known applications, obtaining execution throughput improvements between 59% and 226% and up to 99% invocation latency reduction. © 2021 Owner/Author.","","Fault tolerant computer systems; Well testing; Commodity hardware; Fast implementation; Fault tolerant systems; Latency reduction; Sensitive application; System efficiency; Testing framework; Throughput improvement; Computer hardware","Association for Computing Machinery, Inc","","16th European Conference on Computer Systems, EuroSys 2021","26 April 2021 through 28 April 2021","Virtual, Online","168531","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85105250256"
"Eismann S.; Bui L.; Grohmann J.; Abad C.; Herbst N.; Kounev S.","Eismann, Simon (57203095294); Bui, Long (57226514837); Grohmann, Johannes (57197744369); Abad, Cristina (22957218500); Herbst, Nikolas (55747080400); Kounev, Samuel (23397538000)","57203095294; 57226514837; 57197744369; 22957218500; 55747080400; 23397538000","Sizeless: Predicting the Optimal Size of Serverless Functions","2021","Middleware 2021 - Proceedings of the 22nd International Middleware Conference","","","","248","259","11","58","10.1145/3464298.3493398","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146890584&doi=10.1145%2f3464298.3493398&partnerID=40&md5=3537182228bf5be09e422280f35e136a","Serverless functions are an emerging cloud computing paradigm that is being rapidly adopted by both industry and academia. In this cloud computing model, the provider opaquely handles resource management tasks such as resource provisioning, deployment, and auto-scaling. The only resource management task that developers are still in charge of is selecting how much resources are allocated to each worker instance. However, selecting the optimal size of serverless functions is quite challenging, so developers often neglect it despite its significant cost and performance benefits. Existing approaches aiming to automate serverless functions resource sizing require dedicated performance tests, which are time-consuming to implement and maintain. In this paper, we introduce an approach to predict the optimal resource size of a serverless function using monitoring data from a single resource size. As our approach does not require dedicated performance tests, it enables cloud providers to implement resource sizing on a platform level and automate the last resource management task associated with serverless functions. We evaluate our approach on four different serverless applications on AWS, where it predicts the execution time of the other memory sizes based on monitoring data for a single memory size with an average prediction error of 15.3%. Based on these predictions, it selects the optimal memory size for 79.0% of the serverless functions and the second-best memory size for 12.3% of the serverless functions, which results in an average speedup of 39.7% while also decreasing average costs by 2.6%. © 2021 Copyright held by the owner/author(s)","","Forecasting; Human resource management; Natural resources management; Resource allocation; Cloud-computing; Computing model; Computing paradigm; Management tasks; Memory size; Optimal size; Performance tests; Resource management; Scalings; Workers'; Cloud computing","Association for Computing Machinery, Inc","ACM","22nd International Middleware Conference, Middleware 2021","6 December 2021 through 10 December 2021","Virtual, Online","172202","Conference paper","Final","","Scopus","2-s2.0-85146890584"
"","","","WOSC 2019 - Proceedings of the 2019 5th International Workshop on Serverless Computing, Part of Middleware 2019","2019","WOSC 2019 - Proceedings of the 2019 5th International Workshop on Serverless Computing, Part of Middleware 2019","","","","","","54","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084163658&partnerID=40&md5=7b3e48f547adeddd32778d87bf333c72","[No abstract available]","","","Association for Computing Machinery, Inc","ACM","5th International Workshop on Serverless Computing, WOSC 2019 - Part of Middleware 2019","9 December 2019 through 13 December 2019","Davis","156757","Conference review","Final","","Scopus","2-s2.0-85084163658"
"Bhasi V.M.; Gunasekaran J.R.; Thinakaran P.; Mishra C.S.; Kandemir M.T.; Das C.","Bhasi, Vivek M. (57343857600); Gunasekaran, Jashwant Raj (55440506600); Thinakaran, Prashanth (55428420900); Mishra, Cyan Subhra (57216649340); Kandemir, Mahmut Taylan (35549787100); Das, Chita (7201851990)","57343857600; 55440506600; 55428420900; 57216649340; 35549787100; 7201851990","Kraken: Adaptive container provisioning for deploying dynamic DAGs in serverless platforms","2021","SoCC 2021 - Proceedings of the 2021 ACM Symposium on Cloud Computing","","","","153","167","14","65","10.1145/3472883.3486992","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119277087&doi=10.1145%2f3472883.3486992&partnerID=40&md5=bbfc63b63297c6b2fee0cd9d23859de5","The growing popularity of microservices has led to the proliferation of online cloud service-based applications, which are typically modelled as Directed Acyclic Graphs (DAGs) comprising of tens to hundreds of microservices. The vast majority of these applications are user-facing, and hence, have stringent SLO requirements. Serverless functions, having short resource provisioning times and instant scalability, are suitable candidates for developing such latency-critical applications. However, existing serverless providers are unaware of the workflow characteristics of application DAGs, leading to container over-provisioning in many cases. This is further exacerbated in the case of dynamic DAGs, where the function chain for an application is not known a priori. Motivated by these observations, we propose Kraken, a workflow-aware resource management framework that minimizes the number of containers provisioned for an application DAG while ensuring SLO-compliance. We design and implement Kraken on OpenFaaS and evaluate it on a multi-node Kubernetes-managed cluster. Our extensive experimental evaluation using DeathStarbench workload suite and real-world traces demonstrates that Kraken spawns up to 76% fewer containers, thereby improving container utilization and saving cluster-wide energy by up to 4x and 48%, respectively, when compared to state-of-the art schedulers employed in serverless platforms.  © 2021 Association for Computing Machinery.","Queuing; Resource-management; Scheduling; Serverless","Containers; Directed graphs; Function evaluation; Natural resources management; Scheduling; Cloud services; Critical applications; Design and implements; Over provisioning; Resource management; Resource management framework; Serverless; Service-based applications; Stringents; Work-flows; Resource allocation","Association for Computing Machinery, Inc","ACM Special Interest Groups on Management of Data (ACM SIGMOD); ACM Special Interest Groups on Operating Systems (ACM SIGOPS); et al.; Exotanium; Microsoft; Salesforce","12th Annual ACM Symposium on Cloud Computing, SoCC 2021","1 November 2021 through 4 November 2021","Virtual, Online","173200","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85119277087"
"Larrea J.; Barbalace A.","Larrea, Jon (57217164574); Barbalace, Antonio (23491344500)","57217164574; 23491344500","The serverkernel operating system","2020","EdgeSys 2020 - Proceedings of the 3rd ACM International Workshop on Edge Systems, Analytics and Networking, Part of EuroSys 2020","","","","13","18","5","2","10.1145/3378679.3394537","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086566838&doi=10.1145%2f3378679.3394537&partnerID=40&md5=85ec1ce95e93571b4f253dc701476407","With the idea of exploiting all the computational resources that an IoT environment with multiple interconnected devices offers, serverkernel is presented as a new operating system architecture that blends ideas from distributed operating systems, Unikernel, and LWK. These concepts are mixed with a server in which a user can remotely offload computations and get the result. This single space-address operating system (OS) can be interpreted as a bare-metal OS in which only drivers for CPU, network, and accelerators are required in order to provide service. To demonstrate the advantages of serverkernel, jonOS, an open-source C implementation of this architecture for Raspberry Pi, is provided. Compared with commercial architectures used in IoT devices, serverkernel achieves an improvement ratio of 1.5 in CPU time, 2.5 in real-time, and around 9 times better in network speed. © 2020 ACM.","FaaS; IoT; Operating systems; Serverkernel; Unikernel","Internet of things; Bare metals; Computational resources; Distributed operating systems; Improvement ratio; In networks; Open sources; Operating system architecture; Real time; Network architecture","Association for Computing Machinery","ACM SIGOPS","3rd ACM International Workshop on Edge Systems, Analytics and Networking, in conjunction with ACM EuroSys 2020","27 April 2020","Heraklion","160506","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85086566838"
"Luthra M.; Hennig S.; Agnihotri P.; Wang L.; Koldehofe B.","Luthra, Manisha (57191987623); Hennig, Sebastian (57206470188); Agnihotri, Pratyush (57211551391); Wang, Lin (57188626214); Koldehofe, Boris (6602572685)","57191987623; 57206470188; 57211551391; 57188626214; 6602572685","Highly flexible server agnostic complex event processing operators","2019","Middleware Demos and Posters 2019 - Proceedings of the 2019 20th International Middleware Conference Demos and Posters, Part of Middleware 2019","","","","11","12","1","1","10.1145/3366627.3368110","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078901115&doi=10.1145%2f3366627.3368110&partnerID=40&md5=4093f1d2ff3e8e173448504fdb90ed67","Complex Event Processing (CEP) is a powerful paradigm that can derive correlations from different data sources for a wide variety of applications. CEP provides semantic units called operators e.g., filter and join, that collectively represent a complex event. In current CEP systems, operators are highly dependent on the programming language and the underlying server. This restricts the capability of provisioning user-defined operators at runtime as well as the flexibility of developing server agnostic custom operators. In this paper, we provide a serverless CEP architecture, which offers developers the flexibility to design operators in any language and integrate them at runtime. We embed operators in the function as a service model of serverless architecture. This is very beneficial for applications such as financial fraud detection where complex machine learning operators must be integrated at runtime to avoid service disruption. We show using our preliminary evaluation that only with minimal overhead in latency, we can offer highly flexible server agnostic CEP operators. © 2019 Copyright held by the owner/author(s).","Complex Event Processing; Internet of Things; Serverless","Internet of things; Semantics; Complex event processing; Complex event processing (CEP); Complex machines; Design operators; Financial fraud detections; Serverless; Serverless architecture; Service disruptions; Middleware","Association for Computing Machinery, Inc","ACM","20th International Middleware Conference Demos and Posters, Middleware 2019","9 December 2019 through 13 December 2019","Davis","156762","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85078901115"
"Mahmoudi N.; Khazaei H.","Mahmoudi, Nima (57203428799); Khazaei, Hamzeh (36995959600)","57203428799; 36995959600","Temporal Performance Modelling of Serverless Computing Platforms","2020","WOSC 2020 - Proceedings of the 2020 6th International Workshop on Serverless Computing, Part of Middleware 2020","","","","1","6","5","16","10.1145/3429880.3430092","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099565736&doi=10.1145%2f3429880.3430092&partnerID=40&md5=860846daf831da88a431e2f935f4e095","Analytical performance models have been shown very efficient in analyzing, predicting, and improving the performance of distributed computing systems. However, there is a lack of rigorous analytical models for analyzing the transient behaviour of serverless computing platforms, which is expected to be the dominant computing paradigm in cloud computing. Also, due to its unique characteristics and policies, performance models developed for other systems cannot be directly applied to modelling these systems. In this work, we propose an analytical performance model that is capable of predicting several key performance metrics for serverless workloads using only their average response time for warm and cold requests. The introduced model uses realistic assumptions, which makes it suitable for online analysis of real-world platforms. We validate the proposed model through extensive experimentation on AWS Lambda. Although we focus primarily on AWS Lambda due to its wide adoption in our experimentation, the proposed model can be leveraged for other public serverless computing platforms with similar auto-scaling policies, e.g., Google Cloud Functions, IBM Cloud Functions, and Azure Functions.  © 2020 ACM.","performance; performance modelling; serverless; serverless computing; temporal; transient","Analytical performance model; Computing platform; Distributed computing systems; Lambda's; Performance; Performance Modeling; Serverless; Serverless computing; Temporal; Temporal performance; Analytical models","Association for Computing Machinery, Inc","ACM","6th International Workshop on Serverless Computing, WOSC 2020 - Part of Middleware 2020","7 December 2020 through 11 December 2020","Virtual, Online","166193","Conference paper","Final","","Scopus","2-s2.0-85099565736"
"Russo G.R.; Schiazza A.; Cardellini V.","Russo, Gabriele Russo (57211560923); Schiazza, Antonio (57223124721); Cardellini, Valeria (6603463586)","57211560923; 57223124721; 6603463586","Elastic pulsar functions for distributed stream processing","2021","ICPE 2021 - Companion of the ACM/SPEC International Conference on Performance Engineering","","","","9","16","7","1","10.1145/3447545.3451901","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104953758&doi=10.1145%2f3447545.3451901&partnerID=40&md5=f571a27672dfda7600d17775bb1ca9c4","An increasing number of data-driven applications rely on the ability of processing data flows in a timely manner, exploiting for this purpose Data Stream Processing (DSP) systems. Elasticity is an essential feature for DSP systems, as workload variability calls for automatic scaling of the application processing capacity, to avoid both overload and resource wastage. In this work, we implement auto-scaling in Pulsar Functions, a function-based streaming framework built on top of Apache Pulsar. The latter is is a distributed publish-subscribe messaging platform that natively supports serverless functions. Considering various state-of-the-art policies, we show that the proposed solution is able to scale application parallelism with minimal overhead. © 2021 Association for Computing Machinery.","Auto-Scaling; Data Stream Processing; Self-Adaptation","Distributed parameter control systems; Pulsars; Automatic scaling; Data stream processing; Distributed stream processing; Essential features; Number of datum; Processing capacities; Publish-subscribe; State of the art; Data streams","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC Research","2021 ACM/SPEC International Conference on Performance Engineering, ICPE 2021","19 April 2021 through 21 April 2021","Virtual, Online","168447","Conference paper","Final","","Scopus","2-s2.0-85104953758"
"Nguyen H.D.; Yang Z.; Chien A.A.","Nguyen, Hai Duc (57214666053); Yang, Zhifei (57226169313); Chien, Andrew A. (7004570702)","57214666053; 57226169313; 7004570702","Motivating High Performance Serverless Workloads","2021","HiPS 2021 - Proceedings of the 1st Workshop on High Performance Serverless Computing, co-located with HPDC 2021","","","","25","32","7","6","10.1145/3452413.3464786","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110710905&doi=10.1145%2f3452413.3464786&partnerID=40&md5=780fc73a11e702ca9c37cd29bbbfd534","The historical motivation for serverless comes from internet-of-Things, smartphone client server, and the objective of simplifying programming (no provisioning) and scale-down (pay-for-use). These applications are generally low-performance best-effort. However, the serverless model enables flexible software architectures suitable for a wide range of applications that demand high-performance and guaranteed performance. We have studied three such applications-scientific data streaming, virtual/augmented reality, and document annotation. We describe how each can be cast in a serverless software architecture and how the application performance requirements translate into high performance requirements (invocation rate, low and predictable latency) for the underlying serverless system implementation. These applications can require invocations rates as high as millions per second (40 MHz) and latency deadlines below a microsecond (300 ns), and furthermore require performance predictability. All of these capabilities are far in excess of today's commercial serverless offerings and represent interesting research challenges.  © 2020 ACM.","document annotation; high performance computing; serverless; stream processing; virtual reality","Motivation; Application performance; Client server; Document annotation; Guaranteed performance; Performance requirements; Research challenges; Scientific data; Serverless systems; Application programs","Association for Computing Machinery, Inc","ACM SIGARCH; University of Arizona","1st Workshop on High Performance Serverless Computing, HiPS 2021 - Co-located with HPDC 2021","25 June 2021","Virtual, Online","169886","Conference paper","Final","","Scopus","2-s2.0-85110710905"
"Werner S.; Girke R.; Kuhlenkamp J.","Werner, Sebastian (56354164400); Girke, Richard (57221596224); Kuhlenkamp, Jörn (55448117200)","56354164400; 57221596224; 55448117200","An Evaluation of Serverless Data Processing Frameworks","2020","WOSC 2020 - Proceedings of the 2020 6th International Workshop on Serverless Computing, Part of Middleware 2020","","","","19","24","5","7","10.1145/3429880.3430095","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099529671&doi=10.1145%2f3429880.3430095&partnerID=40&md5=a85d8fcfd16bba67a4ec1b226182e1b8","Serverless computing is a promising cloud execution model that significantly simplifies cloud users' operational concerns by offering features such as auto-scaling and a pay-as-you-go cost model. Consequently, serverless systems promise to provide an excellent fit for ad-hoc data processing. Unsurprisingly, numerous serverless systems/frameworks for data processing emerged recently from research and industry. However, systems researchers, decision-makers, and data analysts are unaware of how these serverless systems compare to each other. In this paper, we identify existing serverless frameworks for data processing. We present a qualitative assessment of different system architectures and an experiment-driven quantitative comparison, including performance, cost, and usability using the TPC-H benchmark. Our results show that the three publicly available serverless data processing frameworks outperform a comparatively sized Apache Spark cluster in terms of performance and cost for ad-hoc queries on cold data.  © 2020 ACM.","Benchmarking; Data Processing; Serverless","Data handling; Decision making; Industrial research; Ad hoc datum; Cost models; Data analysts; Decision data; Decision makers; Pay as you go; Scalings; Serverless; Serverless systems; System framework; Benchmarking","Association for Computing Machinery, Inc","ACM","6th International Workshop on Serverless Computing, WOSC 2020 - Part of Middleware 2020","7 December 2020 through 11 December 2020","Virtual, Online","166193","Conference paper","Final","","Scopus","2-s2.0-85099529671"
"Mirabelli M.E.; García-López P.; Vernik G.","Mirabelli, Mariano Ezequiel (57221608046); García-López, Pedro (24479469800); Vernik, Gil (27068121500)","57221608046; 24479469800; 27068121500","Bringing scaling transparency to Proteomics applications with serverless computing","2020","WOSC 2020 - Proceedings of the 2020 6th International Workshop on Serverless Computing, Part of Middleware 2020","","","","55","60","5","4","10.1145/3429880.3430101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099557165&doi=10.1145%2f3429880.3430101&partnerID=40&md5=ef871c164cf4821636d078eee2a52a23","Scaling transparency means that applications can expand in scale without changes to the system structure or the application algorithms. Serverless Computing's inherent auto-scaling support and fast function launching is ideally suited to support scaling transparency in different domains. In particular, Proteomic applications could considerably benefit from scaling transparency and serverless technologies due to their high concurrency requirements. Therefore, the auto-provisioning nature of serverless platforms makes this computing model an alternative to satisfy dynamically the resources required by protein folding simulation processes. However, the transition to these architectures must face challenges: they should show comparable performance and cost to code running in Virtual Machines (VMs). In this article, we demonstrate that Proteomics applications implemented with the Replica Exchange algorithm can be moved to serverless settings guaranteeing scaling transparency. We also validate that we can reduce the total execution time by around forty percent with comparable cost to cluster technologies (Work Queue) over VMs.  © 2020 ACM.","Replica Exchange; Scaling Transparency; Serverless","Molecular biology; Simulation platform; Computing model; Different domains; High concurrencies; Protein folding simulation; Proteomics; Replica exchange; Scaling transparency; Scalings; Serverless; Systems Structure; Transparency","Association for Computing Machinery, Inc","ACM","6th International Workshop on Serverless Computing, WOSC 2020 - Part of Middleware 2020","7 December 2020 through 11 December 2020","Virtual, Online","166193","Conference paper","Final","","Scopus","2-s2.0-85099557165"
"Byrne A.; Nadgowda S.; Coskun A.K.","Byrne, Anthony (57193609032); Nadgowda, Shripad (24475135200); Coskun, Ayse K. (15060094700)","57193609032; 24475135200; 15060094700","ACE: Just-in-time Serverless Software Component Discovery through Approximate Concrete Execution","2020","WOSC 2020 - Proceedings of the 2020 6th International Workshop on Serverless Computing, Part of Middleware 2020","","","","37","42","5","4","10.1145/3429880.3430098","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099566388&doi=10.1145%2f3429880.3430098&partnerID=40&md5=ba2a681b3bbd3228588246530fea6c8b","While much of the software running on today's serverless platforms is written in easily-analyzed high-level interpreted languages, many performance-conscious users choose to deploy their applications as container-encapsulated compiled binaries on serverless container platforms such as AWS Fargate or Google Cloud Run. Modern CI/CD workflows make this deployment process nearly-instantaneous, leaving little time for in-depth manual application security reviews. This combination of opaque binaries and rapid deployment prevents cloud developers and platform operators from knowing if their applications contain outdated, vulnerable, or legally-compromised code. This paper proposes Approximate Concrete Execution (ACE), a just-in-time binary analysis technique that enables automatic software component discovery for serverless binaries. Through classification and search engine experiments with common cloud software packages, we find that ACE scans binaries 5.2x faster than a state-of-the-art binary analysis tool, minimizing the impact on deployment and cold-start latency while maintaining comparable recall.  © 2020 ACM.","serverless computing; software component discovery; software integrity","Application programs; Concretes; Containers; High level languages; Just in time production; Binary analysis; Deployment process; Interpreted languages; Just-in-time; Performance; Serverless computing; Software component discovery; Software integrity; Software-component; Work-flows; Search engines","Association for Computing Machinery, Inc","ACM","6th International Workshop on Serverless Computing, WOSC 2020 - Part of Middleware 2020","7 December 2020 through 11 December 2020","Virtual, Online","166193","Conference paper","Final","","Scopus","2-s2.0-85099566388"
"Daw N.; Bellur U.; Kulkarni P.","Daw, Nilanjan (57211429693); Bellur, Umesh (15063937300); Kulkarni, Purushottam (7201958713)","57211429693; 15063937300; 7201958713","Xanadu: Mitigating cascading cold starts in serverless function chain deployments","2020","Middleware 2020 - Proceedings of the 2020 21st International Middleware Conference","","","","356","370","14","82","10.1145/3423211.3425690","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098485885&doi=10.1145%2f3423211.3425690&partnerID=40&md5=8e4776490ec08205556e8edaac0873a7","Organization of tasks as workflows are an essential feature to expand the applicability of the serverless computing framework. Existing serverless platforms are either agnostic to function chains (workflows as a composition of functions) or rely on naive provisioning and management mechanisms of the serverless framework-an example is that they provision resources after the trigger to each function in a workflow arrives thereby forcing a setup latency for each function in the workflow. In this work, we focus on mitigating the cascading cold start problem- the latency overheads in triggering a sequence of serverless functions according to a workflow specification. We first establish the nature and extent of the cascading effects in cold start situations across multiple commercial server platforms and cloud providers. Towards mitigating these cascading overheads, we design and develop several optimizations, that are built into our tool Xanadu. Xanadu offers multiple instantiation options based on the desired runtime isolation requirements and supports function chaining with or without explicit workflow specifications. Xanadu's optimizations to address the cascading cold start problem are built on speculative and just-in-time provisioning of resources. Our evaluation of the Xanadu system reveals almost complete elimination of cascading cold starts at minimal cost overheads, outperforming the available state of the art platforms. For even relatively short workflows, Xanadu reduces platform overheads by almost 18x compared to Knative and 10x compared to Apache Openwhisk. © 2020 Association for Computing Machinery.","Just-in-time scheduling; Serverless workflows; Speculative deployment","Specifications; Cascading effects; Cold start problems; Computing frameworks; Essential features; Management mechanisms; Server platform; State of the art; Workflow specification; Middleware","Association for Computing Machinery, Inc","Association for Computing Machinery (ACM); IFIP; TU Delft; USENIX","21st International Middleware Conference, Middleware 2020","7 December 2020 through 11 December 2020","Virtual, Online","165666","Conference paper","Final","","Scopus","2-s2.0-85098485885"
"Skluzacek T.J.; Wong R.; Li Z.; Chard R.; Chard K.; Foster I.","Skluzacek, Tyler J. (57193628652); Wong, Ryan (57215290441); Li, Zhuozhao (56275944000); Chard, Ryan (55588066400); Chard, Kyle (9132950200); Foster, Ian (35572232000)","57193628652; 57215290441; 56275944000; 55588066400; 9132950200; 35572232000","A Serverless Framework for Distributed Bulk Metadata Extraction","2021","HPDC 2021 - Proceedings of the 30th International Symposium on High-Performance Parallel and Distributed Computing","","","","7","18","11","13","10.1145/3431379.3460636","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109556210&doi=10.1145%2f3431379.3460636&partnerID=40&md5=d0f9313b5ab9f9a1cb7615c6af143785","We introduce Xtract, an automated and scalable system for bulk metadata extraction from large, distributed research data repositories. Xtract orchestrates the application of metadata extractors to groups of files, determining which extractors to apply to each file and, for each extractor and file, where to execute. A hybrid computing model, built on the funcX federated FaaS platform, enables Xtract to balance tradeoffs between extraction time and data transfer costs by dispatching each extraction task to the most appropriate location. Experiments on a range of clouds and supercomputers show that Xtract can efficiently process multi-million-file repositories by orchestrating the concurrent execution of container-based extractors on thousands of nodes. We highlight the flexibility of Xtract by applying it to a large, semi-curated scientific data repository and to an uncurated scientific Google Drive repository. We show that by remotely orchestrating metadata extraction across decentralized storage and compute nodes, Xtract can process large repositories in 50% of the time it takes just to transfer the same data to a machine within the same computing facility. We also show that when transferring data is necessary (e.g., no local compute is available), Xtract can scale to process files as fast as they are received, even over a multi-GB/s network.  © 2020 ACM.","files; metadata extraction; search index; serverless; storage","Data transfer; Digital storage; Extraction; Metadata; Supercomputers; Turing machines; Computing facilities; Concurrent execution; Extraction time; Hybrid computing; Meta-data extractions; Research data; Scalable systems; Scientific data; Data mining","Association for Computing Machinery, Inc","ACM SIGARCH; University of Arizona","30th International Symposium on High-Performance Parallel and Distributed Computing, HPDC 2021","21 June 2021 through 25 June 2021","Virtual, Online","169760","Conference paper","Final","","Scopus","2-s2.0-85109556210"
"Wang B.; Ali-Eldin A.; Shenoy P.","Wang, Bin (57221064323); Ali-Eldin, Ahmed (37041380200); Shenoy, Prashant (7005159885)","57221064323; 37041380200; 7005159885","LaSS: Running Latency Sensitive Serverless Computations at the Edge","2021","HPDC 2021 - Proceedings of the 30th International Symposium on High-Performance Parallel and Distributed Computing","","","","239","251","12","56","10.1145/3431379.3460646","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109513207&doi=10.1145%2f3431379.3460646&partnerID=40&md5=5e3ba32ee53e2f6e04496ec2f3df1928","Serverless computing has emerged as a new paradigm for running short-lived computations in the cloud. Due to its ability to handle IoT workloads, there has been considerable interest in running serverless functions at the edge. However, the constrained nature of the edge and the latency sensitive nature of workloads result in many challenges for serverless platforms. In this paper, we present LaSS, a platform that uses model-driven approaches for running latency-sensitive serverless computations on edge resources. LaSS uses principled queuing-based methods to determine an appropriate allocation for each hosted function and auto-scales the allocated resources in response to workload dynamics. LaSS uses a fair-share allocation approach to guarantee a minimum of allocated resources to each function in the presence of overload. In addition, it utilizes resource reclamation methods based on container deflation and termination to reassign resources from over-provisioned functions to under-provisioned ones. We implement a prototype of our approach on an OpenWhisk serverless edge cluster and conduct a detailed experimental evaluation. Our results show that LaSS can accurately predict the resources needed for serverless functions in the presence of highly dynamic workloads, and reprovision container capacity within hundreds of milliseconds while maintaining fair share allocation guarantees.  © 2020 ACM.","cloud computing; edge computing; function-as-a-service (faas); queueing theory; serverless computing; service-level agreement (sla)","Containers; Lanthanum compounds; Allocation approach; Auto-scale; Container capacity; Edge resources; Experimental evaluation; Fair share; Model driven approach; Resource reclamation; Sulfur compounds","Association for Computing Machinery, Inc","ACM SIGARCH; University of Arizona","30th International Symposium on High-Performance Parallel and Distributed Computing, HPDC 2021","21 June 2021 through 25 June 2021","Virtual, Online","169760","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85109513207"
"Satzke K.; Akkus I.E.; Chen R.; Rimac I.; Stein M.; Beck A.; Aditya P.; Vanga M.; Hilt V.","Satzke, Klaus (57207309613); Akkus, Istemi Ekin (15062297200); Chen, Ruichuan (23388102000); Rimac, Ivica (55941252700); Stein, Manuel (57198645931); Beck, Andre (10639921200); Aditya, Paarijaat (55960724400); Vanga, Manohar (56719640500); Hilt, Volker (6602872942)","57207309613; 15062297200; 23388102000; 55941252700; 57198645931; 10639921200; 55960724400; 56719640500; 6602872942","Efficient GPU Sharing for Serverless Workflows","2021","HiPS 2021 - Proceedings of the 1st Workshop on High Performance Serverless Computing, co-located with HPDC 2021","","","","17","24","7","14","10.1145/3452413.3464785","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110778391&doi=10.1145%2f3452413.3464785&partnerID=40&md5=ebc36e2e335304ee6734892d57422ffb","Serverless computing has emerged as a new cloud computing paradigm, where an application consists of individual functions that can be separately managed and executed. However, the function development environment of all serverless computing frameworks at present is CPU-based. In this paper, we propose to extend the open-sourced KNIX high-performance serverless framework so that it can execute functions on shared GPU cluster resources. We have evaluated the performance impacts on the extended KNIX system by measuring overheads and penalties incurred using different deep learning frameworks.  © 2020 ACM.","deep learning; gpu; image processing; neural networks; serverless","Graphics processing unit; Computing frameworks; Development environment; Gpu clusters; Learning frameworks; Performance impact; Work-flows; Deep learning","Association for Computing Machinery, Inc","ACM SIGARCH; University of Arizona","1st Workshop on High Performance Serverless Computing, HiPS 2021 - Co-located with HPDC 2021","25 June 2021","Virtual, Online","169886","Conference paper","Final","","Scopus","2-s2.0-85110778391"
"Dukic V.; Bruno R.; Singla A.; Alonso G.","Dukic, Vojislav (57218219724); Bruno, Rodrigo (56146992200); Singla, Ankit (37007019000); Alonso, Gustavo (7102787521)","57218219724; 56146992200; 37007019000; 7102787521","Photons: Lambdas on a diet","2020","SoCC 2020 - Proceedings of the 2020 ACM Symposium on Cloud Computing","","","","45","59","14","57","10.1145/3419111.3421297","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095437945&doi=10.1145%2f3419111.3421297&partnerID=40&md5=6062e87db4fc3acf5a35d76939d29370","Serverless computing allows users to create short, stateless functions and invoke hundreds of them concurrently to tackle massively parallel workloads. We observe that even though most of the footprint of a serverless function is fixed across its invocations - - language runtime, libraries, and other application state - - today's serverless platforms do not exploit this redundancy. Such an inefficiency has cascading negative impacts: longer startup times, lower throughput, higher latency, and higher cost. To mitigate these problems, we have built Photons, a framework leveraging workload parallelism to co-locate multiple instances of the same function within the same runtime. Concurrent invocations can then share the runtime and application state transparently, without compromising execution safety. Photons reduce function's memory consumption by 25% to 98% per invocation, with no performance degradation compared to today's serverless platforms. We also show that our approach can reduce the overall memory utilization by 30%, and the total number of cold starts by 52%.  © 2020 ACM.","serverless computing; shared runtime; workload collocation","Cloud computing; Fixed platforms; Cold start; Massively parallels; Memory consumption; Memory utilization; Multiple instances; Performance degradation; Runtimes; Photons","Association for Computing Machinery, Inc","ACM SIGMOD; ACM SIGOPS; Alibaba Group; aws; et al.; Microsoft","11th ACM Symposium on Cloud Computing, SoCC 2020","19 October 2020 through 21 October 2020","Virtual, Online","163871","Conference paper","Final","","Scopus","2-s2.0-85095437945"
"Van Eyk E.; Scheuner J.; Eismann S.; Abad C.L.; Iosup A.","Van Eyk, Erwin (57203098031); Scheuner, Joel (56737196000); Eismann, Simon (57203095294); Abad, Cristina L. (22957218500); Iosup, Alexandru (23392350500)","57203098031; 56737196000; 57203095294; 22957218500; 23392350500","Beyond microbenchmarks: The SPEC-RG vision for a comprehensive serverless benchmark","2020","ICPE 2020 - Companion of the ACM/SPEC International Conference on Performance Engineering","","","","26","31","5","16","10.1145/3375555.3384381","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086024894&doi=10.1145%2f3375555.3384381&partnerID=40&md5=724e142e4e756a5e63a0176384e2f48a","Serverless computing services, such as Function-as-a-Service (FaaS), hold the attractive promise of a high level of abstraction and high performance, combined with the minimization of operational logic. Several large ecosystems of serverless platforms, both open- and closed-source, aim to realize this promise. Consequently, a lucrative market has emerged. However, the performance trade-offs of these systems are not well-understood. Moreover, it is exactly the high level of abstraction and the opaqueness of the operational-side that make performance evaluation studies of serverless platforms challenging. Learning from the history of IT platforms, we argue that a benchmark for serverless platforms could help address this challenge. We envision a comprehensive serverless benchmark, which we contrast to the narrow focus of prior work in this area. We argue that a comprehensive benchmark will need to take into account more than just runtime overhead, and include notions of cost, realistic workloads, more (open-source) platforms, and cloud integrations. Finally, we show through preliminary real-world experiments how such a benchmark can help compare the performance overhead when running a serverless workload on state-of-the-art platforms. © 2020 ACM.","function-as-a-service; performance; serverless computing","Abstracting; Commerce; Computation theory; Economic and social effects; Cloud integrations; Computing services; Evaluation study; High level of abstraction; Micro-benchmarks; Performance trade-off; Real world experiment; Runtime overheads; Benchmarking","Association for Computing Machinery, Inc","ACM Special Interest Group on Measurement and Evaluation (SIGMETRICS); ACM Special Interest Group on Software Engineering (SIGSOFT)","11th ACM/SPEC International Conference on Performance Engineering, ICPE 2020","20 April 2020","Edmonton","160111","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85086024894"
"Chard R.; Babuji Y.; Li Z.; Skluzacek T.; Woodard A.; Blaiszik B.; Foster I.; Chard K.","Chard, Ryan (55588066400); Babuji, Yadu (57193607019); Li, Zhuozhao (56275944000); Skluzacek, Tyler (57193628652); Woodard, Anna (57208845223); Blaiszik, Ben (15043808100); Foster, Ian (35572232000); Chard, Kyle (9132950200)","55588066400; 57193607019; 56275944000; 57193628652; 57208845223; 15043808100; 35572232000; 9132950200","FuncX: A Federated Function Serving Fabric for Science","2020","HPDC 2020 - Proceedings of the 29th International Symposium on High-Performance Parallel and Distributed Computing","","","","65","76","11","148","10.1145/3369583.3392683","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088401102&doi=10.1145%2f3369583.3392683&partnerID=40&md5=a6a794b2fbec03de0790c1a595e5a772","Exploding data volumes and velocities, new computational methods and platforms, and ubiquitous connectivity demand new approaches to computation in the sciences. These new approaches must enable computation to be mobile, so that, for example, it can occur near data, be triggered by events (e.g., arrival of new data), be offloaded to specialized accelerators, or run remotely where resources are available. They also require new design approaches in which monolithic applications can be decomposed into smaller components, that may in turn be executed separately and on the most suitable resources. To address these needs we present funcX - -a distributed function as a service (FaaS) platform that enables flexible, scalable, and high performance remote function execution. funcX's endpoint software can transform existing clouds, clusters, and supercomputers into function serving systems, while funcX's cloud-hosted service provides transparent, secure, and reliable function execution across a federated ecosystem of endpoints. We motivate the need for funcX with several scientific case studies, present our prototype design and implementation, show optimizations that deliver throughput in excess of 1 million functions per second, and demonstrate, via experiments on two supercomputers, that funcX can scale to more than more than 130 000 concurrent workers. © 2020 ACM.","federated function serving; function as a service; funcX","Supercomputers; Case-studies; Data volume; Design approaches; Distributed function; New approaches; Prototype designs; Reliable function; Platform as a Service (PaaS)","Association for Computing Machinery, Inc","ACM SIGARCH; Software Research Center (CASTOR); University of Arizona","29th International Symposium on High-Performance Parallel and Distributed Computing, HPDC 2020","23 June 2020 through 26 June 2020","Stockholm","161375","Conference paper","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85088401102"
"Kesidis G.","Kesidis, George (7003540724)","7003540724","Overbooking lambda functions in the cloud","2019","WOC 2019 - Proceedings of the 2019 5th International Workshop on Container Technologies and Container Clouds, Part of Middleware 2019","","","","1","6","5","1","10.1145/3366615.3368351","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078944427&doi=10.1145%2f3366615.3368351&partnerID=40&md5=7e8f71efed24d471b6204821a27b3adb","We consider the problem of scheduling serverless-computing instances such as Amazon Lambda functions. Instead of a quota per tenant/customer, we assume demand for Lambda functions is modulated by token-bucket mechanisms per tenant. Such quotas are due to, e.g., limited resources (as in a fog/edge-cloud context) or to prevent excessive unauthorized invocation of numerous instances by malware. Based on an upper bound on the stationary number of active “Lambda servers"" considering the execution-time distribution of Lambda functions, we describe an approach that the cloud could use to overbook Lambda functions for improved utilization of IT resources. An earlier bound for a single service tier is extended to multiple service tiers. © 2019 Association for Computing Machinery.","","Middleware; IT resources; Multiple services; Time distribution; Token bucket; Upper Bound; Containers","Association for Computing Machinery, Inc","ACM","5th International Workshop on Container Technologies and Container Clouds, WOC 2019 - Part of Middleware 2019","9 December 2019 through 13 December 2019","Davis","156754","Conference paper","Final","","Scopus","2-s2.0-85078944427"
"","","","HPDC 2021 - Proceedings of the 30th International Symposium on High-Performance Parallel and Distributed Computing","2021","HPDC 2021 - Proceedings of the 30th International Symposium on High-Performance Parallel and Distributed Computing","","","","","","270","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109489923&partnerID=40&md5=1563645ee5654b03f15d96167ccc5cd2","The proceedings contain 25 papers. The topics discussed include: a serverless framework for distributed bulk metadata extraction; file system semantics requirements of HPC applications; DStore: a fast, tailless, and quiescent-free object store for PMEM; adaptive configuration of in situ lossy compression for cosmology simulations via fine-grained rate-quality modeling; ARC: an automated approach to resiliency for lossy compressed data via error correcting codes; MPI-CorrBench: towards an MPI correctness benchmark suite; cache-aware sparse patterns for the factorized sparse approximate inverse preconditioner; and AITurbo: unified compute allocation for partial predictable training in commodity clusters.","","","Association for Computing Machinery, Inc","ACM SIGARCH; University of Arizona","30th International Symposium on High-Performance Parallel and Distributed Computing, HPDC 2021","21 June 2021 through 25 June 2021","Virtual, Online","169760","Conference review","Final","","Scopus","2-s2.0-85109489923"
"Kaviani N.; Kalinin D.; Maximilien M.","Kaviani, Nima (22034673400); Kalinin, Dmitriy (57214668056); Maximilien, Michael (6508192375)","22034673400; 57214668056; 6508192375","Towards serverless as commodity: A case of Knative","2019","WOSC 2019 - Proceedings of the 2019 5th International Workshop on Serverless Computing, Part of Middleware 2019","","","","13","18","5","17","10.1145/3366623.3368135","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078952724&doi=10.1145%2f3366623.3368135&partnerID=40&md5=646110fad1b668fef322752c42165706","Serverless computing promises to evolve cloud computing architecture from VMs and containers-as-a-service (CaaS) to function-as-a-service (FaaS). This takes away complexities of managing and scaling underlying infrastructure and can result in simpler code, cheaper realization of services, and higher availability. Nonetheless, one of the primary drawbacks customers face when making decision to move their software to a serverless platform is the potential for getting locked-in with a particular provider. This used to be a concern with Platform-as-a-Service (PaaS) offerings too. However with Kubernetes emerging as the industry standard PaaS layer, PaaS is closer to becoming commodity with the Kubernetes API as its common interface. The question is if a similar unification for the API interface layer and runtime contracts can be achieved for serverless. If achieved, this would free up serverless users from their fears of platform lock-in. Our goal in this paper is to extract a minimal common denominator model of execution that can move us closer to a unified serverless platform. As contributors to Knative [13] with in-depth understanding of its internal design, we use Knative as the baseline for this comparison and contrast its API interface and runtime contracts against other prominent serverless platforms to identify commonalities and differences. Influenced by the work in Knative, we also discuss challenges as well as the necessary evolution we expect to see as serverless platforms themselves reach commodity status. © 2019 Association for Computing Machinery.","Cloud; Performance; Scalability; Serverless","Application programming interfaces (API); Clouds; Computer architecture; Locks (fasteners); Middleware; Scalability; Cloud computing architectures; Common denominators; Common interfaces; In-depth understanding; Industry standards; Model of executions; Performance; Serverless; Platform as a Service (PaaS)","Association for Computing Machinery, Inc","ACM","5th International Workshop on Serverless Computing, WOSC 2019 - Part of Middleware 2019","9 December 2019 through 13 December 2019","Davis","156757","Conference paper","Final","","Scopus","2-s2.0-85078952724"
"Chahal D.; Mishra M.; Palepu S.; Singhal R.","Chahal, Dheeraj (57147492400); Mishra, Mayank (57205914416); Palepu, Surya (57223141130); Singhal, Rekha (36069730400)","57147492400; 57205914416; 57223141130; 36069730400","Performance and cost comparison of cloud services for deep learning workload","2021","ICPE 2021 - Companion of the ACM/SPEC International Conference on Performance Engineering","","","","49","55","6","13","10.1145/3447545.3451184","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104932523&doi=10.1145%2f3447545.3451184&partnerID=40&md5=efdc6b5cdaace89e78e9a941764e8b83","Many organizations are migrating their on-premise artificial intelligence workloads to the cloud due to availability of cost-effective and highly scalable infrastructure, software and platform services. To ease the process of migration, many cloud vendors provide services, frameworks and tools that can be used for deployment of applications on cloud infrastructure. Finding the most appropriate service and infrastructure for a given application that results in a desired performance at minimal cost, is a challenge. In this work, we present a methodology to migrate a deep learning model based recommender system to ML platform and serverless architecture. Furthermore, we show our experimental evaluation of AWS ML platform called SageMaker and the serverless platform service known as Lambda. In our study, we also discuss performance and cost trade-off while using cloud infrastructure. © 2021 Association for Computing Machinery.","AWS SageMaker; Cloud performance; ML Platform; Recommendation system","Cost effectiveness; Costs; Economic and social effects; Recommender systems; Web services; Cloud infrastructures; Cloud services; Cost comparisons; Cost effective; Experimental evaluation; Learning models; Scalable infrastructure; Serverless architecture; Deep learning","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC Research","2021 ACM/SPEC International Conference on Performance Engineering, ICPE 2021","19 April 2021 through 21 April 2021","Virtual, Online","168447","Conference paper","Final","","Scopus","2-s2.0-85104932523"
"","","","HiPS 2021 - Proceedings of the 1st Workshop on High Performance Serverless Computing, co-located with HPDC 2021","2021","HiPS 2021 - Proceedings of the 1st Workshop on High Performance Serverless Computing, co-located with HPDC 2021","","","","","","","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110653872&partnerID=40&md5=6c83258165f2c94bbce69f0b9bac1a32","The proceedings contain 7 papers. The topics discussed include: SLA-aware workload scheduling using hybrid cloud services; Apollo: modular and distributed runtime system for serverless function compositions on cloud, edge, and IoT resources; expanding cost-aware function execution with multidimensional notions of cost; distributed parallel analysis engine for high energy physics using AWS Lambda; efficient GPU sharing for serverless workflows; motivating high performance serverless workloads; and towards a serverless bioinformatics cyberinfrastructure pipeline.","","","Association for Computing Machinery, Inc","ACM SIGARCH; University of Arizona","1st Workshop on High Performance Serverless Computing, HiPS 2021 - Co-located with HPDC 2021","25 June 2021","Virtual, Online","169886","Conference review","Final","","Scopus","2-s2.0-85110653872"
"Mittal V.; Qi S.; Bhattacharya R.; Lyu X.; Li J.; Kulkarni S.G.; Li D.; Hwang J.; Ramakrishnan K.K.; Wood T.","Mittal, Viyom (57200494565); Qi, Shixiong (57219267120); Bhattacharya, Ratnadeep (57215330833); Lyu, Xiaosu (57216978433); Li, Junfeng (57195339324); Kulkarni, Sameer G. (57191339154); Li, Dan (58363591300); Hwang, Jinho (55242900800); Ramakrishnan, K.K. (7101600362); Wood, Timothy (36026983100)","57200494565; 57219267120; 57215330833; 57216978433; 57195339324; 57191339154; 58363591300; 55242900800; 7101600362; 36026983100","Mu: An efficient, fair and responsive serverless framework for resource-constrained edge clouds","2021","SoCC 2021 - Proceedings of the 2021 ACM Symposium on Cloud Computing","","","","168","181","13","42","10.1145/3472883.3487014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119283941&doi=10.1145%2f3472883.3487014&partnerID=40&md5=db918b9d4d5ecce95192e8a3a2adfa31","Serverless computing platforms simplify development, deployment, and automated management of modular software functions. However, existing serverless platforms typically assume an over-provisioned cloud, making them a poor fit for Edge Computing environments where resources are scarce. In this paper we propose a redesigned serverless platform that comprehensively tackles the key challenges for serverless functions in a resource constrained Edge Cloud. Our Mu platform cleanly integrates the core resource management components of a serverless platform: autoscaling, load balancing, and placement. Each worker node in Mu transparently propagates metrics such as service rate and queue length in response headers, feeding this information to the load balancing system so that it can better route requests, and to our autoscaler to anticipate workload fluctuations and proactively meet SLOs. Data from the Autoscaler is then used by the placement engine to account for heterogeneity and fairness across competing functions, ensuring overall resource efficiency, and minimizing resource fragmentation. We implement our design as a set of extensions to the Knative serverless platform and demonstrate its improvements in terms of resource efficiency, fairness, and response time. Evaluating Mu, shows that it improves fairness by more than 2x over the default Kubernetes placement engine, improves 99th percentile response times by 62% through better load balancing, reduces SLO violations and resource consumption by pro-active and precise autoscaling. Mu reduces the average number of pods required by more than ∼15% for a set of real Azure workloads.  © 2021 Copyright held by the owner/author(s).","Edge clouds; Resource management; Serverless","Balancing; Efficiency; Natural resources management; Automated management; Autoscaling; Computing platform; Development management; Edge clouds; Load-Balancing; Modular softwares; Resource efficiencies; Resource management; Serverless; Resource allocation","Association for Computing Machinery, Inc","ACM Special Interest Groups on Management of Data (ACM SIGMOD); ACM Special Interest Groups on Operating Systems (ACM SIGOPS); et al.; Exotanium; Microsoft; Salesforce","12th Annual ACM Symposium on Cloud Computing, SoCC 2021","1 November 2021 through 4 November 2021","Virtual, Online","173200","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85119283941"
"Kuśnierz J.; Malawski M.; Padulano V.E.; Tejedor Saavedra E.; Alonso-Jorda P.","Kuśnierz, Jacek (57226138539); Malawski, Maciej (22433325400); Padulano, Vincenzo Eduardo (57219644507); Tejedor Saavedra, Enric (59158391400); Alonso-Jorda, Pedro (24774559100)","57226138539; 22433325400; 57219644507; 59158391400; 24774559100","Distributed Parallel Analysis Engine for High Energy Physics Using AWS Lambda","2021","HiPS 2021 - Proceedings of the 1st Workshop on High Performance Serverless Computing, co-located with HPDC 2021","","","","13","16","3","2","10.1145/3452413.3464788","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110688239&doi=10.1145%2f3452413.3464788&partnerID=40&md5=0fb19a28283a96d9a5ab7186344a3fba","The High-Energy Physics experiments at CERN produce a high volume of data. It is not possible to analyze big chunks of it within a reasonable time by any single machine. The ROOT framework was recently extended with the distributed computing capabilities for massively parallelized RDataFrame applications. This approach, using the MapReduce pattern underneath, made the heavy computations much more approachable even for the newcomers. This paper explores the possibility of running such analyses on serverless services in public cloud using a purely stateless environment. So far, the distributed approaches used by RDataFrame relied on stateful, fully managed computing frameworks like Apache Spark. Here we show that our newly developed tool is able to use perfectly stateless cloud functions, demonstrating the excellent speedup in parallel stage of processing in our benchmarks.  © 2020 ACM.","aws; cern; hep; lambda; mapreduce; root; serverless; spark","Computer programming; Computer science; Computing capability; Computing frameworks; Distributed approaches; High energy physics experiments; High volumes; Parallel analysis; Public clouds; Single- machines; High energy physics","Association for Computing Machinery, Inc","ACM SIGARCH; University of Arizona","1st Workshop on High Performance Serverless Computing, HiPS 2021 - Co-located with HPDC 2021","25 June 2021","Virtual, Online","169886","Conference paper","Final","","Scopus","2-s2.0-85110688239"
"Goli A.; Hajihassani O.; Khazaei H.; Ardakanian O.; Rashidi M.; Dauphinee T.","Goli, Alireza (57090489100); Hajihassani, Omid (57195320865); Khazaei, Hamzeh (36995959600); Ardakanian, Omid (35242108200); Rashidi, Moe (57220499131); Dauphinee, Tyler (56878855200)","57090489100; 57195320865; 36995959600; 35242108200; 57220499131; 56878855200","Migrating from monolithic to serverless: A fintech case study","2020","ICPE 2020 - Companion of the ACM/SPEC International Conference on Performance Engineering","","","","20","25","5","17","10.1145/3375555.3384380","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086021317&doi=10.1145%2f3375555.3384380&partnerID=40&md5=d63261ac85e90acb3395d715065b41d3","Serverless computing is steadily becoming the implementation paradigm of choice for a variety of applications, from data analytics to web applications, as it addresses the main problems with serverfull and monolithic architecture. In particular, it abstracts away resource provisioning and infrastructure management, enabling developers to focus on the logic of the program instead of worrying about resource management which will be handled by cloud providers. In this paper, we consider a document processing system used in FinTech as a case study and describe the migration journey from a monolithic architecture to a serverless architecture. Our evaluation results show that the serverless implementation significantly improves performance while resulting in only a marginal increase in cost. © 2020 ACM.","FinTech; monolith architecture; performance evaluation; serverless computing","Architecture; Computation theory; Data Analytics; Fintech; Cloud providers; Document-processing; Evaluation results; Infrastructure managements; Monolithic architecture; Resource management; Serverless architecture; WEB application; Computer architecture","Association for Computing Machinery, Inc","ACM Special Interest Group on Measurement and Evaluation (SIGMETRICS); ACM Special Interest Group on Software Engineering (SIGSOFT)","11th ACM/SPEC International Conference on Performance Engineering, ICPE 2020","20 April 2020","Edmonton","160111","Conference paper","Final","","Scopus","2-s2.0-85086021317"
"Oh B.; Kim D.","Oh, Byungsoo (57204711776); Kim, Daeyoung (18433791800)","57204711776; 18433791800","Serverless-enabled permissioned blockchain for elastic transaction processing","2019","Middleware Demos and Posters 2019 - Proceedings of the 2019 20th International Middleware Conference Demos and Posters, Part of Middleware 2019","","","","9","10","1","1","10.1145/3366627.3368118","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078939240&doi=10.1145%2f3366627.3368118&partnerID=40&md5=ae3b12976e109c5aa28bff2bf028f3ea","To overcome the performance limits of purely decentralized blockchains, off-chaining models for compute and storage have been studied recently. Serverless computing offers increased elasticity and scalability in executing microservices in event-driven manner, making it proper fit for off-chain computation tasks. In this work, we present a serverless-enabled off-chaining model for permissioned blockchains with some noticeable early results. © 2019 Copyright held by the owner/author(s).","Hyperledger Sawtooth; Off-chain computation; Permissioned blockchain; Serverless computing","Middleware; Computation tasks; Event-driven; Hyperledger Sawtooth; Performance limits; Serverless computing; Transaction processing; Blockchain","Association for Computing Machinery, Inc","ACM","20th International Middleware Conference Demos and Posters, Middleware 2019","9 December 2019 through 13 December 2019","Davis","156762","Conference paper","Final","","Scopus","2-s2.0-85078939240"
"Gias A.U.; Van Hoorn A.; Zhu L.; Casale G.; Düllmann T.F.; Wurster M.","Gias, Alim U. (35434840600); Van Hoorn, André (26421837600); Zhu, Lulai (57194170382); Casale, Giuliano (21742204200); Düllmann, Thomas F. (57192987112); Wurster, Michael (57191865623)","35434840600; 26421837600; 57194170382; 21742204200; 57192987112; 57191865623","Performance engineering for microservices and serverless applications: The RADON approach","2020","ICPE 2020 - Companion of the ACM/SPEC International Conference on Performance Engineering","","","","46","49","3","4","10.1145/3375555.3383120","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086040628&doi=10.1145%2f3375555.3383120&partnerID=40&md5=856b465ae802bcdbb70de211a4d61480","Microservices and serverless functions are becoming integral parts of modern cloud-based applications. Tailored performance engineering is needed for assuring that the applications meet their requirements for quality attributes such as timeliness, resource efficiency, and elasticity. A novel DevOps-based framework for developing microservices and serverless applications is being developed in the RADON project. RADON contributes to performance engineering by including novel approaches for modeling, deployment optimization, testing, and runtime management. This paper summarizes the contents of our tutorial presented at the 11th ACM/SPEC International Conference on Performance Engineering (ICPE). © 2020 Owner/Author.","microservices; performance engineering; serverless","Engineering; Industrial engineering; Cloud-based applications; Deployment optimization; Integral part; Performance engineering; Quality attributes; Resource efficiencies; Runtime management; Radon","Association for Computing Machinery, Inc","ACM Special Interest Group on Measurement and Evaluation (SIGMETRICS); ACM Special Interest Group on Software Engineering (SIGSOFT)","11th ACM/SPEC International Conference on Performance Engineering, ICPE 2020","20 April 2020","Edmonton","160111","Conference paper","Final","","Scopus","2-s2.0-85086040628"
"Alpernas K.; Panda A.; Ryzhyk L.; Sagiv M.","Alpernas, Kalev (57188875613); Panda, Aurojit (55336403500); Ryzhyk, Leonid (25825629600); Sagiv, Mooly (7004822914)","57188875613; 55336403500; 25825629600; 7004822914","Cloud-scale runtime verification of serverless applications","2021","SoCC 2021 - Proceedings of the 2021 ACM Symposium on Cloud Computing","","","","92","107","15","12","10.1145/3472883.3486977","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119282174&doi=10.1145%2f3472883.3486977&partnerID=40&md5=66da35fa513f3cccc91463aa84bfd68c","Serverless platforms aim to simplify the deployment, scaling, and management of cloud applications. Serverless applications are inherently distributed, and are executed using shortlived ephemeral processes. The use of short-lived ephemeral processes simplifies application scaling and management, but also means that existing approaches to monitoring distributed systems and detecting bugs cannot be applied to serverless applications. In this paper we propose Watchtower, a framework that enables runtime monitoring of serverless applications. Watchtower takes program properties as inputs, and can detect cases where applications violate these properties. We design Watchtower to minimize application changes, and to scale at the same rate as the application. We achieve the former by instrumenting libraries rather than application code, and the latter by structuring Watchtower as a serverless application. Once a bug is found, developers can use the Watchtower debugger to identify and address the root cause of the bug.  © 2021 Copyright held by the owner/author(s).","","Application programs; Application codes; Cloud applications; Debuggers; Detecting bugs; Program properties; Property; Root cause; Run-time verification; Runtime Monitoring; Scalings; Program debugging","Association for Computing Machinery, Inc","ACM Special Interest Groups on Management of Data (ACM SIGMOD); ACM Special Interest Groups on Operating Systems (ACM SIGOPS); et al.; Exotanium; Microsoft; Salesforce","12th Annual ACM Symposium on Cloud Computing, SoCC 2021","1 November 2021 through 4 November 2021","Virtual, Online","173200","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85119282174"
"Fotouhi M.; Chen D.; Lloyd W.J.","Fotouhi, Mohammadbagher (57202303824); Chen, Derek (59111801500); Lloyd, Wes J. (8537012400)","57202303824; 59111801500; 8537012400","Function-as-a-service application service composition: Implications for a natural language processing application","2019","WOSC 2019 - Proceedings of the 2019 5th International Workshop on Serverless Computing, Part of Middleware 2019","","","","49","54","5","11","10.1145/3366623.3368141","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078929165&doi=10.1145%2f3366623.3368141&partnerID=40&md5=e122fbc4b0942e89c98fd5f1492934b0","Serverless computing platforms provide Function-as-a-Service (FaaS) to end users for hosting individual functions known as microservices. In this paper, we describe the deployment of a Natural Language Processing (NLP) application using AWS Lambda. We investigate and study the performance and memory implications of two alternate service compositions. First, we evaluate a switchboard architecture, where a single Lambda deployment package aggregates all of the NLP application functions together into a single package. Second, we consider a service isolation architecture where each NLP function is deployed as a separate FaaS function decomposing the application to run across separate runtime containers. We compared the average runtime and processing throughput of these compositions using different pre-trained network weights to initialize our neural networks to perform inference. Additionally, we varied the workload dataset sizes to evaluate implications of inferencing throughput for our NLP application deployed to a FaaS platform. We found our switchboard composition, that shares FaaS runtime containers for all application tasks, produced a 14.75% runtime performance improvement, and also a 17.3% improvement in NLP processing throughput (samples/second). These results demonstrate the potential for careful application service compositions to provide notable performance improvements and ultimately cost savings for application deployments to serverless FaaS platforms. © 2019 Association for Computing Machinery.","Cloud Computing; FaaS; Natural Language Processing; Serverless; Software architecture","Cloud computing; Containers; Electric switchboards; Function evaluation; Middleware; Network architecture; Software architecture; Application deployment; Application functions; Application services; FaaS; NAtural language processing; Natural language processing applications; Serverless; Service applications; Natural language processing systems","Association for Computing Machinery, Inc","ACM","5th International Workshop on Serverless Computing, WOSC 2019 - Part of Middleware 2019","9 December 2019 through 13 December 2019","Davis","156757","Conference paper","Final","","Scopus","2-s2.0-85078929165"
"Bouizem Y.; Parlavantzas N.; DIb D.; Morin C.","Bouizem, Yasmina (57221598978); Parlavantzas, Nikos (6602956495); DIb, Djawida (55366585100); Morin, Christine (7202218434)","57221598978; 6602956495; 55366585100; 7202218434","Active-Standby for High-Availability in FaaS","2020","WOSC 2020 - Proceedings of the 2020 6th International Workshop on Serverless Computing, Part of Middleware 2020","","","","31","36","5","7","10.1145/3429880.3430097","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099596478&doi=10.1145%2f3429880.3430097&partnerID=40&md5=660f0261eb6c41f814b91156c87b6507","Serverless computing is becoming more and more attractive for cloud solution architects and developers. This new computing paradigm relies on Function-as-a-Service (FaaS) platforms that enable deploying functions without being concerned with the underlying infrastructure. An important challenge in designing FaaS platforms is ensuring the availability of deployed functions. Existing FaaS platforms address this challenge principally through retrying function executions. In this paper, we propose and implement an alternative fault-tolerance approach based on active-standby failover. Results from an experimental evaluation show that our approach increases availability and performance compared to the retry-based approach.  © 2020 ACM.","availability; FaaS; fault tolerance","Fault tolerance; Computing paradigm; Experimental evaluation; Failover; Function-as-a-service; High availability; Performance; Tolerance approach; Availability","Association for Computing Machinery, Inc","ACM","6th International Workshop on Serverless Computing, WOSC 2020 - Part of Middleware 2020","7 December 2020 through 11 December 2020","Virtual, Online","166193","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85099596478"
"Zhang W.; Fang V.; Panda A.; Shenker S.","Zhang, Wen (57221431048); Fang, Vivian (57219779696); Panda, Aurojit (55336403500); Shenker, Scott (35593393300)","57221431048; 57219779696; 55336403500; 35593393300","Kappa: A programming framework for serverless computing","2020","SoCC 2020 - Proceedings of the 2020 ACM Symposium on Cloud Computing","","","","328","343","15","47","10.1145/3419111.3421277","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095438979&doi=10.1145%2f3419111.3421277&partnerID=40&md5=993125eddd43417d122c618b8ed13561","Serverless computing has recently emerged as a new paradigm for running software on the cloud. In this paradigm, programs need to be expressed as a set of short-lived tasks, each of which can complete within a short bounded time (e.g., 15 minutes on AWS Lambda). Serverless computing is beneficial to cloud providers - -by allowing them to better utilize resources - -and to users - -by simplifying management and enabling greater elasticity. However, developing applications to run in this environment is challenging, requiring users to appropriately partition their code, develop new coordination mechanisms, and deal with failure recovery. In this paper, we propose Kappa, a framework that simplifies serverless development. It uses checkpointing to handle lambda function timeouts, and provides concurrency mechanisms that enable parallel computation and coordination.  © 2020 Owner/Author.","distributed computing; serverless","Computer programming; Computer science; Check pointing; Cloud providers; Coordination mechanisms; Failure recovery; Parallel Computation; Programming framework; Cloud computing","Association for Computing Machinery, Inc","ACM SIGMOD; ACM SIGOPS; Alibaba Group; aws; et al.; Microsoft","11th ACM Symposium on Cloud Computing, SoCC 2020","19 October 2020 through 21 October 2020","Virtual, Online","163871","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85095438979"
"Jia Z.; Witchel E.","Jia, Zhipeng (57209220014); Witchel, Emmett (22836994600)","57209220014; 22836994600","Nightcore: Efficient and scalable serverless computing for latency-sensitive, interactive microservices","2021","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","","","","152","166","14","147","10.1145/3445814.3446701","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104777845&doi=10.1145%2f3445814.3446701&partnerID=40&md5=3d69b2b758da41b1804f4254a4434610","The microservice architecture is a popular software engineering approach for building flexible, large-scale online services. Serverless functions, or function as a service (FaaS), provide a simple programming model of stateless functions which are a natural substrate for implementing the stateless RPC handlers of microservices, as an alternative to containerized RPC servers. However, current serverless platforms have millisecond-scale runtime overheads, making them unable to meet the strict sub-millisecond latency targets required by existing interactive microservices. We present Nightcore, a serverless function runtime with microsecond-scale overheads that provides container-based isolation between functions. Nightcore's design carefully considers various factors having microsecond-scale overheads, including scheduling of function requests, communication primitives, threading models for I/O, and concurrent function executions. Nightcore currently supports serverless functions written in C/C++, Go, Node.js, and Python. Our evaluation shows that when running latency-sensitive interactive microservices, Nightcore achieves 1.36×-2.93× higher throughput and up to 69% reduction in tail latency. © 2021 Owner/Author.","Cloud computing; function-as-a-service; microservices; serverless computing","Containers; Software engineering; Communication primitives; Natural substrates; On-line service; Programming models; Runtime overheads; Runtimes; Threading model; C++ (programming language)","Association for Computing Machinery","ACM SIGARCH; ACM SIGOPS; ACM SIGPLAN","26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2021","19 April 2021 through 23 April 2021","Virtual, Online","168445","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85104777845"
"Li J.; Kulkarni S.G.; Ramakrishnan K.K.; Li D.","Li, Junfeng (57195339324); Kulkarni, Sameer G. (57191339154); Ramakrishnan, K.K. (7101600362); Li, Dan (58363591300)","57195339324; 57191339154; 7101600362; 58363591300","Understanding open source serverless platforms: Design considerations and performance","2019","WOSC 2019 - Proceedings of the 2019 5th International Workshop on Serverless Computing, Part of Middleware 2019","","","","37","42","5","51","10.1145/3366623.3368139","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078916655&doi=10.1145%2f3366623.3368139&partnerID=40&md5=84b77ab92db7bf96a22b9c066bfeb71d","Serverless computing is increasingly popular because of the promise of lower cost and the convenience it provides to users who do not need to focus on server management. This has resulted in the availability of a number of proprietary and open-source serverless solutions. We seek to understand how the performance of serverless computing depends on a number of design issues using several popular open-source serverless platforms. We identify the idiosyncrasies affecting performance (throughput and latency) for different open-source serverless platforms. Further, we observe that just having either resource-based (CPU and memory) or workload-based (request per second (RPS) or concurrent requests) auto-scaling is inadequate to address the needs of the serverless platforms. © 2019 Association for Computing Machinery.","Function-as-a-service; Performance; Serverless","Middleware; Concurrent requests; Design considerations; Design issues; Open sources; Performance; Resource-based; Server management; Serverless; Open systems","Association for Computing Machinery, Inc","ACM","5th International Workshop on Serverless Computing, WOSC 2019 - Part of Middleware 2019","9 December 2019 through 13 December 2019","Davis","156757","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85078916655"
"Salehe M.; Hu Z.; Mortazavi S.H.; Capes T.; Mohomed I.","Salehe, Mohammad (56483294800); Hu, Zhiming (57211586446); Mortazavi, Seyed Hossein (56312384200); Capes, Tim (57200079286); Mohomed, Iqbal (8503735900)","56483294800; 57211586446; 56312384200; 57200079286; 8503735900","VideoPipe: Building video stream processing pipelines at the edge","2019","Middleware Industry 2019 - Proceedings of the 2019 20th International Middleware Conference Industrial Track, Part of Middleware 2019","","","","43","49","6","18","10.1145/3366626.3368131","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078932541&doi=10.1145%2f3366626.3368131&partnerID=40&md5=95bdf2fcbf394f4ff25df1a4df8e4fb7","Real-time video processing in the home, with the benefits of low latency and strong privacy guarantees, enables virtual reality (VR) applications, augmented reality (AR) applications and other next-gen interactive applications. However, processing video feeds with computationally expensive machine learning algorithms may be impractical on a single device due to resource limitations. Fortunately, there are ubiquitous underutilized heterogeneous edge devices in the home. In this paper, we propose VideoPipe, a system that bridges the gap and runs flexible video processing pipelines on multiple devices. Towards this end, with inspirations from Function-as-a-Service (FaaS) architecture, we have unified the runtime environments of the edge devices. We do this by introducing modules, which are the basic units of a video processing pipeline and can be executed on any device. With the uniform design of input and output interfaces, we can easily connect any of the edge devices to form a video processing pipeline. Moreover, as some devices support containers, we further design and implement stateless services for more computationally expensive tasks such as object detection, pose detection and image classification. As they are stateless, they can be shared across pipelines and can be scaled easily if necessary. To evaluate the performance of our system, we design and implement a fitness application on three devices connected through Wi-Fi. We also implement a gesture-based Internet of Things (IoT) control application. Experimental results show the the promises of VideoPipe for efficient video analytics on the edge. © 2019 Association for Computing Machinery.","Edge computing; Pipelining; Video streaming","Augmented reality; Edge computing; Internet of things; Learning algorithms; Machine learning; Middleware; Object detection; Pipe linings; Pipelines; Video streaming; Virtual reality; Control applications; Design and implements; Expensive machines; Interactive applications; Internet of Things (IOT); Real-time video processing; Resource limitations; Runtime environments; Pipeline processing systems","Association for Computing Machinery, Inc","ACM","20th International Middleware Conference Industrial Track, Middleware Industry 2019 - Part of Middleware 2019","9 December 2019 through 13 December 2019","Davis","156761","Conference paper","Final","","Scopus","2-s2.0-85078932541"
"","","","SoCC 2020 - Proceedings of the 2020 ACM Symposium on Cloud Computing","2020","SoCC 2020 - Proceedings of the 2020 ACM Symposium on Cloud Computing","","","","","","529","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095457011&partnerID=40&md5=1c695c504dbaf57a18ff0009f6463d27","The proceedings contain 35 papers. The topics discussed include: Wukong: a scalable and locality-enhanced framework for serverless parallel computing; particle: ephemeral endpoints for serverless networking; characterizing serverless platforms with ServerlessBench; photons: lambdas on a diet; ByteSeries : an in-memory time series database for large-scale monitoring systems; BAOVERLAY: a block-accessible overlay file system for fast and efficient container storage; challenges and solutions for fast remote persistent memory access; improving the accuracy, adaptability, and interpretability of SSD failure prediction models; leveraging application classes to save power in highly-utilized data centers; and peafowl: in-application CPU scheduling to reduce power consumption of in-memory key-value stores.","","","Association for Computing Machinery, Inc","ACM SIGMOD; ACM SIGOPS; Alibaba Group; aws; et al.; Microsoft","11th ACM Symposium on Cloud Computing, SoCC 2020","19 October 2020 through 21 October 2020","Virtual, Online","163871","Conference review","Final","","Scopus","2-s2.0-85095457011"
"","","","ICPE 2021 - Companion of the ACM/SPEC International Conference on Performance Engineering","2021","ICPE 2021 - Companion of the ACM/SPEC International Conference on Performance Engineering","","","","","","195","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104960502&partnerID=40&md5=7b309f636d950dc45d38c0abe2159813","The proceedings contain 36 papers. The topics discussed include: towards independent run-time cloud monitoring; distributed double machine learning with a serverless architecture; cloud performance variability prediction; 10 years later: cloud computing is closing the performance gap; performance and cost comparison of cloud services for deep learning workload; GradeML: towards holistic performance analysis for machine learning workflows; an empirical evaluation of the performance of video conferencing systems; enabling containerized, parametric and distributed database deployment and benchmarking as a service; how to measure scalability of distributed stream processing engines?; and performance interference on key-value stores in multi-tenant environments: when block size and write requests matter.","","","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC Research","2021 ACM/SPEC International Conference on Performance Engineering, ICPE 2021","19 April 2021 through 21 April 2021","Virtual, Online","168447","Conference review","Final","","Scopus","2-s2.0-85104960502"
"Kurz M.S.","Kurz, Malte S. (57222072882)","57222072882","Distributed double machine learning with a serverless architecture","2021","ICPE 2021 - Companion of the ACM/SPEC International Conference on Performance Engineering","","","","27","33","6","14","10.1145/3447545.3451181","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104997470&doi=10.1145%2f3447545.3451181&partnerID=40&md5=17cb527b4ce2e1e0e3341990fc0e6ffb","This paper explores serverless cloud computing for double machine learning. Being based on repeated cross-fitting, double machine learning is particularly well suited to exploit the high level of parallelism achievable with serverless computing. It allows to get fast on-demand estimations without additional cloud maintenance effort. We provide a prototype Python implementation DoubleML-Serverless for the estimation of double machine learning models with the serverless computing platform AWS Lambda and demonstrate its utility with a case study analyzing estimation times and costs. © 2021 Copyright held by the owner/author(s).","AWS Lambda; Causal Machine Learning; Distributed Computing; Function-as-a-Service (FaaS); Machine Learning; Serverless Computing","Software prototyping; Computing platform; Machine learning models; Maintenance efforts; On demands; Serverless architecture; Machine learning","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC Research","2021 ACM/SPEC International Conference on Performance Engineering, ICPE 2021","19 April 2021 through 21 April 2021","Virtual, Online","168447","Conference paper","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85104997470"
"Sánchez-Artigas M.; Eizaguirre G.T.; Vernik G.; Stuart L.; Garciá-López P.","Sánchez-Artigas, Marc (10939155200); Eizaguirre, Germán T. (57221861497); Vernik, Gil (27068121500); Stuart, Lachlan (57216275665); Garciá-López, Pedro (24479469800)","10939155200; 57221861497; 27068121500; 57216275665; 24479469800","Primula: A practical shuffle/sort operator for serverless computing","2020","Middleware Industry 2020 - Proceedings of the 2020 21st International Middleware Conference Industrial Track, Part of Middleware 2020","","","","31","37","6","13","10.1145/3429357.3430522","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100496382&doi=10.1145%2f3429357.3430522&partnerID=40&md5=ba3d1429a7e73ec7f8174dc2d8adcd9e","Serverless computing has recently gained much attention as a feasible alternative to always-on IaaS for data processing. However, existing severless frameworks are not (yet) usable enough to reach out to a large number of users. To wit, they still require developers to specify the number of serverless functions for a simple sort job. We report our experience in designing Primula, a serverless sort operator that abstracts away users from the complexities of resource provisioning, skewed data and stragglers, yielding the most accessible sort primitive to date. Our evaluation on the IBM Cloud platform demonstrates the usability of Primula without abandoning performance (e.g., 3x faster than a serverless Spark backend and 62% slower than a hybrid serverless/IaaS solution).  © 2020 ACM.","Function-as-a-service; Serverless computing","Data handling; Cloud platforms; Feasible alternatives; Skewed data; Middleware","Association for Computing Machinery, Inc","ACM","21st International Middleware Conference Industrial Track, Middleware Industry 2020 - Part of Middleware 2020","7 December 2020 through 11 December 2020","Virtual, Online","166461","Conference paper","Final","","Scopus","2-s2.0-85100496382"
"","","","WOSC 2020 - Proceedings of the 2020 6th International Workshop on Serverless Computing, Part of Middleware 2020","2020","WOSC 2020 - Proceedings of the 2020 6th International Workshop on Serverless Computing, Part of Middleware 2020","","","","","","78","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099585776&partnerID=40&md5=c3d75bc84ae4d7d67b1a3aaf979df3e1","The proceedings contain 12 papers. The topics discussed include: temporal performance modelling of serverless computing platforms; implications of public cloud resource heterogeneity for inference serving; resource management for cloud functions with memory tracing, profiling and autotuning; an evaluation of serverless data processing frameworks; evaluation of network file system as a shared data storage in serverless computing; active-standby for high-availability in FaaS; ACE: just-in-time serverless software component discovery through approximate concrete execution; and serverless isn’t server-less: measuring and exploiting resource variability on cloud FaaS platforms.","","","Association for Computing Machinery, Inc","ACM","6th International Workshop on Serverless Computing, WOSC 2020 - Part of Middleware 2020","7 December 2020 through 11 December 2020","Virtual, Online","166193","Conference review","Final","","Scopus","2-s2.0-85099585776"
"Pfandzelter T.; Hasenburg J.; Bermbach D.","Pfandzelter, Tobias (57208737730); Hasenburg, Jonathan (57206475077); Bermbach, David (51461094200)","57208737730; 57206475077; 51461094200","Towards a Computing Platform for the LEO Edge","2021","EdgeSys 2021 - Proceedings of the 4th International Workshop on Edge Systems, Analytics and Networking, Part of EuroSys 2021","","","","43","48","5","40","10.1145/3434770.3459736","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104659129&doi=10.1145%2f3434770.3459736&partnerID=40&md5=e0feb67497141b63f7e013440590a3ae","The new space race is heating up as private companies such as SpaceX and Amazon are building large satellite constellations in low-earth orbit (LEO) to provide global broadband internet access. As the number of subscribers connected to this access network grows, it becomes necessary to investigate if and how edge computing concepts can be applied to LEO satellite networks. In this paper, we discuss the unique characteristics of the LEO edge and analyze the suitability of three organization paradigms for applications considering developer requirements. We conclude that the serverless approach is the most promising solution, opening up the field for future research.  © 2021 ACM.","edge computing; LEO constellations; satellite internet","Earth (planet); Access network; Broadband internet access; Computing platform; LEO satellite networks; Low earth orbit(LEO); Number of subscribers; Private companies; Satellite constellations; Orbits","Association for Computing Machinery, Inc","ACM SIGOPS","4th International Workshop on Edge Systems, Analytics and Networking, EdgeSys 2021, in conjunction with ACM EuroSys 2021","26 April 2021","Virtual, Online","168334","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85104659129"
"Du D.; Yu T.; Xia Y.; Zang B.; Yan G.; Qin C.; Wu Q.; Chen H.","Du, Dong (57200438686); Yu, Tianyi (57215927059); Xia, Yubin (7403027696); Zang, Binyu (6701320221); Yan, Guanglu (57215927029); Qin, Chenggang (57189468335); Wu, Qixuan (57215927944); Chen, Haibo (55743141500)","57200438686; 57215927059; 7403027696; 6701320221; 57215927029; 57189468335; 57215927944; 55743141500","Catalyzer: Sub-millisecond startup for serverless computing with initialization-less booting","2020","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","","","","467","481","14","236","10.1145/3373376.3378512","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082400852&doi=10.1145%2f3373376.3378512&partnerID=40&md5=7efa51b655f8910603f7a378b26b339f","Serverless computing promises cost-efficiency and elasticity for high-productive software development. To achieve this, the serverless sandbox system must address two challenges: strong isolation between function instances, and low startup latency to ensure user experience. While strong isolation can be provided by virtualization-based sandboxes, the initialization of sandbox and application causes non-negligible startup overhead. Conventional sandbox systems fall short in low-latency startup due to their application-agnostic nature: they can only reduce the latency of sandbox initialization through hypervisor and guest kernel customization, which is inadequate and does not mitigate the majority of startup overhead. This paper proposes Catalyzer, a serverless sandbox system design providing both strong isolation and extremely fast function startup. Instead of booting from scratch, Catalyzer restores a virtualization-based function instance from a well-formed checkpoint image and thereby skips the initialization on the critical path (init-less). Catalyzer boosts the restore performance by on-demand recovering both user-level memory state and system state. We also propose a new OS primitive, sfork (sandbox fork), to further reduce the startup latency by directly reusing the state of a running sandbox instance. Fundamentally, Catalyzer removes the initialization cost by reusing state, which enables general optimizations for diverse serverless functions. The evaluation shows that Catalyzer reduces startup latency by orders of magnitude, achieves <1ms latency in the best case, and significantly reduces the end-to-end latency for real-world workloads. Catalyzer has been adopted by Ant Financial, and we also present lessons learned from industrial development. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Checkpoint and restore; Operating system; Serverless computing; Startup latency","Restoration; User experience; Virtual reality; Virtualization; Checkpoint and restore; End to end latencies; General optimizations; Industrial development; Operating system; Orders of magnitude; Serverless computing; Start-up overheads; Software design","Association for Computing Machinery","ACM SIGARCH; ACM SIGOPS; ACM SIGPLAN","25th International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2020","16 March 2020 through 20 March 2020","Lausanne","158436","Conference paper","Final","","Scopus","2-s2.0-85082400852"
"Yu T.; Liu Q.; Du D.; Xia Y.; Zang B.; Lu Z.; Yang P.; Qin C.; Chen H.","Yu, Tianyi (57215927059); Liu, Qingyuan (57219510222); Du, Dong (57200438686); Xia, Yubin (7403027696); Zang, Binyu (6701320221); Lu, Ziqian (58361795500); Yang, Pingchao (57219777864); Qin, Chenggang (57189468335); Chen, Haibo (55743141500)","57215927059; 57219510222; 57200438686; 7403027696; 6701320221; 58361795500; 57219777864; 57189468335; 55743141500","Characterizing serverless platforms with serverlessbench","2020","SoCC 2020 - Proceedings of the 2020 ACM Symposium on Cloud Computing","","","","30","44","14","160","10.1145/3419111.3421280","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095411264&doi=10.1145%2f3419111.3421280&partnerID=40&md5=fafab496a31e5500be33b7d60e745cd7","Serverless computing promises auto-scalability and cost-efficiency (in ""pay-as-you-go""manner) for high-productive software development. Because of its virtue, serverless computing has motivated increasingly new applications and services in the cloud. This, however, also presents new challenges including how to efficiently design high-performance serverless platforms and how to efficiently program on the platforms. This paper proposes ServerlessBench, an open-source benchmark suite for characterizing serverless platforms. It includes test cases exploring characteristic metrics of serverless computing, e.g., communication efficiency, startup latency, stateless overhead, and performance isolation. We have applied the benchmark suite to evaluate the most popular serverless computing platforms, including AWS Lambda, Open-Whisk, and Fn, and present new serverless implications from the study. For example, we show scenarios where decoupling an application into a composition of serverless functions can be beneficial in cost-saving and performance, and that the ""stateless""property in serverless computing can hurt the execution performance of serverless functions. These implications form several design guidelines, which may help platform designers to optimize serverless platforms and application developers to design their functions best fit to the platforms.  © 2020 ACM.","","Cloud computing; Efficiency; Open source software; Application developers; Benchmark suites; Communication efficiency; Computing platform; Cost efficiency; Execution performance; New applications; Pay as you go; Software design","Association for Computing Machinery, Inc","ACM SIGMOD; ACM SIGOPS; Alibaba Group; aws; et al.; Microsoft","11th ACM Symposium on Cloud Computing, SoCC 2020","19 October 2020 through 21 October 2020","Virtual, Online","163871","Conference paper","Final","","Scopus","2-s2.0-85095411264"
"Thomas S.; Ao L.; Voelker G.M.; Porter G.","Thomas, Shelby (56939626700); Ao, Lixiang (56482929900); Voelker, Geoffrey M. (7003306507); Porter, George (36442743000)","56939626700; 56482929900; 7003306507; 36442743000","Particle: Ephemeral endpoints for serverless networking","2020","SoCC 2020 - Proceedings of the 2020 ACM Symposium on Cloud Computing","","","","16","29","13","33","10.1145/3419111.3421275","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095437810&doi=10.1145%2f3419111.3421275&partnerID=40&md5=81553ca214d7b34e721538db0109c8ca","Burst-parallel serverless applications invoke thousands of short-lived distributed functions to complete complex jobs such as data analytics, video encoding, or compilation. While these tasks execute in seconds, starting and configuring the virtual network they rely on is a major bottleneck that can consume up to 84% of total startup time. In this paper we characterize the magnitude of this network cold start problem in three popular overlay networks, Docker Swarm, Weave, and Linux Overlay. We focus on end-to-end startup time that encompasses both the time to boot a group of containers as well as interconnecting them. Our primary observation is that existing overlay approaches for serverless networking scale poorly in short-lived serverless environments. Based on our findings we develop Particle, a network stack tailored for multi-node serverless overlay networks that optimizes network creation without sacrificing multi-tenancy, generality, or throughput. When integrated into a serverless burst-parallel video processing pipeline, Particle improves application runtime by 2.4 - 3X over existing overlays.  © 2020 Owner/Author.","burst parallel; lambda; networking; serverless","Cloud computing; Computer operating systems; Data Analytics; Overlay networks; Cold start problems; Distributed function; Multi tenancies; Network creation; Network stack; Video encodings; Video processing; Virtual networks; Video signal processing","Association for Computing Machinery, Inc","ACM SIGMOD; ACM SIGOPS; Alibaba Group; aws; et al.; Microsoft","11th ACM Symposium on Cloud Computing, SoCC 2020","19 October 2020 through 21 October 2020","Virtual, Online","163871","Conference paper","Final","","Scopus","2-s2.0-85095437810"
"Chadha M.; Jindal A.; Gerndt M.","Chadha, Mohak (57193381434); Jindal, Anshul (57203162595); Gerndt, Michael (22333929300)","57193381434; 57203162595; 22333929300","Towards Federated Learning using FaaS Fabric","2020","WOSC 2020 - Proceedings of the 2020 6th International Workshop on Serverless Computing, Part of Middleware 2020","","","","49","54","5","24","10.1145/3429880.3430100","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099604179&doi=10.1145%2f3429880.3430100&partnerID=40&md5=7835ee3ce9f26a34046d77f90d9ad459","Federated learning (FL) enables resource-constrained edge devices to learn a shared Machine Learning (ML) or Deep Neural Network (DNN) model, while keeping the training data local and providing privacy, security, and economic benefits. However, building a shared model for heterogeneous devices such as resource-constrained edge and cloud makes the efficient management of FL-clients challenging. Furthermore, with the rapid growth of FL-clients, the scaling of FL training process is also difficult. In this paper, we propose a possible solution to these challenges: federated learning over a combination of connected Function-as-a-Service platforms, i.e., FaaS fabric offering a seamless way of extending FL to heterogeneous devices. Towards this, we present FedKeeper, a tool for efficiently managing FL over FaaS fabric. We demonstrate the functionality of FedKeeper by using three FaaS platforms through an image classification task with a varying number of devices/clients, different stochastic optimizers, and local computations (local epochs).  © 2020 ACM.","FaaS; FaaS platforms; Federated learning; Function-as-a-service; Neural networks; Serverless","Deep neural networks; Stochastic systems; Faas; Faas platform; Federated learning; Function-as-a-service; Heterogeneous devices; Learn+; Machine-learning; Neural network model; Neural-networks; Serverless; Optimization","Association for Computing Machinery, Inc","ACM","6th International Workshop on Serverless Computing, WOSC 2020 - Part of Middleware 2020","7 December 2020 through 11 December 2020","Virtual, Online","166193","Conference paper","Final","","Scopus","2-s2.0-85099604179"
"Singhvi A.; Balasubramanian A.; Houck K.; Shaikh M.D.; Venkataraman S.; Akella A.","Singhvi, Arjun (57200408847); Balasubramanian, Arjun (57215312160); Houck, Kevin (57195371768); Shaikh, Mohammed Danish (57219497002); Venkataraman, Shivaram (55312380900); Akella, Aditya (8383144400)","57200408847; 57215312160; 57195371768; 57219497002; 55312380900; 8383144400","Atoll: A scalable low-latency serverless platform","2021","SoCC 2021 - Proceedings of the 2021 ACM Symposium on Cloud Computing","","","","138","152","14","68","10.1145/3472883.3486981","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119283200&doi=10.1145%2f3472883.3486981&partnerID=40&md5=e34d2d96751f597e52f3843564c21af2","With user-facing apps adopting serverless computing, good latency performance of serverless platforms has become a strong fundamental requirement. However, it is difficult to achieve this on platforms today due to the design of their underlying control and data planes that are particularly ill-suited to short-lived functions with unpredictable arrival patterns. We present Atoll, a serverless platform, that overcomes the challenges via a ground-up redesign of the control and data planes. In Atoll, each app is associated with a latency deadline. Atoll achieves its per-app request latency goals by: (a) partitioning the cluster into (semi-global scheduler, worker pool) pairs, (b) performing deadline-aware scheduling and proactive sandbox allocation, and (c) using a load balancing layer to do sandbox-aware routing, and automatically scale the semi-global schedulers per app. Our results show that Atoll reduces missed deadlines by ∼66x and tail latencies by ∼3x compared to state-of-the-art alternatives.  © 2021 Association for Computing Machinery.","Low-Latency; Scalable; Serverless Computing","Arrival patterns; Control planes; Data planes; Data-plane; Global schedulers; Latency performance; Low latency; Scalable; Semi-global; Serverless computing; Scheduling","Association for Computing Machinery, Inc","ACM Special Interest Groups on Management of Data (ACM SIGMOD); ACM Special Interest Groups on Operating Systems (ACM SIGOPS); et al.; Exotanium; Microsoft; Salesforce","12th Annual ACM Symposium on Cloud Computing, SoCC 2021","1 November 2021 through 4 November 2021","Virtual, Online","173200","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85119283200"
"Daw N.; Bellur U.; Kulkarni P.","Daw, Nilanjan (57211429693); Bellur, Umesh (15063937300); Kulkarni, Purushottam (7201958713)","57211429693; 15063937300; 7201958713","Speedo: Fast dispatch and orchestration of serverless workflows","2021","SoCC 2021 - Proceedings of the 2021 ACM Symposium on Cloud Computing","","","","585","599","14","12","10.1145/3472883.3486982","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119265878&doi=10.1145%2f3472883.3486982&partnerID=40&md5=81b4fe1665ff10055b13cfbd21732eee","Structuring cloud applications as collections of interacting fine-grained microservices makes them scalable and affords the flexibility of hot upgrading parts of the application. The current avatar of serverless computing (FaaS) with its dynamic resource allocation and auto-scaling capabilities make it the deployment model of choice for such applications. FaaS platforms operate with user space dispatchers that receive requests over the network and make a dispatch decision to one of multiple workers (usually a container) distributed in the data center. With the granularity of microservices approaching execution times of a few milliseconds combined with loads approaching tens of thousands of requests a second, having a low dispatch latency of less than one millisecond becomes essential to keep up with line rates. When these microservices are part of a workflow making up an application, the orchestrator that coordinates the sequence in which microservices execute also needs to operate with microsecond latency. Our observations reveal that the most significant component of the dispatch/orchestration latency is the time it takes for the request to traverse into and out of the user space from the network. Motivated by the presence of a multitude of low power cores on today's SmartNICs, one approach to keeping up with these high line rates and the stringent latency expectations is to run both the dispatcher and the orchestrator close to the network on a SmartNIC. Doing so will save valuable cycles spent in transferring requests to and back from the user space. The operating characteristics of short-lived ephemeral state and low CPU burst requirements of FaaS dispatcher/orchestrator make them ideal candidates for offloading from the server to the NIC cores. This also brings other benefit of freeing up the server CPU. In this paper, we present Speedo - - a design for offloading of FaaS dispatch and orchestration services to the SmartNIC from the user space. We implemented Speedo on ASIC based Netronome Agilio SmartNICs and our comprehensive evaluation shows that Speedo brings down the dispatch latency from ∼150ms to ∼140μs at a load of 10K requests per second.  © 2021 Association for Computing Machinery.","Orchestration; Programmable SmartNIC; Serverless workflows","Low power electronics; 'current; Cloud applications; Dynamic resource allocations; Fine grained; Line rate; Orchestration; Programmable smartnic; Serverless workflow; User spaces; Work-flows; Space platforms","Association for Computing Machinery, Inc","ACM Special Interest Groups on Management of Data (ACM SIGMOD); ACM Special Interest Groups on Operating Systems (ACM SIGOPS); et al.; Exotanium; Microsoft; Salesforce","12th Annual ACM Symposium on Cloud Computing, SoCC 2021","1 November 2021 through 4 November 2021","Virtual, Online","173200","Conference paper","Final","","Scopus","2-s2.0-85119265878"
"Sreekanti V.; Wu C.; Chhatrapati S.; Gonzalez J.E.; Hellerstein J.M.; Faleiro J.M.","Sreekanti, Vikram (57210114238); Wu, Chenggang (57206251637); Chhatrapati, Saurav (57217096816); Gonzalez, Joseph E. (57200981709); Hellerstein, Joseph M. (35561994000); Faleiro, Jose M. (23974170800)","57210114238; 57206251637; 57217096816; 57200981709; 35561994000; 23974170800","A fault-Tolerance shim for serverless computing","2020","Proceedings of the 15th European Conference on Computer Systems, EuroSys 2020","","","","","","","49","10.1145/3342195.3387535","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087109549&doi=10.1145%2f3342195.3387535&partnerID=40&md5=be7f879740c5a8fd536c529c6753f207","Serverless computing has grown in popularity in recent years, with an increasing number of applications being built on Functions-As-A-Service (FaaS) platforms. By default, FaaS platforms support retry-based fault tolerance, but this is insufficient for programs that modify shared state, as they can unwittingly persist partial sets of updates in case of failures. To address this challenge, we would like atomic visibility of the updates made by a FaaS application. In this paper, we present aft, an atomic fault tolerance shim for serverless applications. aft interposes between a commodity FaaS platform and storage engine and ensures atomic visibility of updates by enforcing the read atomic isolation guarantee. aft supports new protocols to guarantee read atomic isolation in the serverless setting. We demonstrate that aft introduces minimal overhead relative to existing storage engines and scales smoothly to thousands of requests per second, while preventing a significant number of consistency anomalies. © 2020 ACM.","","Atoms; Engines; Fault tolerance; Shims; Virtual storage; Visibility; New protocol; Shared state; Storage engines; Fault tolerant computer systems","Association for Computing Machinery, Inc","ACM Special Interest Group on Operating Systems (SIGOPS)","15th European Conference on Computer Systems, EuroSys 2020","27 April 2020 through 30 April 2020","Heraklion","160104","Conference paper","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85087109549"
"Baughman M.; Kumar R.; Foster I.; Chard K.","Baughman, Matt (57202993030); Kumar, Rohan (57205738039); Foster, Ian (35572232000); Chard, Kyle (9132950200)","57202993030; 57205738039; 35572232000; 9132950200","Expanding Cost-Aware Function Execution with Multidimensional Notions of Cost","2021","HiPS 2021 - Proceedings of the 1st Workshop on High Performance Serverless Computing, co-located with HPDC 2021","","","","9","12","3","2","10.1145/3452413.3464790","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110751460&doi=10.1145%2f3452413.3464790&partnerID=40&md5=a7d052430e58efcb4aab74c3ff369438","Recent advances in networking technology and serverless architectures have enabled automated distribution of compute workloads at the function level. As heterogeneity and physical distribution of computing resources increase, so too does the need to effectively use those resources. This is especially true when leveraging multiple compute resources in the form of local, distributed, and cloud resources. Adding to the complexity of the problem is different notions of ""cost""when it comes to using these resources. Tradeoffs exist due to the inherent difference between costs of computation for the end user. For example, deploying a workload on the cloud could be much faster than using local resources but using the cloud incurs a financial cost. Here, the end user is presented with the tradeoff between time and money. We describe preliminary work towards Delts+, a framework that integrates multidimensional cost objectives, cost tradeoffs, and optimization under constraints.  © 2020 ACM.","computing continuum; heterogeneous computing; serverless","Computer programming; Automated distribution; Compute resources; Computing resource; Financial costs; Function levels; Local resources; Networking technology; Serverless architecture; Computer science","Association for Computing Machinery, Inc","ACM SIGARCH; University of Arizona","1st Workshop on High Performance Serverless Computing, HiPS 2021 - Co-located with HPDC 2021","25 June 2021","Virtual, Online","169886","Conference paper","Final","","Scopus","2-s2.0-85110751460"
"Mvondo D.; Barbalace A.; Tchana A.; Muller G.","Mvondo, Djob (57208125838); Barbalace, Antonio (23491344500); Tchana, Alain (24829973600); Muller, Gilles (7403586118)","57208125838; 23491344500; 24829973600; 7403586118","Tell me when you are sleepy and what may wake you up!","2021","SoCC 2021 - Proceedings of the 2021 ACM Symposium on Cloud Computing","","","","562","569","7","3","10.1145/3472883.3487013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119288294&doi=10.1145%2f3472883.3487013&partnerID=40&md5=d24d5d8c6563e6148f50057e122db6ca","Nowadays, there is a shift in the deployment model of Cloud and Edge applications. Applications are now deployed as a set of several small units communicating with each other - the microservice model. Moreover, each unit - a microservice, may be implemented as a virtual machine, container, function, etc., spanning the different Cloud and Edge service models including IaaS, PaaS, FaaS. A microservice is instantiated upon the reception of a request (e.g., an http packet or a trigger), and a rack-level or data-center-level scheduler decides the placement for such unit of execution considering for example data locality and load balancing. With such a configuration, it is common to encounter scenarios where different units, as well as multiple instances of the same unit, may be running on a single server at the same time. When multiple microservices are running on the same server not necessarily all of them are doing actual processing, some may be busy-waiting - i.e., waiting for events (or requests) sent by other units. However, these ""idle""units are consuming CPU time which could be used by other running units or cloud utility functions on the server (e.g., monitoring daemons). In a controlled experiment, we observe that units can spend up to 20% - 55% of their CPU time waiting, thus a great amount of CPU time is wasted; these values significantly grow when overcommitting CPU resources (i.e., units CPU reservations exceed server CPU capacity), where we observe up to 69% - 75%. This is a result of the lack of information/context about what is running in each unit from the server CPU scheduler perspective. In this paper, we first provide evidence of the problem and discuss several research questions. Then, we propose an handful of solutions worth exploring that consists in revisiting hypervisor and host OS scheduler designs to reduce the CPU time wasted on idle units. Our proposal leverages the concepts of informed scheduling, and monitoring for internal and external events. Based on the aforementioned solutions, we propose our initial implementation on Linux/KVM.  © 2021 Association for Computing Machinery.","","Balancing; Computer operating systems; Cloud services; Containers functions; CPU time; Data load; Data locality; Datacenter; Deployment models; Edge services; Service modeling; Smallest unit; Scheduling","Association for Computing Machinery, Inc","ACM Special Interest Groups on Management of Data (ACM SIGMOD); ACM Special Interest Groups on Operating Systems (ACM SIGOPS); et al.; Exotanium; Microsoft; Salesforce","12th Annual ACM Symposium on Cloud Computing, SoCC 2021","1 November 2021 through 4 November 2021","Virtual, Online","173200","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85119288294"
"","","","Middleware 2021 Demos and Posters - Proceedings of the 2021 International Middleware Conference Demos and Posters","2021","Middleware 2021 Demos and Posters - Proceedings of the 2021 International Middleware Conference Demos and Posters","","","","","","22","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121450810&partnerID=40&md5=29fb8a215094d52b91e2a835654fa17c","The proceedings contain 8 papers. The topics discussed include: Scanflow: an end-to-end agent-based autonomic ML workflow manager for clusters; a zero-knowledge proof system for OpenLibra; benchmarking Apache Kafka under network faults; DyMonD: dynamic application monitoring and service detection framework; a milestone for FaaS pipelines; object storage- vs VM-driven data exchange; benchmarking-as-a-service for cloud-hosted DBMS; bricks: a configurable coordination service with multiple consistency models; and GNOSIS- query-driven multimodal event processing for unstructured data streams.","","","Association for Computing Machinery, Inc","ACM; ETS; University Laval","22nd International Middleware Conference, Middleware 2021","6 December 2021 through 10 December 2021","Virtual, Online","175055","Conference review","Final","","Scopus","2-s2.0-85121450810"
"Eismann S.; Grohmann J.; Van Eyk E.; Herbst N.; Kounev S.","Eismann, Simon (57203095294); Grohmann, Johannes (57197744369); Van Eyk, Erwin (57203098031); Herbst, Nikolas (55747080400); Kounev, Samuel (23397538000)","57203095294; 57197744369; 57203098031; 55747080400; 23397538000","Predicting the costs of serverless workflows","2020","ICPE 2020 - Proceedings of the ACM/SPEC International Conference on Performance Engineering","","","","265","276","11","53","10.1145/3358960.3379133","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085918914&doi=10.1145%2f3358960.3379133&partnerID=40&md5=508f26ab939c912052c5adc9b6b48fb9","Function-as-a-Service (FaaS) platforms enable users to run arbitrary functions without being concerned about operational issues, while only paying for the consumed resources. Individual functions are often composed into workflows for complex tasks. However, the pay-per-use model and nontransparent reporting by cloud providers make it challenging to estimate the expected cost of a workflow, which prevents informed business decisions. Existing cost-estimation approaches assume a static response time for the serverless functions, without taking input parameters into account. In this paper, we propose a methodology for the cost prediction of serverless workflows consisting of input-parameter sensitive function models and a monte-carlo simulation of an abstract workflow model. Our approach enables workflow designers to predict, compare, and optimize the expected costs and performance of a planned workflow, which currently requires time-intensive experimentation. In our evaluation, we show that our approach can predict the response time and output parameters of a function based on its input parameters with an accuracy of 96.1%. In a case study with two audio-processing workflows, our approach predicts the costs of the two workflows with an accuracy of 96.2%. © 2020 ACM.","Cost; Performance; Prediction; Serverless; Workflows","Cost estimating; Forecasting; Intelligent systems; Monte Carlo methods; Parameter estimation; Abstract workflow; Arbitrary functions; Audio processing; Business decisions; Operational issues; Output parameters; Pay-per-use model; Sensitive functions; Cost benefit analysis","Association for Computing Machinery, Inc","ACM Special Interest Group on Measurement and Evaluation (SIGMETRICS); ACM Special Interest Group on Software Engineering (SIGSOFT)","11th ACM/SPEC International Conference on Performance Engineering, ICPE 2020","20 April 2020 through 24 April 2020","Edmonton","160106","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85085918914"
"","","","12th Annual ACM Symposium on Cloud Computing, SoCC 2021","2021","SoCC 2021 - Proceedings of the 2021 ACM Symposium on Cloud Computing","","","","","","677","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119261031&partnerID=40&md5=32a122919c61638792fb734f39c6fde7","The proceedings contain 46 papers. The topics discussed include: Lorien: efficient deep learning workloads delivery; elastic hyperparameter tuning on the cloud; siren: byzantine-robust federated learning via proactive alarming; automating instrumentation choices for performance problems in distributed applications with VAIF; tprof: performance profiling via structural aggregation and automated analysis of distributed systems traces; cloud-scale runtime verification of serverless applications; building reliable cloud services using coyote actors; atoll: a scalable low-latency serverless platform; kraken : adaptive container provisioning for deploying dynamic DAG applications in serverless platforms; and Mu: an efficient, fair and responsive serverless framework for resource-constrained edge clouds.","","","Association for Computing Machinery, Inc","ACM Special Interest Groups on Management of Data (ACM SIGMOD); ACM Special Interest Groups on Operating Systems (ACM SIGOPS); et al.; Exotanium; Microsoft; Salesforce","12th Annual ACM Symposium on Cloud Computing, SoCC 2021","1 November 2021 through 4 November 2021","Virtual, Online","173200","Conference review","Final","","Scopus","2-s2.0-85119261031"
"Gunasekaran J.R.; Mishra C.S.; Thinakaran P.; Kandemir M.T.; Das C.R.","Gunasekaran, Jashwant Raj (55440506600); Mishra, Cyan Subhra (57216649340); Thinakaran, Prashanth (55428420900); Kandemir, Mahmut Taylan (35549787100); Das, Chita R. (7201851990)","55440506600; 57216649340; 55428420900; 35549787100; 7201851990","Implications of Public Cloud Resource Heterogeneity for Inference Serving","2020","WOSC 2020 - Proceedings of the 2020 6th International Workshop on Serverless Computing, Part of Middleware 2020","","","","7","12","5","7","10.1145/3429880.3430093","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099607718&doi=10.1145%2f3429880.3430093&partnerID=40&md5=3c265a4b30f804b436e5c59714686a8b","We are witnessing an increasing trend towards using Machine Learning (ML) based prediction systems, spanning across different application domains, including product recommendation systems, personal assistant devices, facial recognition, etc. These applications typically have diverse requirements in terms of accuracy and response latency, that can be satisfied by a myriad of ML models. However, the deployment cost of prediction serving primarily depends on the type of resources being procured, which by themselves are heterogeneous in terms of provisioning latencies and billing complexity. Thus, it is strenuous for an inference serving system to choose from this confounding array of resource types and model types to provide low-latency and cost-effective inferences. In this work we quantitatively characterize the cost, accuracy and latency implications of hosting ML inferences on different public cloud resource offerings. Our evaluation shows that, prior work does not solve the problem from both dimensions of model and resource heterogeneity. Hence, to holistically address this problem, we need to solve the issues that arise from combining both model and resource heterogeneity towards optimizing for application constraints. Towards this, we discuss the design implications of a self-managed inference serving system, which can optimize for application requirements based on public cloud resource characteristics.  © 2020 ACM.","inference; resource-management; serverless","Face recognition; Applications domains; Inference; Machine-learning; Personal assistants; Prediction systems; Product recommendation system; Public clouds; Resource heterogeneity; Resource management; Serverless; Cost effectiveness","Association for Computing Machinery, Inc","ACM","6th International Workshop on Serverless Computing, WOSC 2020 - Part of Middleware 2020","7 December 2020 through 11 December 2020","Virtual, Online","166193","Conference paper","Final","","Scopus","2-s2.0-85099607718"
"Suresh A.; Gandhi A.","Suresh, Amoghavarsha (56161438700); Gandhi, Anshul (56744293000)","56161438700; 56744293000","ServerMore: Opportunistic execution of serverless functions in the cloud","2021","SoCC 2021 - Proceedings of the 2021 ACM Symposium on Cloud Computing","","","","570","584","14","23","10.1145/3472883.3486979","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119270814&doi=10.1145%2f3472883.3486979&partnerID=40&md5=b223984361798c8fa7631b7ac07814a6","Serverless computing allows customers to submit their jobs to the cloud for execution, with the resource provisioning being taken care of by the cloud provider. Serverless functions are often short-lived and have modest resource requirements, thereby presenting an opportunity to improve server utilization by colocating with latency-sensitive customer workloads. This paper presents ServerMore, a server-level resource manager that opportunistically colocates customer serverless jobs with serverful customer VMs. ServerMore dynamically regulates the CPU, memory bandwidth, and LLC resources on the server to ensure that the colocation between serverful and serverless workloads does not impact application tail latencies. By selectively admitting serverless functions and inferring the performance of black-box serverful workloads, ServerMore improves resource utilization on average by 35.9% to 245% compared to prior works; while having a minimal impact on the latency of both serverful applications and serverless functions.  © 2021 Association for Computing Machinery.","","Black boxes; Cloud providers; Colocations; CPU memory; Memory bandwidths; Performance; Resource managers; Resource requirements; Resources utilizations; Sales","Association for Computing Machinery, Inc","ACM Special Interest Groups on Management of Data (ACM SIGMOD); ACM Special Interest Groups on Operating Systems (ACM SIGOPS); et al.; Exotanium; Microsoft; Salesforce","12th Annual ACM Symposium on Cloud Computing, SoCC 2021","1 November 2021 through 4 November 2021","Virtual, Online","173200","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85119270814"
"Mvondo D.; Bacou M.; Nguetchouang K.; Ngale L.; Pouget S.; Kouam J.; Lachaize R.; Hwang J.; Wood T.; Hagimont D.; De Palma N.; Batchakui B.; Tchana A.","Mvondo, Djob (57208125838); Bacou, Mathieu (57201774110); Nguetchouang, Kevin (57223235284); Ngale, Lucien (57223223022); Pouget, Stéphane (57223227433); Kouam, Josiane (57223230512); Lachaize, Renaud (23393235900); Hwang, Jinho (55242900800); Wood, Tim (36026983100); Hagimont, Daniel (6601906257); De Palma, Noël (15520660100); Batchakui, Bernabé (24832875400); Tchana, Alain (24829973600)","57208125838; 57201774110; 57223235284; 57223223022; 57223227433; 57223230512; 23393235900; 55242900800; 36026983100; 6601906257; 15520660100; 24832875400; 24829973600","OFC: An opportunistic caching system for FaaS platforms","2021","EuroSys 2021 - Proceedings of the 16th European Conference on Computer Systems","","","3456239","228","244","16","70","10.1145/3447786.3456239","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105309748&doi=10.1145%2f3447786.3456239&partnerID=40&md5=7c85ad72c8a3eed07b94470358eb1c79","Cloud applications based on the ""Functions as a Service""(FaaS) paradigm have become very popular. Yet, due to their stateless nature, they must frequently interact with an external data store, which limits their performance. To mitigate this issue, we introduce OFC, a transparent, vertically and horizontally elastic in-memory caching system for FaaS platforms, distributed over the worker nodes. OFC provides these benefits cost-effectively by exploiting two common sources of resource waste: (i) most cloud tenants overprovision the memory resources reserved for their functions because their footprint is non-trivially input-dependent and (ii) FaaS providers keep function sandboxes alive for several minutes to avoid cold starts. Using machine learning models adjusted for typical function input data categories (e.g., multimedia formats), OFC estimates the actual memory resources required by each function invocation and hoards the remaining capacity to feed the cache. We build our OFC prototype based on enhancements to the OpenWhisk FaaS platform, the Swift persistent object store, and the RAM-Cloud in-memory store. Using a diverse set of workloads, we show that OFC improves by up to 82 % and 60 % respectively the execution time of single-stage and pipelined functions. © 2021 ACM.","Cache; Cloud computing; Functions as a service (FaaS); Latency; Serverless","Multimedia systems; Random access storage; Cloud applications; Machine learning models; Memory resources; Multimedia format; Opportunistic caching; Persistent objects; Remaining capacity; Resource wastes; Cache memory","Association for Computing Machinery, Inc","","16th European Conference on Computer Systems, EuroSys 2021","26 April 2021 through 28 April 2021","Virtual, Online","168531","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85105309748"
"Yao S.D.; Gulzar M.A.; Zhang L.; Butt A.R.","Yao, Shunyu David (57226139738); Gulzar, Muhammad Ali (57189494454); Zhang, Liqing (55709248500); Butt, Ali R. (8552023700)","57226139738; 57189494454; 55709248500; 8552023700","Towards a Serverless Bioinformatics Cyberinfrastructure Pipeline","2021","HiPS 2021 - Proceedings of the 1st Workshop on High Performance Serverless Computing, co-located with HPDC 2021","","","","33","40","7","0","10.1145/3452413.3464787","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110588800&doi=10.1145%2f3452413.3464787&partnerID=40&md5=883d9c324a67f32c2fb19781272b461a","Function-As-A-Service (FaaS) and the serverless computing model offer a powerful abstraction for supporting large-scale applications in the cloud. A major hurdle in this context is that it is non-Trivial to transform an application, even an already containerized one, to a FaaS implementation. In this paper, we take the first step towards supporting easier and efficient application transformation to FaaS. We present a systematic scheme to transform applications written in Python into a set of functions that can then be automatically deployed atop platforms such as AWS Lamda. We target a Bioinformatics cyberinfrastructure pipeline, CIWARS, that provides waste-water analysis for the identification of antibiotic-resistant bacteria and viruses such as SARS-CoV-2. Based on our experience with enabling FaaS-based CIWARS, we develop a methodology that would help the conversion of other similar applications to the FaaS model. Our evaluation shows that our approach can correctly transform CIWARS to FaaS, and the new FaaS-based CIWARS incurs only negligible(≤2%) less than 2% overhead for representative workloads.  © 2020 ACM.","function-As-A-service; program decomposition; serverless computing","Bioinformatics; Viruses; Antibiotic-resistant bacteria; Computing model; Cyber infrastructures; Large-scale applications; Non-trivial; Pipelines","Association for Computing Machinery, Inc","ACM SIGARCH; University of Arizona","1st Workshop on High Performance Serverless Computing, HiPS 2021 - Co-located with HPDC 2021","25 June 2021","Virtual, Online","169886","Conference paper","Final","","Scopus","2-s2.0-85110588800"
"Nasirifard P.; Slominski A.; Muthusamy V.; Ishakian V.; Jacobsen H.-A.","Nasirifard, Pezhman (6507596161); Slominski, Aleksander (56236672700); Muthusamy, Vinod (7801671473); Ishakian, Vatche (36630445800); Jacobsen, Hans-Arno (7103073434)","6507596161; 56236672700; 7801671473; 36630445800; 7103073434","Demo: A serverless topic-based and content-based pub/sub broker","2017","Middleware 2017 - Proceedings of the 2017 Middleware Posters and Demos 2017: Proceedings of the Posters and Demos Session of the 18th International Middleware Conference","","","","23","24","1","3","10.1145/3155016.3155024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046077467&doi=10.1145%2f3155016.3155024&partnerID=40&md5=762883d4fc7e623ae6ab681545fef0d4","Building scalable, highly available publish/subscribe (pub/sub) systems can require sophisticated algorithms and a tremendous amount of engineering e?ort. This paper demonstrates a way to build a pub/sub broker on top of the OpenWhisk serverless platform that performs topic-based and contentbased matching. This approach radically simplifes the design and signifcantly reduces the amount of code while still achieving scalability targets. Furthermore, we present a publisher/subscriber client application to interact with the broker as well as an evaluator application that enforces heavy workload on the broker to measure the scalability and latency of the pub/sub system and discover the potential bottlenecks. © 2017 held by the owner/author(s).","Content-based pub/sub; Function as a service(FaaS); OpenWhisk; Serverless; Topic-based pub/sub","Scalability; Client applications; Content-based matching; Heavy workloads; OpenWhisk; Pub/sub; Publish/subscribe; Publisher/subscriber; Serverless; Middleware","Association for Computing Machinery, Inc","ACM; IFIP; The Advanced Computing Systems Association (USENIX)","18th ACM/IFIP/USENIX International Middleware Conference, Middleware 2017","11 December 2017 through 15 December 2017","Las Vegas","133164","Conference paper","Final","","Scopus","2-s2.0-85046077467"
"Gan Y.; Zhang Y.; Cheng D.; Shetty A.; Rathi P.; Katarki N.; Bruno A.; Hu J.; Ritchken B.; Jackson B.; Hu K.; Pancholi M.; He Y.; Clancy B.; Colen C.; Wen F.; Leung C.; Wang S.; Zaruvinsky L.; Espinosa M.; Lin R.; Liu Z.; Padilla J.; Delimitrou C.","Gan, Yu (57202256497); Zhang, Yanqi (57208405061); Cheng, Dailun (57208394034); Shetty, Ankitha (57213952904); Rathi, Priyal (57208404827); Katarki, Nayan (57208388593); Bruno, Ariana (57208388395); Hu, Justin (57208386612); Ritchken, Brian (57208406630); Jackson, Brendon (57208409015); Hu, Kelvin (57208386861); Pancholi, Meghna (57208386677); He, Yuan (57208401696); Clancy, Brett (57208388706); Colen, Chris (57208393389); Wen, Fukang (57208407841); Leung, Catherine (57208394165); Wang, Siyuan (57208399718); Zaruvinsky, Leon (57208409009); Espinosa, Mateo (57208400165); Lin, Rick (57208384974); Liu, Zhongling (57208394253); Padilla, Jake (57208386048); Delimitrou, Christina (38361424300)","57202256497; 57208405061; 57208394034; 57213952904; 57208404827; 57208388593; 57208388395; 57208386612; 57208406630; 57208409015; 57208386861; 57208386677; 57208401696; 57208388706; 57208393389; 57208407841; 57208394165; 57208399718; 57208409009; 57208400165; 57208384974; 57208394253; 57208386048; 38361424300","An Open-Source Benchmark Suite for Microservices and Their Hardware-Software Implications for Cloud & Edge Systems","2019","International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS","","","","3","18","15","489","10.1145/3297858.3304013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064688619&doi=10.1145%2f3297858.3304013&partnerID=40&md5=b9f662fba2a35d83eb40d32009cf7676","Cloud services have recently started undergoing a major shift from monolithic applications, to graphs of hundreds of loosely-coupled microservices. Microservices fundamentally change a lot of assumptions current cloud systems are designed with, and present both opportunities and challenges when optimizing for quality of service (QoS) and utilization. In this paper we explore the implications microservices have across the cloud system stack. We first present Death- StarBench, a novel, open-source benchmark suite built with microservices that is representative of large end-to-end services, modular and extensible. DeathStarBench includes a social network, a media service, an e-commerce site, a banking system, and IoT applications for coordination control of UAV swarms. We then use DeathStarBench to study the architectural characteristics of microservices, their implications in networking and operating systems, their challenges with respect to cluster management, and their trade-offs in terms of application design and programming frameworks. Finally, we explore the tail at scale effects of microservices in real deployments with hundreds of users, and highlight the increased pressure they put on performance predictability. © 2019 Association for Computing Machinery.","acceleration; cloud computing; cluster management; datacenters; fpga; microservices; QoS; serverless","Acceleration; Cloud computing; Cluster computing; Economic and social effects; Field programmable gate arrays (FPGA); Open source software; Open systems; Quality of service; Application design; Cluster management; Coordination control; Data centers; End-to-end service; microservices; Programming framework; serverless; Computer systems programming","Association for Computing Machinery","ACM SIGARCH; ACM SIGOPS; ACM SIGPLAN","24th International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2019","13 April 2019 through 17 April 2019","Providence","147090","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85064688619"
"Wang K.-T.A.; Ho R.; Wu P.","Wang, Kai-Ting Amy (57208131873); Ho, Rayson (57208133492); Wu, Peng (59867864700)","57208131873; 57208133492; 59867864700","Replayable execution optimized for page sharing for a managed runtime environment","2019","Proceedings of the 14th EuroSys Conference 2019","","","3303978","","","","64","10.1145/3302424.3303978","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063898722&doi=10.1145%2f3302424.3303978&partnerID=40&md5=1048458816667ac0c3a52d8d59b3e965","We present Replayable Execution, a system for improving the efficiency of Function-as-a-Service (FaaS) frameworks. It takes advantage of standard kernel features to reduce memory usage and accelerate cold startup speed without changes to the OS kernel, language runtimes, and the surrounding FaaS deployment environment. Replayable Execution exploits the intensive-deflated execution characteristics of the majority of target applications. It uses checkpointing to save an image of an application, allowing this image to be shared across containers and resulting in speedy restoration at service startup. We apply Replayable Execution to a representative FaaS Java framework to create a ReplayableJVM execution, which together with benefits from deterministic execution of a warmed up runtime, offers 2X memory footprint reduction, and over 10X startup time improvement. © 2019 Association for Computing Machinery.","Cloud Computing; Operating Systems; Programming Languages; Runtimes","Cloud computing; Computer operating systems; Computer programming languages; Check pointing; Deterministic execution; Language runtimes; Memory footprint; Runtime environments; Runtimes; Standard kernels; Target application; Distributed computer systems","Association for Computing Machinery, Inc","ACM Special Interest Group on Operating Systems (SIGOPS)","14th European Conference on Computer Systems, EuroSys 2019","25 March 2019 through 28 March 2019","Dresden","146334","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85063898722"
"Vazhkudai S.S.; Ma X.; Freeh V.W.; Strickland J.W.; Tammineedi N.; Scott S.L.","Vazhkudai, Sudharshan S. (6507061661); Ma, Xiaosong (8424540900); Freeh, Vincent W. (6603225952); Strickland, Jonathan W. (14026032000); Tammineedi, Nandan (15048850100); Scott, Stephen L. (55415716900)","6507061661; 8424540900; 6603225952; 14026032000; 15048850100; 55415716900","Freeloader: Scavenging desktop storage resources for scientific data","2005","Proceedings of the International Conference on Supercomputing","2005-November","","","","","","0","10.1109/SC.2005.27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117166109&doi=10.1109%2fSC.2005.27&partnerID=40&md5=be437dcb6a59b0fc391f9a3905f259d4","High-end computing is suffering a data deluge from experiments, simulations, and apparatus that creates overwhelming application dataset sizes. End-user workstations-despite more processing power than ever before-are ill-equipped to cope with such data demands due to insufficient secondary storage space and I/O rates. Meanwhile, a large portion of desktop storage is unused. We present the FreeLoader framework, which aggregates unused desktop storage space and I/O bandwidth into a shared cache/scratch space, for hosting large, immutable datasets and exploiting data access locality. Our experiments show that FreeLoader is an appealing low-cost solution to storing massive datasets, by delivering higher data access rates than traditional storage facilities. In particular, we present novel data striping techniques that allow FreeLoader to efficiently aggregate a workstation's network communication bandwidth and local I/O bandwidth. In addition, the performance impact on the native workload of donor machines is small and can be effectively controlled. © 2005 IEEE.","Distributed storage; Parallel I/O; Scientific data management; Serverless storage system; Storage cache; Storage scavenging; Striped storage","Aggregates; Bandwidth; Information management; Storage management; Distributed storage; Parallel I/O; Scientific data management; Serverless storage system; Storage caches; Storage resources; Storage scavenging; Storage spaces; Storage systems; Striped storage; Cache memory","Association for Computing Machinery","ACM; Association for Computing Machinery Special Interest Group on Computer Architecture (ACM SIGARCH); IEEE; IEEE Computer Society: Committees on Supercomputing Applications and Computer Architecture","2005 ACM/IEEE Conference on Supercomputing, SC 2005","12 November 2005 through 18 November 2005","Seattle","172417","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85117166109"
"Jain A.; Baarzi A.F.; Alfares N.; Kesidis G.; Urgaonkar B.; Kandemir M.","Jain, Aman (57205024886); Baarzi, Ata F. (57218214304); Alfares, Nader (57219230684); Kesidis, George (7003540724); Urgaonkar, Bhuvan (14020908300); Kandemir, Mahmut (35549787100)","57205024886; 57218214304; 57219230684; 7003540724; 14020908300; 35549787100","SpIitServe: Efficiently Splitting Complex Workloads across FaaS and IaaS","2019","SoCC 2019 - Proceedings of the ACM Symposium on Cloud Computing","","","","487","","","4","10.1145/3357223.3366027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091798971&doi=10.1145%2f3357223.3366027&partnerID=40&md5=6cae40a86494d8fa275f85f26312a665","Amazon Web Services (AWS) Lambdas and other ""cloud functions""(CFs) offer much lower startup latencies than virtual machines (VMs) (tens/hundreds of milliseconds vs. a few/several minutes) with lower minimum cost. This makes it appealing to use them for handling unexpected spikes in simple, stateless workloads [2, 3, 5]. If the spike persists, additional VMs may be launched and CFs can be decommissioned when the VMs are ready (VMs are cheaper per unit resource procured than CFs). However, it is not immediately clear if using CFs for complex workloads-those involving significant state exchange among components-is similarly effective. Current CFs have several restrictions that may limit their efficacy: (i) relatively limited resource capacity, especially main memory (e.g., an AWS Lambda may only have up to 3GB memory), (ii) limited lifetime (e.g., Lambdas are terminated after 15 minutes), and (iii) limited support for sharing of intermediate state (e.g., Lambdas must employ an external storage system such as AWS S3). Contrary to conventional wisdom, we show that it is possible to exploit the faster startup times of CFs to improve cost and performance of autoscaling even for complex workloads. Approach: We design SplitServe [1], implemented as an enhancement of Apache Spark [4], that is capable of simultaneously using AWS VMs and Lambdas for serving the tasks comprising a parallel Spark job. The most salient challenges addressed and design choices made in our efforts are: (i) State exchange: Instead of relying on a slower external cloud storage to transfer state, we leverage the resources associated with the procured VMs and employ HDFS for state exchange. We find that this allows both VMs and Lambdas to achieve throughputs close to that of local disks. Since we are using already provisioned disk capacity, we do not pay extra (as we would if we were to use, say, AWS S3). (ii) Segueing from Lambdas to newly available VMs: Simply killing ongoing tasks on Lambdas and rerunning them on newly available VMs triggers Spark's high overhead fault tolerance mechanisms. So, a diaphanous scheduling decision, based on the amount of time a Lambda function has been running, is made at per task granularity. Briefly, as the time since a Lambda was launched approaches the common-case startup delay for a VM, new tasks are not sent to the Lambda. Findings: In our experiments, we find that SplitServe reduces overall job execution time compared to the state of the art with either a homogeneous or heterogeneous execution environment, i.e., either all VMs or all Lambdas, or simultaneously involving both VMs and Lambdas to execute a job's tasks. For the heterogeneous case, our experimental evaluation of SplitServe using four different workloads (interactive TCP-DS, K-means clustering, PageRank, and Pi) shows that SplitServe-Spark improves performance up to 55% for workloads with small to modest amount of shuffling, and up to 31% in workloads with large amounts of shuffling, when compared to only VM based autoscaling. Also, with its novel segueing technique, SplitServe can help reduce costs by up to 21% while still providing almost 40% reduction in execution time. Ongoing Work: We are designing a comprehensive autoscaling system that leverages SplitServe's capabilities. We will carry out an empirical evaluation of the performance/cost improvements such a system can offer over state of the art solutions with diverse workloads that exhibit realistic dynamism and uncertainty. © 2019 Owner/Author.","","Cloud computing; Fault tolerance; K-means clustering; Virtual machine; Web services; Amazon web services; Empirical evaluations; Execution environments; Experimental evaluation; Fault tolerance mechanisms; Intermediate state; Resource capacity; Scheduling decisions; Function evaluation","Association for Computing Machinery","Amazon Web Services (AWS); Cisco; CockroachLabs; et al.; Google Cloud; Microsoft","10th ACM Symposium on Cloud Computing, SoCC 2019","20 November 2019 through 23 November 2019","Santa Cruz","161511","Conference paper","Final","","Scopus","2-s2.0-85091798971"
"","","","ICPE 2018 - Companion of the 2018 ACM/SPEC International Conference on Performance Engineering","2018","ICPE 2018 - Companion of the 2018 ACM/SPEC International Conference on Performance Engineering","2018-January","","","","","215","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051518784&partnerID=40&md5=a07736ae58198f43e2edf0cee33850b2","The proceedings contain 45 papers. The topics discussed include: the vision of self-aware reordering of security network function chains; package-aware scheduling of FaaS functions; better early than never: performance test acceleration by regression test selection; exploratory analysis of spark structured streaming; diagnosis of privacy and performance problems in the context of mobile applications; a workload-dependent performance analysis of an in-memory database in a multi-tenant configuration; towards scalability guidelines for semantic data container management; and challenges in automating performance tool support.","","","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC","9th ACM/SPEC International Conference on Performance Engineering, ICPE 2018","9 April 2018 through 13 April 2018","Berlin","135703","Conference review","Final","","Scopus","2-s2.0-85051518784"
"","","","Middleware 2017 - Proceedings of the 2017 International Middleware Conference","2017","Middleware 2017 - Proceedings of the 2017 International Middleware Conference","","","","","","264","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041240118&partnerID=40&md5=c82a5fc07814851d1fd84ea6da729fd5","The proceedings contain 20 papers. The topics discussed include: HyperDrive: exploring hyperparameters with pop scheduling; sieve: actionable insights from monitored metrics in distributed systems; Rafiki: a middleware for parameter tuning of NoSQL datastores for dynamic metagenomics workloads; Rivulet: a fault-tolerant platform for smart-home applications; binary compatible graphics support in Android for running iOS apps; Sense-Aid: a framework for enabling network as a service for participatory sensing; ORCA: an ORChestration automata for configuring VNFs; improving spark application throughput via memory aware task co-location: a mixture of experts approach; Swayam: distributed autoscaling to meet SLAs of machine learning inference services with resource efficiency; data-driven serverless functions for object storage; a programming model for application-defined multipath TCP scheduling; POLM2: automatic profiling for object lifetime-aware memory management for hotspot big data applications; SPECTRE: supporting consumption policies in window-based parallel complex event processing; efficient covering for top-k filtering in content-based publish/subscribe systems; StreamApprox: approximate computing for stream analytics; X-Search: revisiting private web search using Intel SGX; rectify: black-box intrusion recovery in PaaS Clouds; scheduler activations for interference-resilient SMP virtual machine scheduling; DoubleDecker: a cooperative disk caching framework for derivative clouds; and Ginja: one-dollar cloud-based disaster recovery for databases.","","","Association for Computing Machinery, Inc","Advanced Computing Systems Association (USENIX); Association for Computing Machinery (ACM); IBM; IFIP; Raytheon BBN; SIEMENS","18th ACM/IFIP/USENIX Middleware Conference, Middleware 2017","11 December 2017 through 15 December 2017","Las Vegas","133151","Conference review","Final","","Scopus","2-s2.0-85041240118"
"Grohmann J.; Nicholson P.K.; Iglesias J.O.; Kounev S.; Lugones D.","Grohmann, Johannes (57197744369); Nicholson, Patrick K. (35362255100); Iglesias, Jesus Omana (55001199100); Kounev, Samuel (23397538000); Lugones, Diego (35085333900)","57197744369; 35362255100; 55001199100; 23397538000; 35085333900","Monitorless: Predicting performance degradation in cloud applications with machine learning","2019","Middleware 2019 - Proceedings of the 2019 20th International Middleware Conference","","","","149","162","13","35","10.1145/3361525.3361543","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078008396&doi=10.1145%2f3361525.3361543&partnerID=40&md5=d5119ce10e29ec1dc590628a105417cb","Today, software operation engineers rely on application key performance indicators (KPIs) for sizing and orchestrating cloud resources dynamically. KPIs are monitored to assess the achievable performance and to configure various cloud-specific parameters such as flavors of instances and autoscaling rules, among others. Usually, keeping KPIs within acceptable levels requires application expertise which is expensive and can slow down the continuous delivery of software. Expertise is required because KPIs are normally based on application-specific quality-of-service metrics, like service response time and processing rate, instead of generic platform metrics, like those typical across various environments (e.g., CPU and memory utilization, I/O rate, etc.) In this paper, we investigate the feasibility of outsourcing the management of application performance from developers to cloud operators. In the same way that the serverless paradigm allows the execution environment to be fully managed by a third party, we discuss a monitorless model to streamline application deployment by delegating performance management. We show that training a machine learning model with platform-level data, collected from the execution of representative containerized services, allows inferring application KPI degradation. This is an opportunity to simplify operations as engineers can rely solely on platform metrics – while still fulfilling application KPIs – to configure portable and application agnostic rules and other cloud-specific parameters to automatically trigger actions such as autoscaling, instance migration, network slicing, etc. Results show that monitorless infers KPI degradation with an accuracy of 97% and, notably, it performs similarly to typical autoscaling solutions, even when autoscaling rules are optimally tuned with knowledge of the expected workload. © 2019 Association for Computing Machinery.","Cloud computing; DevOps; Machine learning; Monitoring","Benchmarking; Cloud computing; Environmental management; Learning systems; Machine learning; Middleware; Monitoring; Outsourcing; Quality of service; Achievable performance; Application deployment; Application performance; DevOps; Key performance indicators; Machine learning models; Performance degradation; Quality of service metrics; Application programs","Association for Computing Machinery, Inc","et al.; Hewlett-Packard Enterprise; NSF; Raytheon BBN Technologies; rti; SIEMENS","20th ACM/IFIP/USENIX Middleware Conference, Middleware 2019","9 December 2019 through 13 December 2019","Davis","156194","Conference paper","Final","","Scopus","2-s2.0-85078008396"
"Hafeez F.; Nasirifard P.; Jacobsen H.-A.","Hafeez, Faisal (57213119182); Nasirifard, Pezhman (6507596161); Jacobsen, Hans-Arno (7103073434)","57213119182; 6507596161; 7103073434","Demo abstract: A serverless approach to publish/subscribe systems","2018","Middleware 2018 - Proceedings of the 2018 ACM/IFIP/USENIX Middleware Conference (Posters)","","","","9","10","1","6","10.1145/3284014.3284019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061741466&doi=10.1145%2f3284014.3284019&partnerID=40&md5=af390bd96d1bca24a1cc047ddbc640ef","Building reliable and scalable publish/subscribe (pub/sub) systems require tremendous development efforts. The serverless paradigm simplifies the development and deployment of highly available applications by delegating most of the operational concerns to the cloud providers. The serverless paradigm describes a programming model, where the developers break the application downs into smaller microservices which run on the cloud in response to events. In this paper, we propose a design of a serverless pub/sub system based on the Amazon Web Services Lambdas and Microsoft Azure Functions. Our pub/sub system performs topic-based, content-based and function-based matchings. The function-based matching is a novel matching approach where the subscribers can define highly customizable subscription function which the broker applies to the publications in the cloud.We also provide an evaluation application for investigating the scalability of the designed brokers on different serverless platforms. © 2018 Association for Computing Machinery.","Contentbased pub/sub; Function as a service (faas); Function-based pub/sub; Serverless; Topic-based pub/sub","Middleware; Windows operating system; Amazon web services; Cloud providers; Programming models; Pub/sub; Pub/sub systems; Publish/subscribe; Publish/Subscribe system; Serverless; Web services","Association for Computing Machinery, Inc","ACM","19th ACM/IFIP/USENIX Middleware Conference, Middleware 2018","10 December 2018 through 14 December 2018","Rennes","144676","Conference paper","Final","","Scopus","2-s2.0-85061741466"
"Hwang K.; Jin H.; Ho R.","Hwang, Kai (56699307700); Jin, Hai (56434989100); Ho, Roy (7201453501)","56699307700; 56434989100; 7201453501","RAID-x: A new distributed disk array for I/O-centric cluster computing","2000","Proceedings of the IEEE International Symposium on High Performance Distributed Computing","2000-January","","868660","279","286","7","36","10.1109/HPDC.2000.868660","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033720549&doi=10.1109%2fHPDC.2000.868660&partnerID=40&md5=0815792f9222aa9076b53532cbb46368","A new RAID-x (redundant array of inexpensive disks at level x) architecture is presented for distributed I/O processing on a serverless cluster of computers. The RAID-x architecture is based on a new concept of orthogonal striping and mirroring (OSM) across all distributed disks in the cluster. The primary advantages of this OSM approach lie in: (1) a significant improvement in parallel I/O bandwidth; (2) hiding disk mirroring overhead in the background; and (3) greatly enhanced scalability and reliability in cluster computing applications. All claimed advantages are substantiated with benchmark performance results on the Trojans cluster built at USC in 1999. The authors discuss the issues of scalable I/O performance, enhanced system reliability, and striped checkpointing on distributed RAID-x in a serverless cluster environment. © 2000 IEEE.","Bandwidth; Checkpointing; Computer applications; Computer architecture; Concurrent computing; Distributed computing; Linux; Packaging; Reliability; Scalability","Bandwidth; Benchmarking; Cluster computing; Computer applications; Computer architecture; Computer operating systems; Computer software maintenance; Linux; Nonvolatile storage; Packaging; Reliability; Scalability; Computer architecture; Input output programs; Interconnection networks; Internet; Magnetic disk storage; Personal computers; UNIX; All distributed; Check pointing; Cluster environments; Computing applications; Concurrent computing; Distributed I/O; Distributed RAID; System reliability; Centric cluster computing; Distributed disk array; Orthogonal striping and mirroring; Redundant array of inexpensive disk; Distributed computer systems; Parallel processing systems","Institute of Electrical and Electronics Engineers Inc.","IEEE Computer Society Technical Committee on Distributed Processing; University of Arizona ECE Dept.","9th IEEE International Symposium on High-Performance Distributed Computing, HPDC 2000","1 August 2000 through 4 August 2000","Pittsburgh","116574","Conference paper","Final","","Scopus","2-s2.0-0033720549"
"Carreira J.; Fonseca P.; Tumanov A.; Zhang A.; Katz R.","Carreira, Joao (59112718200); Fonseca, Pedro (56343089800); Tumanov, Alexey (18435442000); Zhang, Andrew (57219226689); Katz, Randy (7401788602)","59112718200; 56343089800; 18435442000; 57219226689; 7401788602","Cirrus: A Serverless Framework for End-To-end ML Workflows","2019","SoCC 2019 - Proceedings of the ACM Symposium on Cloud Computing","","","","13","24","11","163","10.1145/3357223.3362711","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086244066&doi=10.1145%2f3357223.3362711&partnerID=40&md5=93d14acd78be426e6b2361507438ea14","Machine learning (ML) workflows are extremely complex. The typical workflow consists of distinct stages of user interaction, such as preprocessing, training, and tuning, that are repeatedly executed by users but have heterogeneous computational requirements. This complexity makes it challenging for ML users to correctly provision and manage resources and, in practice, constitutes a significant burden that frequently causes over-provisioning and impairs user productivity. Serverless computing is a compelling model to address the resource management problem, in general, but there are numerous challenges to adopt it for existing ML frameworks due to significant restrictions on local resources. This work proposes Cirrus-An ML framework that automates the end-To-end management of datacenter resources for ML workflows by efficiently taking advantage of serverless infrastructures. Cirrus combines the simplicity of the serverless interface and the scalability of the serverless infrastructure (AWS Lambdas and S3) to minimize user effort. We show a design specialized for both serverless computation and iterative ML training is needed for robust and efficient ML training on serverless infrastructure. Our evaluation shows that Cirrus outperforms frameworks specialized along a single dimension: Cirrus is 100x faster than a general purpose serverless system [36] and 3.75x faster than specialized ML frameworks for traditional infrastructures [49]. © 2019 ACM.","Distributed Computing; Machine Learning; Serverless","Computer programming; Computer science; Computational requirements; End-to-End management; Local resources; Over provisioning; Resource management problems; Serverless systems; User interaction; User productivity; Cloud computing","Association for Computing Machinery","Amazon Web Services (AWS); Cisco; CockroachLabs; et al.; Google Cloud; Microsoft","10th ACM Symposium on Cloud Computing, SoCC 2019","20 November 2019 through 23 November 2019","Santa Cruz","161511","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85086244066"
"Patel H.; Jindal A.; Szyperski C.","Patel, Hiren (57210068008); Jindal, Alekh (53881488400); Szyperski, Clemens (6602166242)","57210068008; 53881488400; 6602166242","Big Data Processing at Microsoft: Hyper Scale, Massive Complexity, and Minimal Cost","2019","SoCC 2019 - Proceedings of the ACM Symposium on Cloud Computing","","","","490","","","5","10.1145/3357223.3366029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091790347&doi=10.1145%2f3357223.3366029&partnerID=40&md5=a0d3b6ecbab34674bfb607bdb0cc674d","The past decade has seen a tremendous interest in large-scale data processing at Microsoft. Typical scenarios include building business-critical pipelines such as advertiser feedback loop, index builder, and relevance/ranking algorithms for Bing; analyzing user experience telemetry for Office, Windows or Xbox; and gathering recommendations for products like Windows and Xbox. To address these needs a first-party big data analytics platform, referred to as Cosmos, was developed in the early 2010s at Microsoft. Cosmos makes it possible to store data at exabyte scale and process in a serverless form factor, with SCOPE [4] being the query processing workhorse. Over time, however, several newer challenges have emerged, requiring major technical innovations in Cosmos to meet these newer demands. In this abstract, we describe three such challenges from the query processing viewpoint, and our approaches to handling them. Hyper Scale. Cosmos has witnessed a significant growth in usage from its early days, from the number of customers (starting from Bing to almost every single business unit at Microsoft today), to the volume of data processed (from petabytes to exabytes today), to the amount of processing done (from tens of thousands of SCOPE jobs to hundreds of thousands of jobs today, across hundreds of thousands of machines). Even a single job can consume tens of petabytes of data and produce similar volumes of data by running millions of tasks in parallel. Our approach to handle this unprecedented scale is two fold. First, we decoupled and disaggregated the query processor from storage and resource management components, thereby allowing different components in the Cosmos stack to scale independently. Second, we scaled the data movement in the SCOPE query processor with quasilinear complexity [2]. This is crucial since data movement is often the most expensive step, and hence the bottleneck, in massive-scale data processing. Massive Complexity. Cosmos workloads are also highly complex. Thanks to adoption across the whole of Microsoft, Cosmos needs to support workloads that are representative of multiple industry segments, including search engine (Bing), operating system (Windows), workplace productivity (Office), personal computing (Surface), gaming (XBox), etc. To handle such diverse workloads, our approach has been to provide a one-size-fits-All experience. First of all, to make it easy for the customers to express their computations, SCOPE supports different types of queries, from batch to interactive to streaming and machine learning. Second, SCOPE supports both structured and unstructured data processing. Likewise, multiple data formats, including both propriety and open source source such as Parquet, are supported. Third, users can write business logic using a mix of declarative and imperative languages, over even different imperative languages such as C# and Python, in the same job. Furthermore, users can express all of the above in simple data flow style computation for better readability and maintainability. Finally, considering the diverse workload mix inside Microsoft, we have come to realization that it is not possible to fits all scenarios using SCOPE. Therefore, we also support the popular Spark query processing engine. Overall, the one-size-fits-All query processing experience in Cosmos covers very diverse workloads, including data formats, programming languages, and the backend engines. Minimal Cost. While scale and complexity are hard by themselves, the biggest challenge is to achieve all of that at minimal cost. In fact, there is a pressing need to improve Cosmos efficiency and reduce operational costs. This is challenging due to several reasons. First, optimizing a SCOPE job is hard considering that the SCOPE DAGs are super large (up to 1000s of operators in single job!), and the optimization estimates (cardinality, cost, etc.) are often way off from the actuals. Second, SCOPE optimizes a given query, while the operational costs depend on the overall workload. Therefore workload optimization becomes very important. And finally, SCOPE jobs are typically interlinked in data pipelines, i.e., the output of one job is consumed by other jobs. This means that workload optimization needs to be aware of these dependencies. Our approach is to develop a feedback loop to learn from past workloads in order to optimize the future ones. Specifically, we leverage machine learning to learn models for optimizing individual jobs [3], apply multi-query optimizations to optimize the costs of overall workload [1], and build dependency graphs to identify and optimize for the data pipelines. © 2019 Owner/Author.","","Advanced Analytics; Big data; Cloud computing; Cost benefit analysis; Data Analytics; Data handling; Digital storage; Feedback; Machine learning; Object oriented programming; Open source software; Personal computers; Personal computing; Pipelines; Query processing; User experience; Windows operating system; Imperative languages; Large-scale data processing; Multiquery optimization; Query processing engine; Resource management; Technical innovation; Unstructured data; Workload optimizations; Search engines","Association for Computing Machinery","Amazon Web Services (AWS); Cisco; CockroachLabs; et al.; Google Cloud; Microsoft","10th ACM Symposium on Cloud Computing, SoCC 2019","20 November 2019 through 23 November 2019","Santa Cruz","161511","Conference paper","Final","","Scopus","2-s2.0-85091790347"
"Kuntsevich A.; Nasirifard P.; Jacobsen H.-A.","Kuntsevich, Aleksandr (57206479593); Nasirifard, Pezhman (6507596161); Jacobsen, Hans-Arno (7103073434)","57206479593; 6507596161; 7103073434","Demo abstract: A distributed analysis and benchmarking framework for apache openwhisk serverless platform","2018","Middleware 2018 - Proceedings of the 2018 ACM/IFIP/USENIX Middleware Conference (Posters)","","","","3","4","1","25","10.1145/3284014.3284016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061715251&doi=10.1145%2f3284014.3284016&partnerID=40&md5=488de3a282553606fa7e9baedd318cf2","Serverless computing simplifies the life cycle of scalable web applications, through delegating most of the operational concerns to the cloud providers. One prominent serverless platform is Apache OpenWhisk which is employed by IBM Cloud. Despite the apparent benefits of serverless computing, some limitations of the serverless platform, such as the state-less nature of serverless functions, can introduce scalability bottlenecks. In this work, we propose an analysis and benchmarking approach for investigating potential bottlenecks and limitations of Apache OpenWhisk serverless platform. © 2018 Association for Computing Machinery.","Apache openwhisk; Benchmarking; Function as a service (faas); Serverless","Life cycle; Middleware; Apache openwhisk; Cloud providers; Distributed analysis; Serverless; WEB application; Benchmarking","Association for Computing Machinery, Inc","ACM","19th ACM/IFIP/USENIX Middleware Conference, Middleware 2018","10 December 2018 through 14 December 2018","Rennes","144676","Conference paper","Final","","Scopus","2-s2.0-85061715251"
"Marroquín R.; Müller I.; Makreshanski D.; Alonso G.","Marroquín, Renato (57198767118); Müller, Ingo (57197918389); Makreshanski, Darko (55800405100); Alonso, Gustavo (7102787521)","57198767118; 57197918389; 55800405100; 7102787521","Pay one, get hundreds for free: Reducing cloud costs through shared query execution","2018","SoCC 2018 - Proceedings of the 2018 ACM Symposium on Cloud Computing","","","","439","450","11","11","10.1145/3267809.3267822","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059020022&doi=10.1145%2f3267809.3267822&partnerID=40&md5=8edc8d4be2768a7f80911ae1aefbb5f0","Cloud-based data analysis is nowadays common practice because of the lower system management overhead as well as the pay-as-you-go pricing model. The pricing model, however, is not always suitable for query processing as heavy use results in high costs. For example, in query-as-a-service systems, where users are charged per processed byte, collections of queries accessing the same data frequently can become expensive. The problem is compounded by the limited options for the user to optimize query execution when using declarative interfaces such as SQL. In this paper, we show how, without modifying existing systems and without the involvement of the cloud provider, it is possible to significantly reduce the overhead, and hence the cost, of query-as-a-service systems. Our approach is based on query rewriting so that multiple concurrent queries are combined into a single query. Our experiments show the aggregated amount of work done by the shared execution is smaller than in a query-at-a-time approach. Since queries are charged per byte processed, the cost of executing a group of queries is often the same as executing a single one of them. As an example, we demonstrate how the shared execution of the TPC-H benchmark is up to 100x and 16x cheaper in Amazon Athena and bigquery than using a query-at-a-time approach while achieving a higher throughput. © 2018 IW3C2 (International World Wide Web Conference Committee).","Cloud Computing; Data Warehouse; Query Processing; Serverless; Shared Workload Execution","Cloud computing; Cost reduction; Data warehouses; Information management; Query processing; Common practices; Existing systems; Query rewritings; Serverless; Service systems; System management; Tpc-h benchmarks; Workload execution; Search engines","Association for Computing Machinery, Inc","ACM SIGMOD; ACM SIGOPS","2018 ACM Symposium on Cloud Computing, SoCC 2018","11 October 2018 through 13 October 2018","Carlsbad","142611","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85059020022"
"","","","SoCC 2019 - Proceedings of the ACM Symposium on Cloud Computing","2019","SoCC 2019 - Proceedings of the ACM Symposium on Cloud Computing","","","","","","498","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091756864&partnerID=40&md5=f45421ec9eb18e916b7de3bc095bf18e","The proceedings contain 52 papers. The topics discussed include: a system-wide debugging assistant powered by natural language processing; narrowing the gap between serverless and its state with storage functions; Cirrus: a serverless framework for end-to-end ML workflows; Cartel: a system for collaborative transfer learning at the edge; HyperSched: dynamic resource reallocation for model development on a deadline; lessons from large-scale software as a service at databricks; BurScale: using burstable instances for cost-effective autoscaling in the public cloud; characterizing and synthesizing task dependencies of data-parallel jobs in Alibaba cloud; and an automated, cross-layer instrumentation framework for diagnosing performance problems in distributed applications.","","","Association for Computing Machinery","Amazon Web Services (AWS); Cisco; CockroachLabs; et al.; Google Cloud; Microsoft","10th ACM Symposium on Cloud Computing, SoCC 2019","20 November 2019 through 23 November 2019","Santa Cruz","161511","Conference review","Final","","Scopus","2-s2.0-85091756864"
"Kaffes K.; Yadwadkar N.J.; Kozyrakis C.","Kaffes, Kostis (57212510735); Yadwadkar, Neeraja J. (56429519900); Kozyrakis, Christos (6602525246)","57212510735; 56429519900; 6602525246","Centralized Core-granular Scheduling for Serverless Functions","2019","SoCC 2019 - Proceedings of the ACM Symposium on Cloud Computing","","","","158","164","6","91","10.1145/3357223.3362709","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091770578&doi=10.1145%2f3357223.3362709&partnerID=40&md5=55e6a8163cf50fe7af99df21b5104efe","In recent years, many applications have started using serverless computing platforms primarily due to the ease of deployment and cost efficiency they offer. However, the existing scheduling mechanisms of serverless platforms fall short in catering to the unique characteristics of such applications: burstiness, short and variable execution times, statelessness and use of a single core. Specifically, the existing mechanisms fall short in meeting the requirements generated due to the combined effect of these characteristics: scheduling at a scale of millions of function invocations per second while achieving predictable performance. In this paper, we argue for a cluster-level centralized and core-granular scheduler for serverless functions. By maintaining a global view of the cluster resources, the centralized approach eliminates queue imbalances while the core granularity reduces interference; together these properties enable reduced performance variability. We expect such a scheduler to increase the adoption of serverless computing platforms by various latency and throughput sensitive applications. © 2019 ACM.","cloud computing; resource allocation; scheduling; serverless computing","Cloud computing; Centralized approaches; Combined effect; Computing platform; Cost efficiency; Performance variability; Scheduling mechanism; Sensitive application; Variable execution time; Scheduling","Association for Computing Machinery","Amazon Web Services (AWS); Cisco; CockroachLabs; et al.; Google Cloud; Microsoft","10th ACM Symposium on Cloud Computing, SoCC 2019","20 November 2019 through 23 November 2019","Santa Cruz","161511","Conference paper","Final","","Scopus","2-s2.0-85091770578"
"","","","Middleware 2017 - Proceedings of the 2017 Middleware Posters and Demos 2017: Proceedings of the Posters and Demos Session of the 18th International Middleware Conference","2017","Middleware 2017 - Proceedings of the 2017 Middleware Posters and Demos 2017: Proceedings of the Posters and Demos Session of the 18th International Middleware Conference","","","","","","29","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046069894&partnerID=40&md5=1387e5652fc82bb747a258664e59fecd","The proceedings contain 12 papers. The topics discussed include: a software-defined hybrid cache with reduced energy; realizing an elastic MEMcached via cached data migration; a shared multi-stakeholder platform for IoT; Drowsy-DC, data-center power management inspired by smartphones; abstractions for supercomputing platforms to facilitate consolidation of services; improving server utilization via resource-adaptive batch VMS; programming application-defined multipath TCP schedulers; HyperPubSub: a decentralized, permissioned, publish/subscribe service using blockchains; combat state-aware interest management for online games; VIBES: fast blockchain simulations for large-scale peer-to-peer networks; MoCA+: incorporating user modeling into mobile contextual advertising; and a serverless topic-based and content-based pub/sub broker.","","","Association for Computing Machinery, Inc","ACM; IFIP; The Advanced Computing Systems Association (USENIX)","18th ACM/IFIP/USENIX International Middleware Conference, Middleware 2017","11 December 2017 through 15 December 2017","Las Vegas","133164","Conference review","Final","","Scopus","2-s2.0-85046069894"
"Jindal S.; Ricci R.","Jindal, Sonika (57537397500); Ricci, Robert (7103409000)","57537397500; 7103409000","MME-FaaS Cloud-Native Control for Mobile Networks","2019","SoCC 2019 - Proceedings of the ACM Symposium on Cloud Computing","","","","152","157","5","4","10.1145/3357223.3362722","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091743595&doi=10.1145%2f3357223.3362722&partnerID=40&md5=5215ac6dd239fbe45b32519798483b39","The control plane for mobile wireless (eg. cellular) networks faces challenges with respect to scaling, robustness, and handling of bursty traffic. In this paper, we take a cloud-native approach to building a mobile control plane, employing a design that maps transitions of device state to serverless functions. Using a prototype of the LTE/EPC Mobility Management Entity (MME), we demonstrate how to architect a mobile control plane using serverless computing primitives. We demonstrate the practicality of this approach, which differs significantly from designs based on traditional telecom infrastructure. © 2019 ACM.","cloud-native; FaaS; MME; mobile networks; Serverless","Cloud computing; Bursty traffic; Control planes; Mobile controls; Mobile wireless; Mobility management; Telecom infrastructures; Mobile telecommunication systems","Association for Computing Machinery","Amazon Web Services (AWS); Cisco; CockroachLabs; et al.; Google Cloud; Microsoft","10th ACM Symposium on Cloud Computing, SoCC 2019","20 November 2019 through 23 November 2019","Santa Cruz","161511","Conference paper","Final","","Scopus","2-s2.0-85091743595"
"Abad C.L.; Boza E.F.; Eyk E.V.","Abad, Cristina L. (22957218500); Boza, Edwin F. (57196238001); Eyk, Erwin van (57203098031)","22957218500; 57196238001; 57203098031","Package-aware scheduling of FaaS functions","2018","ICPE 2018 - Companion of the 2018 ACM/SPEC International Conference on Performance Engineering","2018-January","","","101","106","5","32","10.1145/3185768.3186294","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052016570&doi=10.1145%2f3185768.3186294&partnerID=40&md5=fcc93d89492fb2b2746c2e23f1f30694","We consider the problem of scheduling small cloud functions on serverless computing platforms. Fast deployment and execution of these functions is critical, for example, for microservices architectures. However, functions that require large packages or libraries are bloated and start slowly. A solution is to cache packages at the worker nodes instead of bundling them with the functions. However, existing FaaS schedulers are vanilla load balancers, agnostic of any packages that may have been cached in response to prior function executions, and cannot reap the benefits of package caching (other than by chance). To address this problem, we propose a package-aware scheduling algorithm that tries to assign functions that require the same package to the same worker node. Our algorithm increases the hit rate of the package cache and, as a result, reduces the latency of the cloud functions. At the same time, we consider the load sustained by the workers and actively seek to avoid imbalance beyond a configurable threshold. Our preliminary evaluation shows that, even with our limited exploration of the configuration space so-far, we can achieve 66% performance improvement at the cost of a (manageable) higher node imbalance. © 2018 Association for Computing Machinery.","Cloud computing; Functions-as-a-service; Load balancing; Scheduling; Serverless computing","Cloud computing; Distributed computer systems; Resource allocation; Scheduling; Computing platform; Configuration space; Fast deployments; Load balancer; Microservices; Performance improvements; Prior functions; Serverless computing; Scheduling algorithms","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC","9th ACM/SPEC International Conference on Performance Engineering, ICPE 2018","9 April 2018 through 13 April 2018","Berlin","135703","Conference paper","Final","","Scopus","2-s2.0-85052016570"
"","","","Middleware Industry 2018 - Proceedings of the 2018 ACM/IFIP/USENIX Middleware Conference (Industrial Track)","2018","Middleware Industry 2018 - Proceedings of the 2018 ACM/IFIP/USENIX Middleware Conference (Industrial Track)","","","","","","63","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061805082&partnerID=40&md5=0b4f3c6d12a47f3d234ff2707a28a12a","The proceedings contain 8 papers. The topics discussed include: serverless data analytics in the IBM cloud; the design and implementation of a real time visual search system on JD E-commerce platform; exploratory study of privacy preserving fraud detection; NBWGuard: realizing network QoS for kubernetes; a high performance, scalable DNS service for very large scale container cloud platforms; resource fairness and prioritization of transactions in permissioned blockchain systems (industry; and BcWAN: a federated low-power WAN for the Internet of things.","","","Association for Computing Machinery, Inc","ACM","19th ACM IFIP USENIX Middleware Conference, Middleware 2018","10 December 2018 through 14 December 2018","Rennes","144677","Conference review","Final","","Scopus","2-s2.0-85061805082"
"Van Eyk E.; Iosup A.; Abad C.L.; Grohmann J.; Eismann S.","Van Eyk, Erwin (57203098031); Iosup, Alexandru (23392350500); Abad, Cristina L. (22957218500); Grohmann, Johannes (57197744369); Eismann, Simon (57203095294)","57203098031; 23392350500; 22957218500; 57197744369; 57203095294","A SPEC RG cloud group’s vision on the performance challenges of FaaS cloud architectures","2018","ICPE 2018 - Companion of the 2018 ACM/SPEC International Conference on Performance Engineering","2018-January","","","21","24","3","66","10.1145/3185768.3186308","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050561054&doi=10.1145%2f3185768.3186308&partnerID=40&md5=052b7b321dd10100d0b70db2985bd96f","As a key part of the serverless computing paradigm, Function-as-a-Service (FaaS) platforms enable users to run arbitrary functions without being concerned about operational issues. However, there are several performance-related issues surrounding the state-of-the-art FaaS platforms that can deter widespread adoption of FaaS, including sizeable overheads, unreliable performance, and new forms of the cost-performance trade-off. In this work we, the SPEC RG Cloud Group, identify six performance-related challenges that arise specifically in this FaaS model, and present our roadmap to tackle these problems in the near future. This paper aims at motivating the community to solve these challenges together. © 2018 Copyright held by the owner/author(s).","","Engineering; Industrial engineering; Arbitrary functions; Cloud architectures; Computing paradigm; Cost performance; New forms; Operational issues; Performance challenges; State of the art; Economic and social effects","Association for Computing Machinery, Inc","ACM SIGMETRICS; ACM SIGSOFT; SPEC","9th ACM/SPEC International Conference on Performance Engineering, ICPE 2018","9 April 2018 through 13 April 2018","Berlin","135703","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85050561054"
"Kim J.; Lee K.","Kim, Jeongchul (57210793873); Lee, Kyungyong (57196193080)","57210793873; 57196193080","Practical Cloud Workloads for Serverless FaaS","2019","SoCC 2019 - Proceedings of the ACM Symposium on Cloud Computing","","","","477","","","37","10.1145/3357223.3365439","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091129148&doi=10.1145%2f3357223.3365439&partnerID=40&md5=f15e01747a88e65bac71657edd8650ff","Serverless computing is gaining popularity with the Function-Asa-Service (FaaS) execution model. Without incurring overheads involved in provisioning cloud instances and with high availability and scalability, serverless computing allows developers to focus on implementation of core application logic using other well-developed cloud services. By abstracting the complex resource management task, serverless computing opens new opportunities for the cloud service adoption even to non-cloud experts [2]. With the popularity, many research results have been published using the FaaS execution model. They include investigation of serverless computing opportunities [1], proposing new serverless applications, function run-Time optimization, and public service comparison. Without a common test benchmark suite, authors in the previous work had evaluated proposed systems using fairly simple FaaS applications, such as micro-benchmarks that emphasize specific resources exclusively, e.g., CPU, disk I/O, and network. However, such simple workloads do not represent realistic FaaS system applications, and the evaluations might not compare proposed systems appropriately. To overcome the limitation of lacking a comprehensive benchmark suite for the serverless computing and FaaS execution model, the authors create FunctionBench that provides various FaaS workloads that are ready to be executed on public cloud function execution services-AWS Lambda, Google Cloud Functions, and Azure functions1. Since the inception of serving the FaaS workloads, we keep working to expand the supported applications and add scenarios in big-data processing, back-end web applications, and security. To represent big-data applications, we add a MapReduce WordCount workload, which counts the number of occurrences of each word in a given partitioned input dataset from Wikipedia. To cover web back-end applications, we add Chameleon. The application renders a template using the Chameleon module in Python PIP library to create an HTML table of N rows and M columns that are provided as input arguments. Another web-related application is JSON serialize-deserialize module. The application performs JSON deserialization using a JSON-encoded string dataset (Awesome JSON Dataset) downloaded from a public object storage service, and it serializes the JSON object again. To represent security-related applications, we add Pyaes benchmark that performs private key-based encryption and decryption. It is a pure-Python implementation of the AES block-cipher algorithm in CTR mode. We also add gzip-compression benchmark to represent realistic disk IO-heavy applications. The degree (High, Medium, Low) of resource usage characteristics of newly proposed applications are summarized in Table 1. Please refer to [3] to read the description of comprehensive applications list. The proposed FunctionBench provides a variety of FaaS applications in multiple categories, and we are sure that it will enable fair evaluation of new research work in the relevant field with practical application scenarios. © 2019 Owner/Author.","benchmark; cloud computing; faas; serverless; workload","Benchmarking; Computation theory; Cryptography; Data handling; Digital storage; Function evaluation; High level languages; Large dataset; Web services; Application logic; Application scenario; Big data applications; Encryption and decryption; High availability; Resource management; Runtime optimization; System applications; Storage as a service (STaaS)","Association for Computing Machinery","Amazon Web Services (AWS); Cisco; CockroachLabs; et al.; Google Cloud; Microsoft","10th ACM Symposium on Cloud Computing, SoCC 2019","20 November 2019 through 23 November 2019","Santa Cruz","161511","Conference paper","Final","","Scopus","2-s2.0-85091129148"
"White B.S.; Walker M.; Humphrey M.; Grimshaw A.S.","White, Brian S. (57213235392); Walker, Michael (57672231000); Humphrey, Marty (7102783804); Grimshaw, Andrew S. (35617732400)","57213235392; 57672231000; 7102783804; 35617732400","LegionFS: A Secure and Scalable File System Supporting Cross-Domain High-Performance Applications","2001","Proceedings of the International Conference on Supercomputing","","","","59","","","41","10.1145/582034.582093","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77049124361&doi=10.1145%2f582034.582093&partnerID=40&md5=6905d10603cda6741dc1a6153e1cb672","Realizing that current file systems can not cope with the diverse requirements of wide-area collaborations, researchers have developed data access facilities to meet their needs. Recent work has focused on comprehensive data access architectures. In order to fulfill the evolving requirements in this environment, we suggest a more fully-integrated architecture built upon the fundamental tenets of naming, security, scalability, extensibility, and adaptability. These form the underpinning of the Legion File System (LegionFS). This paper motivates the need for these requirements and presents benchmarks that highlight the scalability of LegionFS. LegionFS aggregate throughput follows the linear growth of the network, yielding an aggregate read bandwidth of 193.8 MB/s on a 100 Mbps Ethernet backplane with 50 simultaneous readers. The serverless architecture of LegionFS is shown to benefit important scientific applications, such as those accessing the Protein Data Bank, within both local- and wide-area environments. © 2001 ACM.","","Aggregates; Computer architecture; Network architecture; Scalability; 'current; Access facilities; Aggregate throughput; Cross-domain; Data access; Filesystem; Fully integrated; High performance applications; Integrated architecture; Linear growth; File organization","Association for Computing Machinery","The Association for Computing Machinery, Special Interest Group on Computer Architecture (ACM SIGARCH); The IEEE Computer Society","2001 ACM/IEEE Conference on Supercomputing, SC 2001","10 November 2001 through 16 November 2001","Denver","178964","Conference paper","Final","","Scopus","2-s2.0-77049124361"
"Sampé J.; Vernik G.; Sánchez-Artigas M.; García-López P.","Sampé, Josep (57053360400); Vernik, Gil (27068121500); Sánchez-Artigas, Marc (10939155200); García-López, Pedro (24479469800)","57053360400; 27068121500; 10939155200; 24479469800","Serverless data analytics in the IBM cloud","2018","Middleware Industry 2018 - Proceedings of the 2018 ACM/IFIP/USENIX Middleware Conference (Industrial Track)","","","","1","8","7","78","10.1145/3284028.3284029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061789926&doi=10.1145%2f3284028.3284029&partnerID=40&md5=a0de6b43ddbebaedf7685d315c6b64f4","Unexpectedly, the rise of serverless computing has also collaterally started the “democratization” of massive-scale data parallelism. This new trend heralded by PyWren pursues to enable untrained users to execute single-machine code in the cloud at massive scale through platforms like AWS Lambda. Inspired by this vision, this industry paper presents IBM-PyWren, which continues the pioneering work begun by PyWren in this field. It must be noted that IBM-PyWren is not, however, just a mere reimplementation of PyWren’s API atop IBM Cloud Functions. Rather, it is must be viewed as an advanced extension of PyWren to run broader MapReduce jobs. We describe the design, innovative features (API extensions, data discovering & partitioning, composability, etc.) and performance of IBM-PyWren, along with the challenges encountered during its implementation. © 2018 Association for Computing Machinery.","Distributed computing; IBM cloud functions; IBM cloud object storage; PyWren; Serverless computing","Data Analytics; Distributed computer systems; Middleware; Composability; Data parallelism; Map-reduce; Object storages; PyWren; Serverless computing; Single- machines; Digital storage","Association for Computing Machinery, Inc","ACM","19th ACM IFIP USENIX Middleware Conference, Middleware 2018","10 December 2018 through 14 December 2018","Rennes","144677","Conference paper","Final","","Scopus","2-s2.0-85061789926"
"","","","Middleware 2018 - Proceedings of the 2018 ACM/IFIP/USENIX Middleware Conference (Posters)","2018","Middleware 2018 - Proceedings of the 2018 ACM/IFIP/USENIX Middleware Conference (Posters)","","","","","","27","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061700486&partnerID=40&md5=f3b2055fc2ede2391ec34328550ea413","The proceedings contain 12 papers. The topics discussed include: a distributed analysis and benchmarking framework for apache OpenWhisk serverless platform; attack and vulnerability simulation framework for bitcoin-like blockchain technologies; CIDDS: a configurable and distributed DAG-based distributed ledger simulation framework; a serverless approach to publish/subscribe systems; eVIBES: configurable and interactive ethereum blockchain simulation framework; applying web-technologies for device state processing in IoT middleware; and kernel packet processing for manycore systems.","","","Association for Computing Machinery, Inc","ACM","19th ACM/IFIP/USENIX Middleware Conference, Middleware 2018","10 December 2018 through 14 December 2018","Rennes","144676","Conference review","Final","","Scopus","2-s2.0-85061700486"
"Yan M.; Castro P.; Cheng P.; Ishakian V.","Yan, Mengting (59033765200); Castro, Paul (7102781769); Cheng, Perry (54790519500); Ishakian, Vatche (36630445800)","59033765200; 7102781769; 54790519500; 36630445800","Building a chatbot with serverless computing","2016","Proceedings of the 1st International Workshop on Mashups of Things and APIs, MOTA 2016","","","5","","","","107","10.1145/3007203.3007217","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010402842&doi=10.1145%2f3007203.3007217&partnerID=40&md5=28f9808e5f94d7363cdde2638a9abc31","Chatbots are emerging as the newest platform used by millions of consumers worldwide due in part to the commoditization of natural language services, which provide provide developers with many building blocks to create chatbots inexpensively. However, it is still difficult to build and deploy chatbots. Developers need to handle the coordination of the cognitive services to build the chatbot interface, integrate the chatbot with external services, and worry about extensibility, scalability, and maintenance. In this work, we present the architecture and prototype of a chatbot using a serverless platform, where developers compose stateless functions together to perform useful actions. We describe our serverless architecture based on function sequences, and how we used these functions to coordinate the cognitive microservices in the Watson Developer Cloud to allow the chatbot to interact with external services. The serverless model improves the extensibility of our chatbot, which currently supports 6 abilities: location based weather reports, jokes, date, reminders, and a simple music tutor. © is held by the owner/author(s).","Bots; Cloud computing; FaaS; Serverless","Cloud computing; Bots; Building blockes; Commoditization; FaaS; Function sequence; Natural languages; nocv2; Serverless; Serverless architecture; Middleware","Association for Computing Machinery, Inc","Assoc. for Computing Machinery (ACM); IFIP; The Advanced Computing System Association (USENIX)","1st International Workshop on Mashups of Things and APIs, MOTA 2016, Co-located with 17th International Middleware Conference","12 December 2016 through 16 December 2016","Trento","125682","Conference paper","Final","","Scopus","2-s2.0-85010402842"
"Ao L.; Voelker G.M.; Izhikevich L.; Porter G.","Ao, Lixiang (56482929900); Voelker, Geoffrey M. (7003306507); Izhikevich, Liz (57205202194); Porter, George (36442743000)","56482929900; 7003306507; 57205202194; 36442743000","Sprocket: A serverless video processing framework","2018","SoCC 2018 - Proceedings of the 2018 ACM Symposium on Cloud Computing","","","","263","274","11","162","10.1145/3267809.3267815","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059032967&doi=10.1145%2f3267809.3267815&partnerID=40&md5=83ed319957e88d4c1e6baa7ed90e7c33","Sprocket is a highly configurable, stage-based, scalable, serverless video processing framework that exploits intra-video parallelism to achieve low latency. Sprocket enables developers to program a series of operations over video content in a modular, extensible manner. Programmers implement custom operations, ranging from simple video transformations to more complex computer vision tasks, in a simple pipeline specification language to construct custom video processing pipelines. Sprocket then handles the underlying access, encoding and decoding, and processing of video and image content across operations in a highly parallel manner. In this paper we describe the design and implementation of the Sprocket system on the AWS Lambda serverless cloud infrastructure, and evaluate Sprocket under a variety of conditions to show that it delivers its performance goals of high parallelism, low latency, and low cost (10s of seconds to process a 3,600 second video 1000-way parallel for less than $3). © 2018 Association for Computing Machinery.","","Cloud computing; Costs; Pipeline processing systems; Pipelines; Specification languages; Sprockets; Wheels; Cloud infrastructures; Design and implementations; Encoding and decoding; Highly parallels; Image content; Simple pipelines; Video contents; Video processing; Video signal processing","Association for Computing Machinery, Inc","ACM SIGMOD; ACM SIGOPS","2018 ACM Symposium on Cloud Computing, SoCC 2018","11 October 2018 through 13 October 2018","Carlsbad","142611","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85059032967"
"","","","1st Workshop on Emerging Technologies for Software-Defined and Reconfigurable Hardware-Accelerated Cloud Datacenters, ETCD 2017 - Held in conjunction with 22nd ACM ASPLOS 2017","2017","1st Workshop on Emerging Technologies for Software-Defined and Reconfigurable Hardware-Accelerated Cloud Datacenters, ETCD 2017 - Held in conjunction with 22nd ACM ASPLOS 2017","","","","","","31","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040095702&partnerID=40&md5=e451768e06e12f77cfe784cd28f5b014","The proceedings contain 5 papers. The topics discussed include: an experimental comparison between genetic algorithm and particle swarm optimization in spark performance tuning; distributed SAR image change detection with OpenCL-enabled spark; a study of FPGA virtualization and accelerator scheduling; DoCE: direct extension of on-chip interconnects over converged Ethernet for rack-scale memory sharing; slow or down? seem to be the same for cloud users; anomaly detection in clouds: challenges and practice; rethinking the SDN abstraction; TCS: FaaS (FPGA as a service); customized architecture technology for high performance computing; building the reconfigurable cloud ecosystem; and programming FPGAs using OpenCL from performance model to application study.","","","Association for Computing Machinery, Inc","ACM Special Interest Group on Computer Architecture (SIGARCH); ACM Special Interest Group on Embedded Systems (SIGBED); ACM Special Interest Group on Operating Systems (SIGOPS); ACM Special Interest Group on Programming Languages (SIGPLAN)","1st Workshop on Emerging Technologies for Software-Defined and Reconfigurable Hardware-Accelerated Cloud Datacenters, ETCD 2017 - Held in conjunction with 22nd ACM ASPLOS","8 April 2017","Xi'an","131982","Conference review","Final","","Scopus","2-s2.0-85040095702"
"Zhang T.; Xie D.; Li F.; Stutsman R.","Zhang, Tian (57206942249); Xie, Dong (57037762100); Li, Feifei (8918895500); Stutsman, Ryan (14055061000)","57206942249; 57037762100; 8918895500; 14055061000","Narrowing the Gap between Serverless and its State with Storage Functions","2019","SoCC 2019 - Proceedings of the ACM Symposium on Cloud Computing","","","","1","12","11","70","10.1145/3357223.3362723","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090506802&doi=10.1145%2f3357223.3362723&partnerID=40&md5=f4380b0158ac7552e9b1c9bbc311bcaf","Serverless computing has gained attention due to its fine-grained provisioning, large-scale multi-Tenancy, and on-demand scaling. However, it also forces applications to externalize state in remote storage, adding substantial overheads. To fix this ""data shipping problem""we built Shredder, a low-latency multi-Tenant cloud store that allows small units of computation to be performed directly within storage nodes. Storage tenants provide Shredder with JavaScript functions (or WebAssembly programs), which can interact directly with data without moving them over the network. The key challenge in Shredder is safely isolating thousands of tenant storage functions while minimizing data interaction costs. Shredder uses a unique approach where its data store and networking paths are implemented in native code to ensure performance, while isolated tenant functions interact with data using a V8-specific intermediate representation that avoids expensive cross-protection-domain calls and data copying. As a result, Shredder can execute 4 million remotely-invoked tenant functions per second spread over thousands of tenants with median and 99th-percentile response latencies of less than 50 µs and 500 µs, respectively. Our evaluation shows that Shredder achieves a 14% to 78% speedup against conventional remote storage when fetching items with just one to three data dependencies between them. We also demonstrate Shredder's effectiveness in accelerating data-intensive applications, including a k-hop query on social graphs that shows orders of magnitude gain. © 2019 Owner/Author.","","Cloud computing; Data dependencies; Data interactions; Data-intensive application; Intermediate representations; Multi tenancies; Orders of magnitude; Protection domains; Storage function; Digital storage","Association for Computing Machinery","Amazon Web Services (AWS); Cisco; CockroachLabs; et al.; Google Cloud; Microsoft","10th ACM Symposium on Cloud Computing, SoCC 2019","20 November 2019 through 23 November 2019","Santa Cruz","161511","Conference paper","Final","","Scopus","2-s2.0-85090506802"
"Huang M.; Wu D.; Yu C.H.; Fang Z.; Interlandi M.; Condie T.; Cong J.","Huang, Muhuan (48261120600); Wu, Di (55813257900); Yu, Cody Hao (57188989180); Fang, Zhenman (38361266400); Interlandi, Matteo (35932341300); Condie, Tyson (9336164100); Cong, Jason (25931913900)","48261120600; 55813257900; 57188989180; 38361266400; 35932341300; 9336164100; 25931913900","Programming and runtime support to Blaze FPGA accelerator deployment at datacenter scale","2016","Proceedings of the 7th ACM Symposium on Cloud Computing, SoCC 2016","","","","456","469","13","70","10.1145/2987550.2987569","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995688234&doi=10.1145%2f2987550.2987569&partnerID=40&md5=97a27f5c7c4be2b5411b009d87172864","With the end of CPU core scaling due to dark silicon limitations, customized accelerators on FPGAs have gained increased attention in modern datacenters due to their lower power, high performance and energy efficiency. Evidenced by Microsoft's FPGA deployment in its Bing search engine and Intel's 16.7 billion acquisition of Altera, integrating FPGAs into datacenters is considered one of the most promising approaches to sustain future datacenter growth. However, it is quite challenging for existing big data computing systems-like Apache Spark and Hadoop- to access the performance and energy benefits of FPGA accelerators. In this paper we design and implement Blaze to provide programming and runtime support for enabling easy and efficient deployments of FPGA accelerators in datacenters. In particular, Blaze abstracts FPGA accelerators as a service (FaaS) and provides a set of clean programming APIs for big data processing applications to easily utilize those accelerators. Our Blaze runtime implements an FaaS framework to efficiently share FPGA accelerators among multiple heterogeneous threads on a single node, and extends Hadoop YARN with accelerator-centric scheduling to efficiently share them among multiple computing tasks in the cluster. Experimental results using four representative big data applications demonstrate that Blaze greatly reduces the programming efforts to access FPGA accelerators in systems like Apache Spark and YARN, and improves the system throughput by 1.7× to 3× (and energy efficiency by 1.5× to 2.7×) compared to a conventional CPU-only cluster.","FPGA-as-a-service; Heterogeneous datacenter","Acceleration; Application programming interfaces (API); Cloud computing; Cluster computing; Computer systems programming; Data handling; Energy efficiency; Field programmable gate arrays (FPGA); Reconfigurable hardware; Search engines; Wool; Yarn; Big data applications; Computing system; Data processing applications; Datacenter; Design and implements; Fpga accelerators; Runtime support; System throughput; Big data","Association for Computing Machinery, Inc","ACM SIGMOD; ACM SIGOPS; et al.; Google; IBM; Microsoft","7th ACM Symposium on Cloud Computing, SoCC 2016","5 October 2016 through 7 October 2016","Santa Clara","124260","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-84995688234"
"Sampé J.; Sanchez-Artigas M.; Garcia-Lopez P.; Paris G.","Sampé, Josep (57053360400); Sanchez-Artigas, Marc (10939155200); Garcia-Lopez, Pedro (24479469800); Paris, Gerard (24721553300)","57053360400; 10939155200; 24479469800; 24721553300","Data-driven serverless functions for object storage","2017","Middleware 2017 - Proceedings of the 2017 International Middleware Conference","","","","121","133","12","34","10.1145/3135974.3135980","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041214629&doi=10.1145%2f3135974.3135980&partnerID=40&md5=7e407193f055def8c9e684ab8aeb9267","Traditionally, active storage techniques have been proposed to move computation tasks to storage nodes in order to exploit data locality. However, we argue in this paper that active storage is ill-suited for cloud storage for two reasons: 1. Lack of elasticity: Computing can only scale out with the number of storage nodes; and 2. Resource Contention: Sharing compute resources can produce interferences in the storage system. Serverless computing is now emerging as a promising alternative for ensuring painless scalability, and also, for simplifying the development of disaggregated computing tasks. Here we present an innovative data-driven serverless computing middleware for object storage. It is a lightweight compute solution that allows users to create small, stateless functions that intercept and operate on data flows in a scalable manner without the need to manage a server or a runtime environment. We demonstrate through different use cases how our solution scales with minimal overhead, while getting rid of the resource contention problems incurred by active storage tasks. © 2017 ACM.","Cloud computing; Data flow interception; Data management; Object storage; Openstack swift; Programmability; Serverless functions","Cloud computing; Data transfer; Digital storage; Middleware; Computation tasks; Data flow; Object storages; Openstack swift; Programmability; Resource contention; Resource contention problem; Runtime environments; Information management","Association for Computing Machinery, Inc","Advanced Computing Systems Association (USENIX); Association for Computing Machinery (ACM); IBM; IFIP; Raytheon BBN; SIEMENS","18th ACM/IFIP/USENIX Middleware Conference, Middleware 2017","11 December 2017 through 15 December 2017","Las Vegas","133151","Conference paper","Final","","Scopus","2-s2.0-85041214629"
"Jonas E.; Pu Q.; Venkataraman S.; Stoica I.; Recht B.","Jonas, Eric (56640967500); Pu, Qifan (55841232900); Venkataraman, Shivaram (55312380900); Stoica, Ion (7007009125); Recht, Benjamin (6602593952)","56640967500; 55841232900; 55312380900; 7007009125; 6602593952","Occupy the cloud: Distributed computing for the 99%","2017","SoCC 2017 - Proceedings of the 2017 Symposium on Cloud Computing","","","","445","451","6","348","10.1145/3127479.3128601","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032457554&doi=10.1145%2f3127479.3128601&partnerID=40&md5=297fd2995b565f6919acd69e150d9936","Distributed computing remains inaccessible to a large number of users, in spite of many open source platforms and extensive commercial offerings. While distributed computation frameworks have moved beyond a simple map-reduce model, many users are still left to struggle with complex cluster management and configuration tools, even for running simple embarrassingly parallel jobs. We argue that stateless functions represent a viable platform for these users, eliminating cluster management overhead, fulfilling the promise of elasticity. Furthermore, using our prototype implementation, Py-Wren, we show that this model is general enough to implement a number of distributed computing models, such as BSP, efficiently. Extrapolating from recent trends in network bandwidth and the advent of disaggregated storage, we suggest that stateless functions are a natural fit for data processing in future computing environments. © 2017 Association for Computing Machinery.","AWS lambda; Distributed computing; PyWren; Serverless","Cloud computing; Data handling; Digital storage; Open systems; AWS lambda; Computing environments; Distributed computation framework; Distributed computing models; Open source platforms; Prototype implementations; PyWren; Serverless; Distributed computer systems","Association for Computing Machinery, Inc","ACM SIGMOD; ACM SIGOPS","2017 Symposium on Cloud Computing, SoCC 2017","24 September 2017 through 27 September 2017","Santa Clara","130803","Conference paper","Final","","Scopus","2-s2.0-85032457554"
"Anderson T.E.; Dahlin M.D.; Neefe J.M.; Patterson D.A.; Roselli D.S.; Wang R.Y.","Anderson, Thomas E. (35560665700); Dahlin, Michael D. (7003483638); Neefe, Jeanna M. (7402837019); Patterson, David A. (7401930147); Roselli, Drew S. (6602760776); Wang, Randolph Y. (7405339046)","35560665700; 7003483638; 7402837019; 7401930147; 6602760776; 7405339046","Serverless network file systems","1995","Proceedings of the 15th ACM Symposium on Operating Systems Principles, SOSP 1995","","","","109","126","17","256","10.1145/224056.224067","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883505875&doi=10.1145%2f224056.224067&partnerID=40&md5=d85e8eabb2c22f6fa75265489dd36e22","In this paper, we propose a new paradigm for network file system design, serverless network file systems. While traditional network file systems rely on a central server machine, a serverless system utilizes workstations cooperating as peers to provide all file system services. Any machine in the system can store, cache, or control any block of data. Our approach uses this location independence, in combination with fast local area networks, to provide better performance and scalability than traditional file systems. Further, because any machine in the system can assume the responsibilities of a failed component, our serverless design also provides high availability via redundant data storage. To demonstrate our approach, we have implemented a prototype serverless network file system called xFS. Preliminary performance measurements suggest that our architecture achieves its goal of scalability. For instance, in a 32-node xFS system with 32 active clients, each client receives nearly as much read or write throughput as it would see if it were the only active client. © 1995 ACM.","","Scalability; UNIX; Better performance; File system services; High availability; Location independence; Network file system; Performance measurements; Serverless systems; Write throughputs; File organization","","ACM Special Interest Group on Operating Systems (SIGOPS)","15th ACM Symposium on Operating Systems Principles, SOSP 1995","3 December 1995 through 6 December 1995","Copper Mountain, CO","99154","Conference paper","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-84883505875"
"","","","Proceedings of the 1st International Workshop on Mashups of Things and APIs, MOTA 2016","2016","Proceedings of the 1st International Workshop on Mashups of Things and APIs, MOTA 2016","","","","","","33","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010379951&partnerID=40&md5=f9b89007468b56b74fe63a2675772e4e","The proceedings contain 6 papers. The topics discussed include: a microservice architecture for the Intranet of Things and energy in smart buildings; situational data-analytics for the web-of-things; building a Chatbot with serverless computing; FRED: a hosted data flow platform for the IoT; orchestrating the internet of things dynamically; and dynamic sets - a programming abstraction for ubiquitous computing and the Internet of Things.","","","Association for Computing Machinery, Inc","Assoc. for Computing Machinery (ACM); IFIP; The Advanced Computing System Association (USENIX)","1st International Workshop on Mashups of Things and APIs, MOTA 2016, Co-located with 17th International Middleware Conference","12 December 2016 through 16 December 2016","Trento","125682","Conference review","Final","","Scopus","2-s2.0-85010379951"
"Karhula P.; Janak J.; Schulzrinne H.","Karhula, Pekka (56210748500); Janak, Jan (36617483700); Schulzrinne, Henning (7006775893)","56210748500; 36617483700; 7006775893","Checkpointing and migration of IoT edge functions","2019","EdgeSys 2019 - Proceedings of the 2nd ACM International Workshop on Edge Systems, Analytics and Networking, Part of EuroSys 2019","","","","60","65","5","45","10.1145/3301418.3313947","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063891217&doi=10.1145%2f3301418.3313947&partnerID=40&md5=d0c7483c45d299c94b5d03fac64dd340","The serverless and functions as a service (FaaS) paradigms are currently trending among cloud providers and are now increasingly being applied to the network edge, and to the Internet of Things (IoT) devices. The benefits include reduced latency for communication, less network traffic and increased privacy for data processing. However, there are challenges as IoT devices have limited resources for running multiple simultaneous containerized functions, and also FaaS does not typically support long-running functions. Our implementation utilizes Docker and CRIU for checkpointing and suspending long-running blocking functions. The results show that checkpointing is slightly slower than regular Docker pause, but it saves memory and allows for more long-running functions to be run on an IoT device. Furthermore, the resulting checkpoint files are small, hence they are suitable for live migration and backing up stateful functions, therefore improving availability and reliability of the system. © 2019 Copyright held by the owner/author(s).","Checkpointing; Function as a service; Internet of things; Light-weight virtualization; Serverless","Data handling; Check pointing; Cloud providers; Internet of thing (IOT); Less networks; Light weight; Live migrations; Reduced latencies; Serverless; Internet of things","Association for Computing Machinery, Inc","ACM SIGOPS","2nd ACM International Workshop on Edge Systems, Analytics and Networking, EdgeSys 2019, Part of EuroSys 2019","25 March 2019 through 25 March 2019","Dresden","146333","Conference paper","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85063891217"
