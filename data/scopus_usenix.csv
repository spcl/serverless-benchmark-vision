"Authors","Author full names","Author(s) ID","Title","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","Cited by","DOI","Link","Abstract","Author Keywords","Index Keywords","Publisher","Sponsors","Conference name","Conference date","Conference location","Conference code","Document Type","Publication Stage","Open Access","Source","EID"
"Zhang H.; Tang Y.; Khandelwal A.; Chen J.; Stoica I.","Zhang, Hong (59874271100); Tang, Yupeng (57226779139); Khandelwal, Anurag (57189265710); Chen, Jingrong (57223836523); Stoica, Ion (7007009125)","59874271100; 57226779139; 57189265710; 57223836523; 7007009125","Caerus: NIMBLE task scheduling for serverless analytics","2021","Proceedings of the 18th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2021","","","","653","668","15","44","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106195917&partnerID=40&md5=1ade861b8356005c6c8c846cf66c8d26","Serverless platforms facilitate transparent resource elasticity and fine-grained billing, making them an attractive choice for data analytics. We find that while server-centric analytics frameworks typically optimize for job completion time (JCT), resource utilization and isolation via inter-job scheduling policies, serverless analytics requires optimizing for JCT and cost of execution instead, introducing a new scheduling problem. We present Caerus, a task scheduler for serverless analytics frameworks that employs a fine-grained NIMBLE scheduling algorithm to solve this problem. NIMBLE efficiently pipelines task executions within a job, minimizing execution cost while being Pareto-optimal between cost and JCT for arbitrary analytics jobs. To this end, NIMBLE models a wide range of execution parameters-pipelineable and non-piplineable data dependencies, data generation, consumption and processing rates, etc.-to determine the ideal task launch times. Our evaluation results show that in practice, Caerus is able to achieve both optimal cost and JCT for queries across a wide range of analytics workloads.  © 2021 by The USENIX Association.","","Data Analytics; Pareto principle; Pipelines; Systems analysis; Data dependencies; Evaluation results; Execution costs; Job scheduling policies; Processing rates; Resource utilizations; Scheduling problem; Task-scheduling; Scheduling","USENIX Association","Amazon; et al.; Facebook; Futurewei Technologies; Google; NetApp","18th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2021","12 April 2021 through 14 April 2021","Virtual, Online","168826","Conference paper","Final","","Scopus","2-s2.0-85106195917"
"Thorpe J.; Qiao Y.; Eyolfson J.; Teng S.; Hu G.; Jia Z.; Wei J.; Vora K.; Netravali R.; Kim M.; Xu G.H.","Thorpe, John (57208125811); Qiao, Yifan (57224728333); Eyolfson, Jonathan (40461387300); Teng, Shen (57224742045); Hu, Guanzhou (57222342008); Jia, Zhihao (57145966000); Wei, Jinliang (59883489800); Vora, Keval (56275447300); Netravali, Ravi (22958227800); Kim, Miryung (57203466476); Xu, Guoqing Harry (57214267818)","57208125811; 57224728333; 40461387300; 57224742045; 57222342008; 57145966000; 59883489800; 56275447300; 22958227800; 57203466476; 57214267818","Dorylus: Affordable, scalable, and accurate GNN training with distributed CPU servers and serverless threads","2021","Proceedings of the 15th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2021","","","","495","514","19","105","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113859558&partnerID=40&md5=c5dbc42b57dcf75b32cea00ef763e64b","A graph neural network (GNN) enables deep learning on structured graph data. There are two major GNN training obstacles: 1) it relies on high-end servers with many GPUs which are expensive to purchase and maintain, and 2) limited memory on GPUs cannot scale to today’s billion-edge graphs. This paper presents Dorylus: a distributed system for training GNNs. Uniquely, Dorylus can take advantage of serverless computing to increase scalability at a low cost. The key insight guiding our design is computation separation. Computation separation makes it possible to construct a deep, bounded-asynchronous pipeline where graph and tensor parallel tasks can fully overlap, effectively hiding the network latency incurred by Lambdas. With the help of thousands of Lambda threads, Dorylus scales GNN training to billion-edge graphs. Currently, for large graphs, CPU servers offer the best performance per dollar over GPU servers. Just using Lambdas on top of Dorylus offers up to 2.75× more performance-per-dollar than CPU-only servers. Concretely, Dorylus is 1.22× faster and 4.83× cheaper than GPU servers for massive sparse graphs. Dorylus is up to 3.8× faster and 10.7× cheaper compared to existing sampling-based systems. © 2021 by The USENIX Association. All rights reserved.","","Distributed database systems; Graphic methods; Program processors; Systems analysis; Asynchronous pipeline; Distributed systems; Graph neural networks; High-end servers; Limited memory; Network latencies; Sampling-based; Structured graphs; Deep learning","USENIX Association","Amazon; ByteDance; et al.; Facebook; Futurewei Technologies; USENIX","15th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2021","14 July 2021 through 16 July 2021","Virtual, Online","170585","Conference paper","Final","","Scopus","2-s2.0-85113859558"
"Bag M.; Jindal A.; Patel H.","Bag, Malay (57202576649); Jindal, Alekh (53881488400); Patel, Hiren (57210068008)","57202576649; 53881488400; 57210068008","Towards plan-aware resource allocation in serverless query processing","2020","HotCloud 2020 - 12th USENIX Workshop on Hot Topics in Cloud Computing, co-located with USENIX ATC 2020","","","","","","","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091926041&partnerID=40&md5=59a01d613d4a809991c1e846444dccf2","Resource allocation for serverless query processing is a challenge. Unfortunately, prior approaches have treated queries as black boxes, thereby missing significant resource optimization opportunities. In this paper, we propose a plan-aware resource allocation approach where the resources are adaptively allocated based on the runtime characteristics of the query plan. We show the savings opportunity from such an allocation scheme over production SCOPE workloads at Microsoft. We present our current implementation of a greedy version that periodically estimates the peak resource for the remaining of the query as the query execution progresses. Our experimental evaluation shows that such an implementation could already save more than 8% resource usage over one of our production virtual clusters. We conclude by opening the discussion on various strategies for plan-aware resource allocation and their implications on the cloud computing stack. © HotCloud 2020 - 12th USENIX Workshop on Hot Topics in Cloud Computing, co-located with USENIX ATC 2020. All rights reserved.","","Cloud computing; Query processing; Black boxes; Experimental evaluation; MicroSoft; Query execution; Resource optimization; Resource usage; Runtimes; Virtual clusters; Resource allocation","USENIX Association","Google; NetApp; VMware","12th USENIX Workshop on Hot Topics in Cloud Computing, HotCloud 2020, co-located with USENIX ATC 2020","13 July 2020 through 14 July 2020","Virtual, Online","162465","Conference paper","Final","","Scopus","2-s2.0-85091926041"
"Fu Y.; Xue L.; Huang Y.; Brabete A.-O.; Ustiugov D.; Patel Y.; Mai L.","Fu, Yao (57217383487); Xue, Leyang (57830272900); Huang, Yeqi (58849709800); Brabete, Andrei-Octavian (57210734485); Ustiugov, Dmitrii (57211122881); Patel, Yuvraj (57196441156); Mai, Luo (55841094800)","57217383487; 57830272900; 58849709800; 57210734485; 57211122881; 57196441156; 55841094800","ServerlessLLM: Low-Latency Serverless Inference for Large Language Models","2024","Proceedings of the 18th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2024","","","","135","153","18","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196227125&partnerID=40&md5=765e50157a0202b6de7dbb44b86365d8","This paper presents ServerlessLLM, a distributed system designed to support low-latency serverless inference for Large Language Models (LLMs). By harnessing the substantial near-GPU storage and memory capacities of inference servers, ServerlessLLM achieves effective local checkpoint storage, minimizing the need for remote checkpoint downloads and ensuring efficient checkpoint loading. The design of ServerlessLLM features three core contributions: (i) fast multi-tier checkpoint loading, featuring a new loading-optimized checkpoint format and a multi-tier loading system, fully utilizing the bandwidth of complex storage hierarchies on GPU servers; (ii) efficient live migration of LLM inference, which enables newly initiated inferences to capitalize on local checkpoint storage while ensuring minimal user interruption; and (iii) startup-time-optimized model scheduling, which assesses the locality statuses of checkpoints on each server and schedules the model onto servers that minimize the time to start the inference. Comprehensive evaluations, including microbenchmarks and real-world scenarios, demonstrate that ServerlessLLM dramatically outperforms state-of-the-art serverless systems, reducing latency by 10 - 200X across various LLM inference workloads. © OSDI 2024.All rights reserved.","","Associative storage; Computer graphics equipment; Problem oriented languages; Virtual storage; Checkpoint storages; Distributed systems; Language model; Loading system; Low latency; Memory capacity; Model inference; Multi-tier; Storage capacity; Storage hierarchy; Graphics processing unit","USENIX Association","Amazon; Databricks; et al.; Futurewei Technologies; Roblox; USENIX Association","18th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2024","10 July 2024 through 12 July 2024","Santa Clara","201531","Conference paper","Final","","Scopus","2-s2.0-85196227125"
"Song W.W.; Um T.; Elnikety S.; Jeon M.; Chun B.-G.","Song, Won Wook (57194219657); Um, Taegeon (57195963402); Elnikety, Sameh (8366429300); Jeon, Myeongjae (55696387500); Chun, Byung-Gon (7102410988)","57194219657; 57195963402; 8366429300; 55696387500; 7102410988","Sponge: Fast Reactive Scaling for Stream Processing with Serverless Frameworks","2023","Proceedings of the 2023 USENIX Annual Technical Conference, ATC 2023","","","","301","314","13","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180374920&partnerID=40&md5=1974d16c57214b39fdc4e8c8d129704f","Streaming workloads deal with data that is generated in real-time. This data is often unpredictable and changes rapidly in volume. To deal with these fluctuations, current systems aim to dynamically scale in and out, redistribute, and migrate computing tasks across a cluster of machines. While many prior works have focused on reducing the overhead of system reconfiguration and state migration on pre-allocated cluster resources, these approaches still face significant challenges in meeting latency SLOs at low operational costs, especially upon facing unpredictable bursty loads. In this paper, we propose Sponge, a new stream processing system that enables fast reactive scaling of long-running stream queries by leveraging serverless framework (SF) instances. Sponge absorbs sudden, unpredictable increases in input loads from existing VMs with low latency and cost by taking advantage of the fact that SF instances can be initiated quickly, in just a few hundred milliseconds. Sponge efficiently tracks a small number of metrics to quickly detect bursty loads and make fast scaling decisions based on these metrics. Moreover, by incorporating optimization logic at compile-time and triggering fast data redirection and partial-state merging mechanisms at runtime, Sponge avoids optimization and state migration overheads during runtime while efficiently offloading bursty loads from existing VMs to new SF instances. Our evaluation on AWS EC2 and Lambda using the NEXMark benchmark shows that Sponge promptly reacts to bursty input loads, reducing 99thpercentile tail latencies by 88% on average compared to other stream query scaling methods on VMs. Sponge also reduces cost by 83% compared to methods that over-provision VMs to handle unpredictable bursty loads. © 2023 by The USENIX Association All Rights Reserved.","","Cluster computing; Cost reduction; Computing-task; Current system; Fluctuation current; Real- time; Runtimes; Scalings; State migrations; Stream processing; System reconfiguration; System state; Computation theory","USENIX Association","Akamai; et al.; Futurewei Technologies; NSF; USENIX Association; VMWare University Research Fund","2023 USENIX Annual Technical Conference, ATC 2023","10 July 2023 through 12 July 2023","Boston","195092","Conference paper","Final","","Scopus","2-s2.0-85180374920"
"","","","HotCloud 2020 - 12th USENIX Workshop on Hot Topics in Cloud Computing, co-located with USENIX ATC 2020","2020","HotCloud 2020 - 12th USENIX Workshop on Hot Topics in Cloud Computing, co-located with USENIX ATC 2020","","","","","","144","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091942680&partnerID=40&md5=92574b35ca1f0c61b09e1348ee6e4754","The proceedings contain 22 papers. The topics discussed include: a cloud gaming framework for dynamic graphical rendering towards achieving distributed game engines; towards supporting millions of users in modifiable virtual environments by redesigning minecraft-like games as serverless systems; AI4DL: mining behaviors of deep learning workloads for resource management; Spotnik: designing distributed machine learning for transient cloud resources; model-switching: dealing with fluctuating workloads in machine-learning-as-a-service systems; towards GPU utilization prediction for cloud deep learning; serverless boom or bust? an analysis of economic incentives; no reservations: a first look at Amazon’s reserved instance marketplace; and auto-sizing for stream processing applications at LinkedIn.","","","USENIX Association","Google; NetApp; VMware","12th USENIX Workshop on Hot Topics in Cloud Computing, HotCloud 2020, co-located with USENIX ATC 2020","13 July 2020 through 14 July 2020","Virtual, Online","162465","Conference review","Final","","Scopus","2-s2.0-85091942680"
"Cheng Y.; Zhou Z.","Cheng, Yingchao (57007430600); Zhou, Zhongrun (57205505098)","57007430600; 57205505098","Autonomous resource scheduling for real-time and stream processing","2018","Proceedings - 2018 IEEE SmartWorld, Ubiquitous Intelligence and Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People and Smart City Innovations, SmartWorld/UIC/ATC/ScalCom/CBDCom/IoP/SCI 2018","","","8560183","1181","1184","3","5","10.1109/SmartWorld.2018.00205","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060276633&doi=10.1109%2fSmartWorld.2018.00205&partnerID=40&md5=3a8d1105382fb1052193a8adf4159fe0","This work proposes the ARS(FaaS) framework, scheduling and provisioning resources for streaming applications autonomously. It ensures real-time response on unpredictable and fluctuating streaming data. We use a HPC Cloud platform as the de facto platform, and explore FaaS for stream processing on it. The major contribution of this work is effective and efficient autonomous resource scheduling for real-time streaming analytic. © 2018 IEEE.","Autonomous scheduling; Cloud computing; FaaS; Steam processing","Big data; Cloud computing; Scheduling; Smart city; Trusted computing; Autonomous scheduling; FaaS; Real time response; Real time streaming; Resource-scheduling; Stream processing; Streaming applications; Streaming data; Ubiquitous computing","Institute of Electrical and Electronics Engineers Inc.","","4th IEEE SmartWorld, 15th IEEE International Conference on Ubiquitous Intelligence and Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People and Smart City Innovations, SmartWorld/UIC/ATC/ScalCom/CBDCom/IoP/SCI 2018","7 October 2018 through 11 October 2018","Guangzhou","143372","Conference paper","Final","","Scopus","2-s2.0-85060276633"
"Li Z.; Guo L.; Chen Q.; Cheng J.; Xu C.; Zeng D.; Song Z.; Ma T.; Yang Y.; Li C.; Guo M.","Li, Zijun (57278389100); Guo, Linsong (57394013200); Chen, Quan (36623232500); Cheng, Jiagan (57393989800); Xu, Chuhao (57949219100); Zeng, Deze (24722168400); Song, Zhuo (57211901693); Ma, Tao (57211901430); Yang, Yong (57211949893); Li, Chao (56697637700); Guo, Minyi (7201564780)","57278389100; 57394013200; 36623232500; 57393989800; 57949219100; 24722168400; 57211901693; 57211901430; 57211949893; 56697637700; 7201564780","Help Rather Than Recycle: Alleviating Cold Startup in Serverless Computing Through Inter-Function Container Sharing","2022","Proceedings of the 2022 USENIX Annual Technical Conference, ATC 2022","","","","69","84","15","71","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137787065&partnerID=40&md5=cae1d5bf3a230156bd84ee6d47324708","In serverless computing, each function invocation is executed in a container (or a Virtual Machine), and container cold startup results in long response latency. We observe that some functions suffer from cold container startup, while the warm containers of other functions are idle. Based on the observation, other than booting a new container for a function from scratch, we propose to alleviate the cold startup by re-purposing a warm but idle container from another function. We implement a container management scheme, named Pagurus, to achieve the purpose. Pagurus comprises an intra-function manager for replacing an idle warm container to be a container that other functions can use without introducing additional security issues, an inter-function scheduler for scheduling containers between functions, and a sharingaware function balancer at the cluster-level for balancing the workload across different nodes. Experiments using Azure serverless traces show that Pagurus alleviates 84.6% of the cold startup, and the cold startup latency is reduced from hundreds of milliseconds to 16 milliseconds if alleviated. © 2022 USENIX Annual Technical Conference, ATC 2022.All rights reserved.","","Balancing; Scheduling; Container management; Management scheme; Security issues; Containers","USENIX Association","Alibaba Group; Apple; et al.; PagerDuty; USENIX Association; VMware University Research Fund","2022 USENIX Annual Technical Conference, ATC 2022","11 July 2022 through 13 July 2022","Carlsbad","183226","Conference paper","Final","","Scopus","2-s2.0-85137787065"
"Zhuang S.; Wang S.; Liang E.; Cheng Y.; Stoica I.","Zhuang, Siyuan (57219586350); Wang, Stephanie (57195424708); Liang, Eric (55268196400); Cheng, Yi (58774852900); Stoica, Ion (7007009125)","57219586350; 57195424708; 55268196400; 58774852900; 7007009125","ExoFlow: A Universal Workflow System for Exactly-Once DAGs","2023","Proceedings of the 17th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2023","","","","269","286","17","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180377045&partnerID=40&md5=52b58f406171c3c816af48dca6cbbab4","Given the fundamental tradeoff between run-time and recovery performance, current distributed systems often build application-specific recovery strategies to minimize overheads. However, it is increasingly common for different applications to be composed into heterogeneous pipelines. Implementing multiple interoperable recovery techniques in the same system is rare and difficult. Thus, today’s users must choose between: (1) building on a single system, and face a fixed choice of performance vs. recovery overheads, or (2) the challenging task of stitching together multiple systems that can offer application-specific tradeoffs. We present ExoFlow, a universal workflow system that enables a flexible choice of recovery vs. performance tradeoffs, even within the same application. The key insight behind our solution is to decouple execution from recovery and provide exactly-once semantics as a separate layer from execution. For generality, workflow tasks can return references that capture arbitrary inter-task communication. To enable the workflow system and therefore the end user to take control of recovery, we design task annotations that specify execution semantics such as nondeterminism. ExoFlow generalizes recovery for existing workflow applications ranging from ETL pipelines to stateful serverless workflows, while enabling further optimizations in task communication and recovery. © OSDI 2023.All rights reserved.","","Interoperability; Pipelines; Semantics; 'current; Application specific; Distributed systems; Performance; Recovery performance; Recovery strategies; Recovery techniques; Runtime performance; Work-flow systems; Work-flows; Recovery","USENIX Association","Akamai; Amazon; et al.; Futurewei Technologies; NSF; USENIX Association","17th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2023","10 July 2023 through 12 July 2023","Boston","195094","Conference paper","Final","","Scopus","2-s2.0-85180377045"
"Mahgoub A.; Yi E.B.; Shankar K.; Elnikety S.; Chaterji S.; Bagchi S.","Mahgoub, Ashraf (56878717700); Yi, Edgardo Barsallo (57203268424); Shankar, Karthick (57215202832); Elnikety, Sameh (8366429300); Chaterji, Somali (18133715900); Bagchi, Saurabh (14821898700)","56878717700; 57203268424; 57215202832; 8366429300; 18133715900; 14821898700","ORION and the Three Rights: Sizing, Bundling, and Prewarming for Serverless DAGs","2022","Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2022","","","","303","320","17","80","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131698575&partnerID=40&md5=e21e869863830f4f2992daa375794bef","Serverless applications represented as DAGs have been growing in popularity. For many of these applications, it would be useful to estimate the end-to-end (E2E) latency and to allocate resources to individual functions so as to meet probabilistic guarantees for the E2E latency. This goal has not been met till now due to three fundamental challenges. The first is the high variability and correlation in the execution time of individual functions, the second is the skew in execution times of the parallel invocations, and the third is the incidence of cold starts. In this paper, we introduce ORION to achieve this goal. We first analyze traces from a production FaaS infrastructure to identify three characteristics of serverless DAGs. We use these to motivate and design three features. The first is a performance model that accounts for runtime variabilities and dependencies among functions in a DAG. The second is a method for co-locating multiple parallel invocations within a single VM thus mitigating content-based skew among these invocations. The third is a method for pre-warming VMs for subsequent functions in a DAG with the right look-ahead time. We integrate these three innovations and evaluate ORION on AWS Lambda with three serverless DAG applications. Our evaluation shows that compared to three competing approaches, ORION achieves up to 90% lower P95 latency without increasing $ cost, or up to 53% lower $ cost without increasing P95 latency. © 2022 by The USENIX Association. All rights reserved.","","Cold-start; Content-based; End to end; Increasing costs; Lambda's; Look-ahead time; Low-costs; Performance Modeling; Probabilistic guarantees; Runtime variabilities; Function evaluation","USENIX Association","Akamai; Amazon; Apple; et al.; Google; USENIX Association","16th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2022","11 July 2022 through 13 July 2022","Carlsbad","183231","Conference paper","Final","","Scopus","2-s2.0-85131698575"
"Song J.; Kim B.; Kwak M.; Lee B.; Seo E.; Jeong J.","Song, Jaehyun (57222710736); Kim, Bumsuk (59260070200); Kwak, Minwoo (59259838800); Lee, Byoungyoung (56434753000); Seo, Euiseong (14056969000); Jeong, Jinkyu (23397248100)","57222710736; 59260070200; 59259838800; 56434753000; 14056969000; 23397248100","A Secure, Fast, and Resource-Efficient Serverless Platform with Function REWIND","2024","Proceedings of the 2024 USENIX Annual Technical Conference, ATC 2024","","","","597","613","16","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201184904&partnerID=40&md5=187829f9fa12da8806044b16ccaa4f99","Serverless computing often utilizes the warm container technique to improve response times. However, this method, which allows the reuse of function containers across different function requests of the same type, creates persistent vulnerabilities in memory and file systems. These vulnerabilities can lead to security breaches such as data leaks. Traditional approaches to address these issues often suffer from performance drawbacks and high memory requirements due to the extensive use of user-level snapshots and complex restoration process. The paper introduces REWIND, an innovative and efficient serverless function execution platform designed to address these security and efficiency concerns. REWIND ensures that after each function request, the container is reset to an initial state free of any sensitive data, including a thorough restoration of the file system to prevent data leakage. It incorporates a kernel-level memory snapshot management system, which significantly lowers memory usage and accelerates the rewind process. Additionally, REWIND optimizes runtime by reusing memory regions and leveraging the temporal locality of function executions, enhancing performance while maintaining strict data isolation between requests. The prototype of REWIND is implemented on OpenWhisk and Linux and evaluated with serverless benchmark workloads. The evaluation results have demonstrated that REWIND provides substantial memory savings while providing high function execution performance. Especially, the low memory usage makes more warm containers kept alive thereby improving the throughput as well as the latency of function executions while providing isolation between function requests. © 2024 Proceedings of the 2024 USENIX Annual Technical Conference, ATC 2024. All rights reserved.","","Computer operating systems; File organization; Restoration; Sensitive data; Filesystem; Low memory; Memory requirements; Memory systems; Memory usage; Performance drawback; Resource-efficient; Reuse; Security breaches; Traditional approaches; Containers","USENIX Association","Futurewei Technologies; IBM; Meta; NetApp; NSF; USENIX Association","2024 USENIX Annual Technical Conference, ATC 2024","10 July 2024 through 12 July 2024","Santa Clara","201530","Conference paper","Final","","Scopus","2-s2.0-85201184904"
"Sun T.; Jiang B.; Li B.; Lv J.; Gao Y.; Dong W.","Sun, Tong (59151569200); Jiang, Bowen (59151087600); Li, Borui (57217088942); Lv, Jiamei (57203095089); Gao, Yi (55731336900); Dong, Wei (7202224406)","59151569200; 59151087600; 57217088942; 57203095089; 55731336900; 7202224406","SimEnc: A High-Performance Similarity-Preserving Encryption Approach for Deduplication of Encrypted Docker Images","2024","Proceedings of the 2024 USENIX Annual Technical Conference, ATC 2024","","","","615","630","15","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201206858&partnerID=40&md5=e591d61f8b756a42f769105c817f1295","Encrypted Docker images are becoming increasingly popular in Docker registries for privacy. As the Docker registry is tasked with managing an increasing number of images, it becomes essential to implement deduplication to conserve storage space. However, deduplication for encrypted images is difficult because deduplication exploits identical content, while encryption tries to make all contents look random. Existing state-of-the-art works try to decompress images and perform message-locked encryption (MLE) to deduplicate encrypted images. Unfortunately, our measurements uncover two limitations in current works: (i) even minor modifications to the image content can hinder MLE deduplication, (ii) decompressing image layers would increase the size of the storage for duplicate data, and significantly compromise user pull latency and deduplication throughput. In this paper, we propose SimEnc, a high-performance similarity-preserving encryption approach for deduplication of encrypted Docker images. SimEnc is the first work that integrates the semantic hash technique into MLE to extract semantic information among layers for improving the deduplication ratio. SimEnc builds on a fast similarity space selection mechanism for flexibility. Unlike existing works completely decompressing the layer, we explore a new similarity space by Huffman decoding that achieves a better deduplication ratio and performance. Experiments show that SimEnc outperforms both the state-of-the-art encrypted serverless platform and plaintext Docker registry, reducing storage consumption by up to 261.7% and 54.2%, respectively. Meanwhile, SimEnc can surpass them in terms of pull latency. © 2024 Proceedings of the 2024 USENIX Annual Technical Conference, ATC 2024. All rights reserved.","","Digital storage; Semantics; 'current; Art work; Deduplication; Encrypted images; Image content; Performance; Similarity preserving; Similarity spaces; State of the art; Storage spaces; Cryptography","USENIX Association","Futurewei Technologies; IBM; Meta; NetApp; NSF; USENIX Association","2024 USENIX Annual Technical Conference, ATC 2024","10 July 2024 through 12 July 2024","Santa Clara","201530","Conference paper","Final","","Scopus","2-s2.0-85201206858"
"Wei X.; Lu F.; Chen R.; Chen H.","Wei, Xingda (57112888500); Lu, Fangming (57440649500); Chen, Rong (56428216800); Chen, Haibo (55743141500)","57112888500; 57440649500; 56428216800; 55743141500","KRCORE: A Microsecond-scale RDMA Control Plane for Elastic Computing","2022","Proceedings of the 2022 USENIX Annual Technical Conference, ATC 2022","","","","121","136","15","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137705584&partnerID=40&md5=8e0b3c8c19436472fbc965abbced4d35","We present KRCORE, an RDMA library with a microsecondscale control plane on commodity RDMA hardware for elastic computing. KRCORE can establish a full-fledged RDMA connection within 10μs (hundreds or thousands of times faster than verbs), while only maintaining a (small) fixed-sized connection metadata at each node, regardless of the cluster scale. The key ideas include virtualizing pre-initialized kernel-space RDMA connections instead of creating one from scratch, and retrofitting advanced RDMA dynamic connected transport with static transport for both low connection overhead and high networking speed. Under load spikes, KRCORE can shorten the worker bootstrap time of an existing disaggregated key-value store (namely RACE Hashing) by 83%. In serverless computing (namely Fn), KRCORE can also reduce the latency for transferring data through RDMA by 99%. © 2022 USENIX Annual Technical Conference, ATC 2022.All rights reserved.","","Bootstrap time; Cluster scale; Control planes; Elastic computing; Kernel space; Key-value stores; Under loads; Workers'","USENIX Association","Alibaba Group; Apple; et al.; PagerDuty; USENIX Association; VMware University Research Fund","2022 USENIX Annual Technical Conference, ATC 2022","11 July 2022 through 13 July 2022","Carlsbad","183226","Conference paper","Final","","Scopus","2-s2.0-85137705584"
"Dong W.; Li B.; Li H.; Wu H.; Gong K.; Zhang W.; Gao Y.","Dong, Wei (7202224406); Li, Borui (57217088942); Li, Haoyu (58249365500); Wu, Hao (58599304700); Gong, Kaijie (58249365600); Zhang, Wenzhao (57210935423); Gao, Yi (55731336900)","7202224406; 57217088942; 58249365500; 58599304700; 58249365600; 57210935423; 55731336900","LinkLab 2.0: A Multi-tenant Programmable IoT Testbed for Experimentation with Edge-Cloud Integration","2023","Proceedings of the 20th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2023","","","","1683","1699","16","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159285673&partnerID=40&md5=f0a103354b8f1fe1cebc91599180eb00","In this paper, we present LinkLab 2.0, a completely programmable and controllable IoT testbed with the support of edge devices and cloud infrastructures. To be more specific, LinkLab 2.0 leverages a tiered architecture for the programmable devices and the management system to achieve scalability. To better support the integrated experiment among IoT, edge and cloud, LinkLab 2.0 provides one-site programming support and leverages the customizable offloading with serverless functions. Moreover, LinkLab 2.0 proposes a device-involved multi-tenancy approach to ensure responsiveness for concurrent requests. Furthermore, targeting 24/7 availability for experimenters, LinkLab 2.0 leverages proactive and reactive anomaly detection to improve the reliability of the testbed. Finally, we describe the supported research experiments and the outreach usage by external users. We also report lessons learned from the four-year operation. LinkLab 2.0 has supported experiments for 2, 100+ users. The accumulated usage time across all the devices exceeds 17, 300 hours. © NSDI 2023.All rights reserved","","Anomaly detection; Internet of things; Systems analysis; Cloud infrastructures; Cloud integrations; Customizable; Edge clouds; Linklab; Management systems; Multi tenants; Programmable devices; Programming support; Tiered architecture; Multi tenancies; Testbeds","USENIX Association","Amazon; et al.; Futurewei Technologies; Meta; NSF; USENIX Association","20th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2023","17 April 2023 through 19 April 2023","Boston","188341","Conference paper","Final","","Scopus","2-s2.0-85159285673"
"Shahrad M.; Fonseca R.; Goiri Í.; Chaudhry G.; Batum P.; Cooke J.; Laureano E.; Tresness C.; Russinovich M.; Bianchini R.","Shahrad, Mohammad (56943394900); Fonseca, Rodrigo (59847856900); Goiri, Íñigo (24801831000); Chaudhry, Gohar (57219259486); Batum, Paul (57219260740); Cooke, Jason (57219258980); Laureano, Eduardo (6506602105); Tresness, Colby (57219250962); Russinovich, Mark (6602399070); Bianchini, Ricardo (7006939544)","56943394900; 59847856900; 24801831000; 57219259486; 57219260740; 57219258980; 6506602105; 57219250962; 6602399070; 7006939544","Serverless in the wild: Characterizing and optimizing the serverless workload at a large cloud provider","2020","Proceedings of the 2020 USENIX Annual Technical Conference, ATC 2020","","","","205","218","13","502","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091934381&partnerID=40&md5=16dd6343f7a2bb6ef374f99e1bb924da","Function as a Service (FaaS) has been gaining popularity as a way to deploy computations to serverless backends in the cloud. This paradigm shifts the complexity of allocating and provisioning resources to the cloud provider, which has to provide the illusion of always-available resources (i.e., fast function invocations without cold starts) at the lowest possible resource cost. Doing so requires the provider to deeply understand the characteristics of the FaaS workload. Unfortunately, there has been little to no public information on these characteristics. Thus, in this paper, we first characterize the entire production FaaS workload of Azure Functions. We show for example that most functions are invoked very infrequently, but there is an 8-order-of-magnitude range of invocation frequencies. Using observations from our characterization, we then propose a practical resource management policy that significantly reduces the number of function cold starts, while spending fewer resources than state-of-the-practice policies. Copyright © Proc. of the 2020 USENIX Annual Technical Conference, ATC 2020. All rights reserved.","","Cloud providers; Cold start; Paradigm shifts; Public information; Resource costs; Resource management policy; State of the practice","USENIX Association","et al.; Facebook; Microsoft; Oracle; USENIX Association; VMware","2020 USENIX Annual Technical Conference, ATC 2020","15 July 2020 through 17 July 2020","Virtual, Online","162460","Conference paper","Final","","Scopus","2-s2.0-85091934381"
"Pang X.; Zhang Y.; Liu L.; Cheng D.; Xu C.; Zhou X.","Pang, Xingguo (59259640300); Zhang, Yanze (59259640400); Liu, Liu (57206979337); Cheng, Dazhao (56048653500); Xu, Chengzhong (55600419500); Zhou, Xiaobo (55743262600)","59259640300; 59259640400; 57206979337; 56048653500; 55600419500; 55743262600","Expeditious High-Concurrency MicroVM SnapStart in Persistent Memory with an Augmented Hypervisor","2024","Proceedings of the 2024 USENIX Annual Technical Conference, ATC 2024","","","","985","998","13","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201218650&partnerID=40&md5=e3163af19b419b4d26ac17e2a92c5b20","The industry has embraced snapshotting to tackle the cold starts and efficiently manage numerous short-lived functions for microservice-native architectures, serverless computing, and machine learning inference. A cutting-edge research approach FaaSnap, while innovative in reducing page faults during on-demand paging through prefetching the profiled working set pages into DRAM, incurs high caching overheads and I/O demands, potentially degrading system efficiency. This paper introduces PASS, a system leveraging byte-addressable persistent memory (PMEM) for cost-effective and highly concurrent MicroVM SnapStart execution. PASS, functioning as a PMEM-aware augmented hypervisor in the user space, revolutionizes MicroVM memory restoration. It constructs complete address indexing of the guest memory mapped to single-tier PMEM space, enabling zero-copy on-demand paging by exploiting PMEM’s direct access feature. This approach bypasses the cache layer and maintains guest OS transparency, avoiding invasive modifications. Experimental results, derived from real-world applications, reveal that PASS substantially decreases SnapStart execution time, achieving up to 72% reduction compared to the Firecracker hypervisor on the PMEM filesystem, and 47% reduction compared to FaaSnap. Moreover, PASS achieves double the maximum concurrency compared to both Firecracker and FaaSnap. It improves the cost-effectiveness by 2.2x and 1.6x over the Firecracker and FaaSnap, respectively. © 2024 Proceedings of the 2024 USENIX Annual Technical Conference, ATC 2024. All rights reserved.","","Cache memory; Dynamic random access storage; Memory architecture; % reductions; Cold-start; Cutting edges; Demand paging; High concurrencies; Hypervisors; Machine-learning; On demands; Persistent memory; Research approach; Cost effectiveness","USENIX Association","Futurewei Technologies; IBM; Meta; NetApp; NSF; USENIX Association","2024 USENIX Annual Technical Conference, ATC 2024","10 July 2024 through 12 July 2024","Santa Clara","201530","Conference paper","Final","","Scopus","2-s2.0-85201218650"
"Akkus I.E.; Chen R.; Rimac I.; Satzke M.S.K.; Beck A.; Aditya P.; Hilt V.","Akkus, Istemi Ekin (15062297200); Chen, Ruichuan (23388102000); Rimac, Ivica (55941252700); Satzke, Manuel Stein Klaus (57212875738); Beck, Andre (10639921200); Aditya, Paarijaat (55960724400); Hilt, Volker (6602872942)","15062297200; 23388102000; 55941252700; 57212875738; 10639921200; 55960724400; 6602872942","SAND: Towards high-performance serverless computing","2020","Proceedings of the 2018 USENIX Annual Technical Conference, USENIX ATC 2018","","","","923","935","12","317","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077465955&partnerID=40&md5=e91084dedc46ba5ee2049a8269682a68","Serverless computing has emerged as a new cloud computing paradigm, where an application consists of individual functions that can be separately managed and executed. However, existing serverless platforms normally isolate and execute functions in separate containers, and do not exploit the interactions among functions for performance. These practices lead to high startup delays for function executions and inefficient resource usage. This paper presents SAND, a new serverless computing system that provides lower latency, better resource efficiency and more elasticity than existing serverless platforms. To achieve these properties, SAND introduces two key techniques: 1) application-level sandboxing, and 2) a hierarchical message bus. We have implemented and deployed a complete SAND system. Our results show that SAND outperforms the state-of-the-art serverless platforms significantly. For example, in a commonly-used image processing application, SAND achieves a 43% speedup compared to Apache OpenWhisk. © Proceedings of the 2018 USENIX Annual Technical Conference, USENIX ATC 2018. All rights reserved.","","Image processing; Application level; Computing system; Image processing applications; Message bus; Resource efficiencies; Resource usage; Sand systems; State of the art; Sand","USENIX Association","et al.; Facebook; NetApp; NSF; Oracle; The USENIX Association","2018 USENIX Annual Technical Conference, USENIX ATC 2018","11 July 2018 through 13 July 2018","Boston","155411","Conference paper","Final","","Scopus","2-s2.0-85077465955"
"Zhang C.; Yu M.; Wang W.; Yan F.","Zhang, Chengliang (57203225545); Yu, Minchen (57205201325); Wang, Wei (57234263600); Yan, Feng (50862222500)","57203225545; 57205201325; 57234263600; 50862222500","Mark: Exploiting cloud services for cost-effective, slo-aware machine learning inference serving","2019","Proceedings of the 2019 USENIX Annual Technical Conference, USENIX ATC 2019","","","","1049","1062","13","233","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076183718&partnerID=40&md5=de8d4f1dca43c789793276270808d25f","The advances of Machine Learning (ML) have sparked a growing demand of ML-as-a-Service: developers train ML models and publish them in the cloud as online services to provide low-latency inference at scale. The key challenge of ML model serving is to meet the response-time Service-Level Objectives (SLOs) of inference workloads while minimizing the serving cost. In this paper, we tackle the dual challenge of SLO compliance and cost effectiveness with MArk (Model Ark), a general-purpose inference serving system built in Amazon Web Services (AWS). MArk employs three design choices tailor-made for inference workload. First, MArk dynamically batches requests and opportunistically serves them using expensive hardware accelerators (e.g., GPU) for improved performance-cost ratio. Second, instead of relying on feedback control scaling or over-provisioning to serve dynamic workload, which can be too slow or too expensive for inference serving, MArk employs predictive autoscaling to hide the provisioning latency at low cost. Third, given the stateless nature of inference serving, MArk exploits the flexible, yet costly serverless instances to cover the occasional load spikes that are hard to predict. We evaluated the performance of MArk using several state-of-the-art ML models trained in popular frameworks including TensorFlow, MXNet, and Keras. Compared with the premier industrial ML serving platform SageMaker, MArk reduces the serving cost up to 7.8× while achieving even better latency performance. © Proceedings of the 2019 USENIX Annual Technical Conference, USENIX ATC 2019. All rights reserved.","","Cost effectiveness; Costs; Machine learning; Amazon web services; Expensive hardware; Latency performance; On-line service; Over provisioning; Performance-cost ratio; Service level objective; State of the art; Web services","USENIX Association","Facebook; NSF; Nutanix; Oracle; The USENIX Association; VMware","2019 USENIX Annual Technical Conference, USENIX ATC 2019","10 July 2019 through 12 July 2019","Renton","155412","Conference paper","Final","","Scopus","2-s2.0-85076183718"
"","","","Proceedings of the 18th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2024","2024","Proceedings of the 18th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2024","","","","","","1010","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201261581&partnerID=40&md5=c90790b5d94aab0ac12f421b8a426726","The proceedings contain 53 papers. The topics discussed include: Nomad: non-exclusive memory tiering via transactional page migration; managing memory tiers with CXL in virtualized environments; harvesting memory-bound CPU stall cycles in software with MSH; a tale of two paths: toward a hybrid data plane for efficient far-memory applications; DRust: language-guided distributed shared memory with fine granularity, full transparency, and ultra efficiency; taming throughput-latency tradeoff in LLM inference with Sarathi-Serve; ServerlessLLM: low-latency serverless inference for large language models; InfiniGen: efficient generative inference of large language models with dynamic KV cache management; Llumnix: dynamic scheduling for large language model serving; DistServe: disaggregating prefill and decoding for goodput-optimized large language model serving; and beaver: practical partial snapshots for distributed cloud services.","","","USENIX Association","Amazon; Databricks; et al.; Futurewei Technologies; Roblox; USENIX Association","18th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2024","10 July 2024 through 12 July 2024","Santa Clara","201531","Conference review","Final","","Scopus","2-s2.0-85201261581"
"Kotni S.; Nayak A.; Ganapathy V.; Basu A.","Kotni, Swaroop (57226501227); Nayak, Ajay (57224633814); Ganapathy, Vinod (55712440200); Basu, Arkaprava (24469737700)","57226501227; 57224633814; 55712440200; 24469737700","Faastlane: Accelerating function-as-a-service workflows","2021","2021 USENIX Annual Technical Conference","","","","957","971","14","85","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111773082&partnerID=40&md5=ec6a9fd1ed341761f6ab42acf01dbed1","In FaaS workflows, a set of functions implement application logic by interacting and exchanging data among themselves. Contemporary FaaS platforms execute each function of a workflow in separate containers. When functions in a workflow interact, the resulting latency slows execution. Faastlane minimizes function interaction latency by striving to execute functions of a workflow as threads within a single process of a container instance, which eases data sharing via simple load/store instructions. For FaaS workflows that operate on sensitive data, Faastlane provides lightweight thread-level isolation domains using Intel Memory Protection Keys (MPK). While threads ease sharing, implementations of languages such as Python and Node.js (widely used in FaaS applications) disallow concurrent execution of threads. Faastlane dynamically identifies opportunities for parallelism in FaaS workflows and fork processes (instead of threads) or spawns new container instances to concurrently execute parallel functions of a workflow. We implemented Faastlane atop Apache OpenWhisk and show that it accelerates workflow instances by up to 15×, and reduces function interaction latency by up to 99.95% compared to OpenWhisk. © 2021 USENIX Annual Technical Conference. All rights reserved.","","Containers; Data privacy; Application logic; Concurrent execution; Memory protection; Parallel functions; Sensitive datas; Single process; Work-flows; Workflow instances; Data Sharing","USENIX Association","Facebook; Google; IBM; USENIX; VMware","2021 USENIX Annual Technical Conference, ATC 2021","14 July 2021 through 16 July 2021","Virtual, Online","170531","Conference paper","Final","","Scopus","2-s2.0-85111773082"
"Wang L.; Li M.; Zhang Y.; Ristenpart T.; Swift M.","Wang, Liang (59086727400); Li, Mengyuan (57200512759); Zhang, Yinqian (36669844400); Ristenpart, Thomas (23393983800); Swift, Michael (57209111762)","59086727400; 57200512759; 36669844400; 23393983800; 57209111762","Peeking behind the curtains of serverless platforms","2020","Proceedings of the 2018 USENIX Annual Technical Conference, USENIX ATC 2018","","","","133","145","12","425","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077448177&partnerID=40&md5=4c9a798d44b920fec75b104bd8d7da8f","Serverless computing is an emerging paradigm in which an application's resource provisioning and scaling are managed by third-party services. Examples include AWS Lambda, Azure Functions, and Google Cloud Functions. Behind these services' easy-to-use APIs are opaque, complex infrastructure and management ecosystems. Taking on the viewpoint of a serverless customer, we conduct the largest measurement study to date, launching more than 50,000 function instances across these three services, in order to characterize their architectures, performance, and resource management efficiency. We explain how the platforms isolate the functions of different accounts, using either virtual machines or containers, which has important security implications. We characterize performance in terms of scalability, coldstart latency, and resource efficiency, with highlights including that AWS Lambda adopts a bin-packing-like strategy to maximize VM memory utilization, that severe contention between functions can arise in AWS and Azure, and that Google had bugs that allow customers to use resources for free. © Proceedings of the 2018 USENIX Annual Technical Conference, USENIX ATC 2018. All rights reserved.","","Efficiency; Bin packing; Complex infrastructures; Measurement study; Memory utilization; Resource efficiencies; Resource management; Security implications; Third party services; Memory architecture","USENIX Association","et al.; Facebook; NetApp; NSF; Oracle; The USENIX Association","2018 USENIX Annual Technical Conference, USENIX ATC 2018","11 July 2018 through 13 July 2018","Boston","155411","Conference paper","Final","","Scopus","2-s2.0-85077448177"
"Wang A.; Chang S.; Tian H.; Wang H.; Yang H.; Li H.; Du R.; Cheng Y.","Wang, Ao (57215285001); Chang, Shuai (57224667574); Tian, Huangshi (57203228417); Wang, Hongqi (57224668237); Yang, Haoran (57224666403); Li, Huiba (36351958500); Du, Rui (57224667914); Cheng, Yue (56022559100)","57215285001; 57224667574; 57203228417; 57224668237; 57224666403; 36351958500; 57224667914; 56022559100","FAASNET: Scalable and fast provisioning of custom serverless container runtimes at Alibaba cloud function compute","2021","2021 USENIX Annual Technical Conference","","","","443","457","14","89","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111751592&partnerID=40&md5=530200872d7c299a88fc36602c2489b0","Serverless computing, or Function-as-a-Service (FaaS), enables a new way of building and scaling applications by allowing users to deploy fine-grained functions while providing fully-managed resource provisioning and auto-scaling. Custom FaaS container support is gaining traction as it enables better control over OSes, versioning, and tooling for modernizing FaaS applications. However, providing rapid container provisioning introduces non-trivial challenges for FaaS providers, since container provisioning is costly, and real-world FaaS workloads exhibit highly dynamic patterns. In this paper, we design FAASNET, a highly-scalable middleware system for accelerating FaaS container provisioning. FAASNET is driven by the workload and infrastructure requirements of the FaaS platform at one of the world's largest cloud providers, Alibaba Cloud Function Compute. FAASNET enables scalable container provisioning via a lightweight, adaptive function tree (FT) structure. FAASNET uses an I/O efficient, on-demand fetching mechanism to further reduce provisioning costs at scale. We implement and integrate FAASNET in Alibaba Cloud Function Compute. Evaluation results show that FAASNET: (1) finishes provisioning 2;500 function containers on 1;000 virtual machines in 8.3 seconds, (2) scales 13.4× and 16.3× faster than Alibaba Cloud's current FaaS platform and a state-of-the-art P2P container registry (Kraken), respectively, and (3) sustains a bursty workload using 75:2% less time than an optimized baseline. © 2021 USENIX Annual Technical Conference. All rights reserved.","","Middleware; Traction control; Adaptive functions; Bursty workloads; Cloud providers; Dynamic patterns; Evaluation results; Fine grained; Middleware system; State of the art; Containers","USENIX Association","Facebook; Google; IBM; USENIX; VMware","2021 USENIX Annual Technical Conference, ATC 2021","14 July 2021 through 16 July 2021","Virtual, Online","170531","Conference paper","Final","","Scopus","2-s2.0-85111751592"
"","","","Proceedings of the 2022 USENIX Annual Technical Conference, ATC 2022","2022","Proceedings of the 2022 USENIX Annual Technical Conference, ATC 2022","","","","","","1052","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140962094&partnerID=40&md5=85cbc5ec4c8cdfef5838a0d1e787b147","The proceedings contain 64 papers. The topics discussed include: building a high-performance fine-grained deduplication framework for backup storage with high deduplication ratio; secure and lightweight deduplicated storage via shielded deduplication-before-encryption; help rather than recycle: alleviating cold startup in serverless computing through inter-function container sharing; zero-change object transmission for distributed big data analytics; sift: using refinement-guided automation to verify complex distributed systems; co-opting Linux processes for high-performance network simulation; KSG: augmenting kernel fuzzing with system call specification generation; Modulo: finding convergence failure bugs in distributed systems with divergence resync models; SoftTRR: protect page tables against Rowhammer attacks using software-only target row refresh; and Vigil-KV: hardware-software co-design to integrate strong latency determinism into log-structured merge key-value stores.","","","USENIX Association","Alibaba Group; Apple; et al.; PagerDuty; USENIX Association; VMware University Research Fund","2022 USENIX Annual Technical Conference, ATC 2022","11 July 2022 through 13 July 2022","Carlsbad","183226","Conference review","Final","","Scopus","2-s2.0-85140962094"
"Pu Q.; Venkataraman S.; Stoica I.","Pu, Qifan (55841232900); Venkataraman, Shivaram (55312380900); Stoica, Ion (7007009125)","55841232900; 55312380900; 7007009125","Shuffling, fast and slow: Scalable analytics on serverless infrastructure","2019","Proceedings of the 16th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2019","","","","193","206","13","204","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072316299&partnerID=40&md5=5a9b520ced150de89f99ddac23436c41","Serverless computing is poised to fulfill the long-held promise of transparent elasticity and millisecond-level pricing. To achieve this goal, service providers impose a fine-grained computational model where every function has a maximum duration, a fixed amount of memory and no persistent local storage. We observe that the fine-grained elasticity of serverless is key to achieve high utilization for general computations such as analytics workloads, but that resource limits make it challenging to implement such applications as they need to move large amounts of data between functions that don't overlap in time. In this paper, we present Locus, a serverless analytics system that judiciously combines (1) cheap but slow storage with (2) fast but expensive storage, to achieve good performance while remaining cost-efficient. Locus applies a performance model to guide users in selecting the type and the amount of storage to achieve the desired cost-performance trade-off. We evaluate Locus on a number of analytics applications including TPC-DS, CloudSort, Big Data Benchmark and show that Locus can navigate the cost-performance trade-off, leading to 4×-500× performance improvements over slow storage-only baseline and reducing resource usage by up to 59% while achieving comparable performance with running Apache Spark on a cluster of virtual machines, and within 2× slower compared to Redshift. © 2019 by The USENIX Association. All Rights Reserved.","","Benchmarking; Digital storage; Economic and social effects; Elasticity; Systems analysis; Analytics systems; Computational model; Cost performance; Cost-efficient; High utilizations; Large amounts of data; Performance Model; Service provider; Costs","USENIX Association","Amazon; ByteDance; et al.; Facebook; NSF; The USENIX Association","16th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2019","26 February 2019 through 28 February 2019","Boston","155225","Conference paper","Final","","Scopus","2-s2.0-85072316299"
"Karthikeyan A.; Natarajan N.; Somashekar G.; Zhao L.; Bhagwan R.; Fonseca R.; Racheva T.; Bansal Y.","Karthikeyan, Ajaykrishna (57219794944); Natarajan, Nagarajan (7004412700); Somashekar, Gagan (57213267316); Zhao, Lei (58248735300); Bhagwan, Ranjita (6506809871); Fonseca, Rodrigo (59847856900); Racheva, Tatiana (58248735400); Bansal, Yogesh (57949090300)","57219794944; 7004412700; 57213267316; 58248735300; 6506809871; 59847856900; 58248735400; 57949090300","SelfTune: Tuning Cluster Managers","2023","Proceedings of the 20th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2023","","","","1097","1114","17","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158850658&partnerID=40&md5=9204d00f47ed8701bf486f9c6565f9b2","Large-scale cloud providers rely on cluster managers for container allocation and load balancing (e.g., Kubernetes), VM provisioning (e.g., Protean), and other management tasks. These cluster managers use algorithms or heuristics whose behavior depends upon multiple configuration parameters. Currently, operators manually set these parameters using a combination of domain knowledge and limited testing. In very large-scale and dynamic environments, these manually-set parameters may lead to sub-optimal cluster states, adversely affecting important metrics such as latency and throughput. In this paper we describe SelfTune, a framework that automatically tunes such parameters in deployment. SelfTune piggybacks on the iterative nature of cluster managers which, through multiple iterations, drives a cluster to a desired state. Using a simple interface, developers integrate SelfTune into the cluster manager code, which then uses a principled reinforcement learning algorithm to tune important parameters over time. We have deployed SelfTune on tens of thousands of machines that run a large-scale background task scheduler at Microsoft. SelfTune has improved throughput by as much as 20% in this deployment by continuously tuning a key configuration parameter that determines the number of jobs concurrently accessing CPU and disk on every machine. We also evaluate SelfTune with two Azure FaaS workloads, the Kubernetes Vertical Pod Autoscaler, and the DeathStar microservice benchmark. In all cases, SelfTune significantly improves cluster performance. © NSDI 2023.All rights reserved","","Balancing; Clustering algorithms; Domain Knowledge; Iterative methods; Parameter estimation; Cloud providers; Cluster managers; Configuration parameters; Container allocations; Large-scales; Load-Balancing; Management tasks; Multiple configurations; Self-tune; VM provisioning; Reinforcement learning","USENIX Association","Amazon; et al.; Futurewei Technologies; Meta; NSF; USENIX Association","20th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2023","17 April 2023 through 19 April 2023","Boston","188341","Conference paper","Final","","Scopus","2-s2.0-85158850658"
"Chow M.; Wang Y.; Wang W.; Hailu A.; Bopardikar R.; Zhang B.; Qu J.; Meisner D.; Sonawane S.; Zhang Y.; Paim R.; Ward M.; Huang I.; McNally M.; Hodges D.; Farkas Z.; Gocmen C.; Huang E.; Tang C.","Chow, Mike (59797552900); Wang, Yang (59855470600); Wang, William (59260838600); Hailu, Ayichew (59261743000); Bopardikar, Rohan (57735899400); Zhang, Bin (59260838700); Qu, Jialiang (59261196400); Meisner, David (26867913000); Sonawane, Santosh (59261743100); Zhang, Yunqi (59824481100); Paim, Rodrigo (59261555600); Ward, Mack (59261555700); Huang, Ivor (59261013000); McNally, Matt (59261013100); Hodges, Daniel (59260654100); Farkas, Zoltan (59261376200); Gocmen, Caner (57203384116); Huang, Elvis (59261013200); Tang, Chunqiang (59844235800)","59797552900; 59855470600; 59260838600; 59261743000; 57735899400; 59260838700; 59261196400; 26867913000; 59261743100; 59824481100; 59261555600; 59261555700; 59261013000; 59261013100; 59260654100; 59261376200; 57203384116; 59261013200; 59844235800","ServiceLab: Preventing Tiny Performance Regressions at Hyperscale through Pre-Production Testing","2024","Proceedings of the 18th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2024","","","","545","562","17","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201302635&partnerID=40&md5=76af85c494819293cfc8fa4d1136f05c","This paper presents ServiceLab, a large-scale performance testing platform developed at Meta. Currently, the diverse set of applications and ML models it tests consumes millions of machines in production, and each year it detects performance regressions that could otherwise lead to the wastage of millions of machines. A major challenge for ServiceLab is to detect small performance regressions, sometimes as tiny as 0.01%. These minor regressions matter due to our large fleet size and their potential to accumulate over time. For instance, the median regression detected by ServiceLab for our large serverless platform, running on more than half a million machines, is only 0.14%. Another challenge is running performance tests in our private cloud, which, like the public cloud, is a noisy environment that exhibits inherent performance variances even for machines of the same instance type. To address these challenges, we conduct a large-scale study with millions of performance experiments to identify machine factors, such as the kernel, CPU, and datacenter location, that introduce variance to test results. Moreover, we present statistical analysis methods to robustly identify small regressions. Finally, we share our seven years of operational experience in dealing with a diverse set of applications. © OSDI 2024.All rights reserved.","","Logistic regression; Fleet sizes; Large-scales; Performance; Performance testing; Performance tests; Pre-production; Private clouds; Production testing; Public clouds; Testing platforms; Fleet operations","USENIX Association","Amazon; Databricks; et al.; Futurewei Technologies; Roblox; USENIX Association","18th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2024","10 July 2024 through 12 July 2024","Santa Clara","201531","Conference paper","Final","","Scopus","2-s2.0-85201302635"
"Ding H.; Wang Z.; Shen Z.; Chen R.; Chen H.","Ding, Haoran (59061535700); Wang, Zhaoguo (37040023500); Shen, Zhuohao (57223916274); Chen, Rong (56428216800); Chen, Haibo (55743141500)","59061535700; 37040023500; 57223916274; 56428216800; 55743141500","Automated Verification of Idempotence for Stateful Serverless Applications","2023","Proceedings of the 17th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2023","","","","887","910","23","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177736972&partnerID=40&md5=2646e105694a1fd2eb01ed02bcb8cb3c","Serverless computing has become a popular cloud computing paradigm. By default, when a serverless function fails, the serverless platform re-executes the function to tolerate the failure. However, such a retry-based approach requires functions to be idempotent, which means that functions should expose the same behavior regardless of retries. This requirement is challenging for developers, especially when functions are stateful. Failures may cause functions to repeatedly read and update shared states, potentially corrupting data consistency. This paper presents Flux, the first toolkit that automatically verifies the idempotence of serverless applications. It proposes a new correctness definition, idempotence consistency, which stipulates that a serverless function’s retry is transparent to users. To verify idempotence consistency, Flux defines a novel property, idempotence simulation, which decomposes the proof for a concurrent serverless application into the reasoning of individual functions. Furthermore, Flux extends existing verification techniques to realize automated reasoning, enabling Flux to identify idempotence-violating operations and fix them with existing log-based methods. We demonstrate the efficacy of Flux with 27 representative serverless applications. Flux has successfully identified previously unknown issues in 12 applications. Developers have confirmed 8 issues. Compared to state-of-the-art systems (namely Beldi and Boki) that log every operation, Flux achieves up to 6× lower latency and 10× higher peak throughput, as it logs only the identified idempotence-violating ones. © OSDI 2023.All rights reserved.","","Automated reasoning; Automated verification; Cloud-computing; Computing paradigm; Data consistency; Idempotence; Idempotent; Property; Shared state; Verification techniques","USENIX Association","Akamai; Amazon; et al.; Futurewei Technologies; NSF; USENIX Association","17th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2023","10 July 2023 through 12 July 2023","Boston","195094","Conference paper","Final","","Scopus","2-s2.0-85177736972"
"Donkervliet J.; Trivedi A.; Iosup A.","Donkervliet, Jesse (56377366500); Trivedi, Animesh (55014115500); Iosup, Alexandru (23392350500)","56377366500; 55014115500; 23392350500","Towards supporting millions of users in modifiable virtual environments by redesigning minecraft-like games as serverless systems","2020","HotCloud 2020 - 12th USENIX Workshop on Hot Topics in Cloud Computing, co-located with USENIX ATC 2020","","","","","","","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091894748&partnerID=40&md5=71bacf254c1b39f33d3e743212e2ba7a","How can Minecraft-like games become scalable cloud services? Hundreds of Minecraft-like games, that is, games acting as modifiable virtual environments (MVEs), are currently played by over 100 million players, but surprisingly they do not scale and are frequently not published as cloud services. We envision a new architecture for large-scale MVEs, supporting much larger numbers of concurrent users by scaling up and out using serverless technology. In our vision, developers focus on the game (business) logic, while cloud providers manage resource management and scheduling (RMS) and guarantee non-functional properties. We provide a definition for MVEs, model their services and deployments, present a vision for large-scale MVEs architected as serverless systems, and suggest concrete steps towards realizing this vision. © HotCloud 2020 - 12th USENIX Workshop on Hot Topics in Cloud Computing, co-located with USENIX ATC 2020. All rights reserved.","","Cloud computing; Scheduling; Web services; Cloud providers; Cloud services; Concurrent users; Non functional properties; Resource management and scheduling; Scaling-up; Serverless systems; Computation theory","USENIX Association","Google; NetApp; VMware","12th USENIX Workshop on Hot Topics in Cloud Computing, HotCloud 2020, co-located with USENIX ATC 2020","13 July 2020 through 14 July 2020","Virtual, Online","162465","Conference paper","Final","","Scopus","2-s2.0-85091894748"
"Wei X.; Lu F.; Wang T.; Gu J.; Yang Y.; Chen R.; Chen H.","Wei, Xingda (57112888500); Lu, Fangming (57440649500); Wang, Tianxia (57884297300); Gu, Jinyu (57196063512); Yang, Yuhan (57884036700); Chen, Rong (56428216800); Chen, Haibo (55743141500)","57112888500; 57440649500; 57884297300; 57196063512; 57884036700; 56428216800; 55743141500","No Provisioned Concurrency: Fast RDMA-codesigned Remote Fork for Serverless Computing","2023","Proceedings of the 17th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2023","","","","497","517","20","31","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174262447&partnerID=40&md5=0d5ace9812c1eea07e8254620cba3b14","Serverless platforms essentially face a tradeoff between container startup time and provisioned concurrency (i.e., cached instances), which is further exaggerated by the frequent need for remote container initialization. This paper presents MITOSIS, an operating system primitive that provides fast remote fork, which exploits a deep codesign of the OS kernel with RDMA. By leveraging the fast remote read capability of RDMA and partial state transfer across serverless containers, MITOSIS bridges the performance gap between local and remote container initialization. MITOSIS is the first to fork over 10,000 new containers from one instance across multiple machines within a second, while allowing the new containers to efficiently transfer the pre-materialized states of the forked one. We have implemented MITOSIS on Linux and integrated it with FN, a popular serverless platform. Under load spikes in real-world serverless workloads, MITOSIS reduces the function tail latency by 89% with orders of magnitude lower memory usage. For serverless workflow that requires state transfer, MITOSIS improves its execution time by 86%. © OSDI 2023.All rights reserved.","","Computer operating systems; Co-designs; Low memory; Multiple machine; Orders of magnitude; Partial state; Performance gaps; Real-world; Startup time; State transfer; Under loads; Containers","USENIX Association","Akamai; Amazon; et al.; Futurewei Technologies; NSF; USENIX Association","17th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2023","10 July 2023 through 12 July 2023","Boston","195094","Conference paper","Final","","Scopus","2-s2.0-85174262447"
"Feng E.; Lu X.; Du D.; Yang B.; Jiang X.; Xia Y.; Zang B.; Chen H.","Feng, Erhu (57238836600); Lu, Xu (55602806000); Du, Dong (57200438686); Yang, Bicheng (57238311600); Jiang, Xueqiang (57238665400); Xia, Yubin (7403027696); Zang, Binyu (6701320221); Chen, Haibo (55743141500)","57238836600; 55602806000; 57200438686; 57238311600; 57238665400; 7403027696; 6701320221; 55743141500","Scalable memory protection in the PENGLAI enclave","2021","Proceedings of the 15th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2021","","","","275","294","19","75","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113907038&partnerID=40&md5=e5949c963118b954ad1312ede14ccfb8","Secure hardware enclaves have been widely used for protecting security-critical applications in the cloud. However, existing enclave designs fail to meet the requirements of scalability demanded by new scenarios like serverless computing, mainly due to the limitations in their secure memory protection mechanisms, including static allocation, restricted capacity and high-cost initialization. In this paper, we propose a software-hardware co-design to support dynamic, fine-grained, large-scale secure memory as well as fast-initialization. We first introduce two new hardware primitives: 1) Guarded Page Table (GPT), which protects page table pages to support page-level secure memory isolation; 2) Mountable Merkle Tree (MMT), which supports scalable integrity protection for secure memory. Upon these two primitives, our system can scale to thousands of concurrent enclaves with high resource utilization and eliminate the high-cost initialization of secure memory using fork-style enclave creation without weakening the security guarantees. We have implemented a prototype of our design based on PENGLAI [24], an open-sourced enclave system for RISC-V. The experimental results show that PENGLAI can support 1,000s enclave instances running concurrently and scale up to 512GB secure memory with both encryption and integrity protection. The overhead of GPT is 5% for memory-intensive workloads (e.g., Redis) and negligible for CPU-intensive workloads (e.g., RV8 and Coremarks). PENGLAI also reduces the latency of secure memory initialization by three orders of magnitude and gains 3.6x speedup for real-world applications (e.g., MapReduce). © 2021 by The USENIX Association. All rights reserved.","","Computer hardware; Cryptography; Systems analysis; CPU-intensive; Integrity protection; Resource utilizations; Restricted capacities; Scalable memory; Secure memory; Security critical applications; Three orders of magnitude; Hardware-software codesign","USENIX Association","Amazon; ByteDance; et al.; Facebook; Futurewei Technologies; USENIX","15th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2021","14 July 2021 through 16 July 2021","Virtual, Online","170585","Conference paper","Final","","Scopus","2-s2.0-85113907038"
"Shillaker S.; Pietzuch P.","Shillaker, Simon (57219251124); Pietzuch, Peter (22734675800)","57219251124; 22734675800","FAASM: Lightweight isolation for efficient stateful serverless computing","2020","Proceedings of the 2020 USENIX Annual Technical Conference, ATC 2020","","","","419","433","14","209","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091895916&partnerID=40&md5=a3c197671df0d207c7b55fc4d21f4950","Serverless computing is an excellent fit for big data processing because it can scale quickly and cheaply to thousands of parallel functions. Existing serverless platforms isolate functions in ephemeral, stateless containers, preventing them from directly sharing memory. This forces users to duplicate and serialise data repeatedly, adding unnecessary performance and resource costs. We believe that a new lightweight isolation approach is needed, which supports sharing memory directly between functions and reduces resource overheads. We introduce Faaslets, a new isolation abstraction for high-performance serverless computing. Faaslets isolate the memory of executed functions using software-fault isolation (SFI), as provided by WebAssembly, while allowing memory regions to be shared between functions in the same address space. Faaslets can thus avoid expensive data movement when functions are co-located on the same machine. Our runtime for Faaslets, FAASM, isolates other resources, e.g. CPU and network, using standard Linux cgroups, and provides a low-level POSIX host interface for networking, file system access and dynamic loading. To reduce initialisation times, FAASM restores Faaslets from already-initialised snapshots. We compare FAASM to a standard container-based platform and show that, when training a machine learning model, it achieves a 2× speed-up with 10× less memory; for serving machine learning inference, FAASM doubles the throughput and reduces tail latency by 90%. Copyright © Proc. of the 2020 USENIX Annual Technical Conference, ATC 2020. All rights reserved.","","Computer operating systems; Containers; Dynamic loads; Machine learning; Turing machines; Data movements; Dynamic loadings; Machine learning models; Memory region; Parallel functions; Resource costs; Sharing memory; Software fault isolations; Data handling","USENIX Association","et al.; Facebook; Microsoft; Oracle; USENIX Association; VMware","2020 USENIX Annual Technical Conference, ATC 2020","15 July 2020 through 17 July 2020","Virtual, Online","162460","Conference paper","Final","","Scopus","2-s2.0-85091895916"
"Bhardwaj A.; Gupta M.; Stutsman R.","Bhardwaj, Ankit (57219255754); Gupta, Meghana (57219258039); Stutsman, Ryan (14055061000)","57219255754; 57219258039; 14055061000","On the impact of isolation costs on locality-aware cloud scheduling","2020","HotCloud 2020 - 12th USENIX Workshop on Hot Topics in Cloud Computing, co-located with USENIX ATC 2020","","","","","","","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091948986&partnerID=40&md5=50d561a5dc3f80a7464da31455bd6748","Serverless applications create an opportunity for more granular scheduling across machines in cloud platforms that can improve efficiency, especially if functions can be run within storage services to eliminate data movement. However, embedding code within storage services creates code isolation overheads that offset some of those savings. We argue for a new approach to serverless function scheduling that can look within serverless applications' functions, profile their data movement and networking costs, and model the impact of different code placement and isolation schemes for those costs. Beyond improvements in efficiency, such an approach would fuel innovation in cloud isolation schemes and programming abstractions, since a scheduler with a modular cost modeling approach could incorporate new schemes and automatically use them to improve efficiency for pre-existing applications. © HotCloud 2020 - 12th USENIX Workshop on Hot Topics in Cloud Computing, co-located with USENIX ATC 2020. All rights reserved.","","Codes (symbols); Computer programming; Digital storage; Efficiency; Scheduling; Cloud platforms; Cloud scheduling; Code placement; Data movements; Locality aware; New approaches; Programming abstractions; Storage services; Storage as a service (STaaS)","USENIX Association","Google; NetApp; VMware","12th USENIX Workshop on Hot Topics in Cloud Computing, HotCloud 2020, co-located with USENIX ATC 2020","13 July 2020 through 14 July 2020","Virtual, Online","162465","Conference paper","Final","","Scopus","2-s2.0-85091948986"
"Brooker M.; Danilov M.; Greenwood C.; Piwonka P.","Brooker, Marc (57225348685); Danilov, Mike (57222379014); Greenwood, Chris (58312782200); Piwonka, Phil (57219239442)","57225348685; 57222379014; 58312782200; 57219239442","On-demand Container Loading in AWS Lambda","2023","Proceedings of the 2023 USENIX Annual Technical Conference, ATC 2023","","","","315","328","13","31","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171256802&partnerID=40&md5=5256c72881c91d2d6b9b343fd517ebae","AWS Lambda is a serverless event-driven compute service, part of a category of cloud compute offerings sometimes called Function-as-a-service (FaaS). When we first released AWS Lambda, functions were limited to 250MB of code and dependencies, packaged as a simple compressed archive. In 2020, we released support for deploying container images as large as 10GiB as Lambda functions, allowing customers to bring much larger code bases and sets of dependencies to Lambda. Supporting larger packages, while still meeting Lambda’s goals of rapid scale (adding up to 15,000 new containers per second for a single customer, and much more in aggregate), high request rate (millions of requests per second), high scale (millions of unique workloads), and low start-up times (as low as 50ms) presented a significant challenge. We describe the storage and caching system we built, optimized for delivering container images on-demand, and our experiences designing, building, and operating it at scale. We focus on challenges around security, efficiency, latency, and cost, and how we addressed these challenges in a system that combines caching, deduplication, convergent encryption, erasure coding, and block-level demand loading. Since building this system, it has reliably processed hundreds of trillions of Lambda invocations for over a million AWS customers, and has shown excellent resilience to load and infrastructure failures. © 2023 by The USENIX Association All Rights Reserved.","","Codes (symbols); Cryptography; Sales; Caching system; Container loading; Event-driven; Lambda's; Large code basis; On demands; Service parts; Simple++; Startup time; Storage systems; Containers","USENIX Association","Akamai; et al.; Futurewei Technologies; NSF; USENIX Association; VMWare University Research Fund","2023 USENIX Annual Technical Conference, ATC 2023","10 July 2023 through 12 July 2023","Boston","195092","Conference paper","Final","","Scopus","2-s2.0-85171256802"
"Lin X.C.; Gonzalez J.E.; Hellerstein J.M.","Lin, Xiayue Charles (57219029776); Gonzalez, Joseph E. (57200981709); Hellerstein, Joseph M. (35561994000)","57219029776; 57200981709; 35561994000","Serverless boom or bust? An analysis of economic incentives","2020","HotCloud 2020 - 12th USENIX Workshop on Hot Topics in Cloud Computing, co-located with USENIX ATC 2020","","","","","","","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091923349&partnerID=40&md5=09031d54b0cc8fe2da02dc9ff60fc849","Serverless computing is a new paradigm that promises to free cloud users from the burden of having to provision and manage resources. However, the degree to which serverless computing will replace provisioned servers remains an open question. To address this, we develop an economic model that aims to quantify the value of serverless to providers and customers. A simple model of incentives for rational providers and customers allows us to see, in broad strokes, when and why serverless technologies are worth pursuing. By characterizing the conditions under which mutually beneficial economic incentives exist, our model suggests that many classes of customers can already benefit from switching to a serverless model and taking advantage of autoscaling at today's price points. Our model also helps characterize technical research directions that would be likely to have impact in the market. © HotCloud 2020 - 12th USENIX Workshop on Hot Topics in Cloud Computing, co-located with USENIX ATC 2020. All rights reserved.","","Cloud computing; Autoscaling; Economic incentive; Economic modeling; Simple modeling; Technical research; Sales","USENIX Association","Google; NetApp; VMware","12th USENIX Workshop on Hot Topics in Cloud Computing, HotCloud 2020, co-located with USENIX ATC 2020","13 July 2020 through 14 July 2020","Virtual, Online","162465","Conference paper","Final","","Scopus","2-s2.0-85091923349"
"Mahgoub A.; Shankar K.; Mitra S.; Klimovic A.; Chaterji S.; Bagchi S.","Mahgoub, Ashraf (56878717700); Shankar, Karthick (57215202832); Mitra, Subrata (57197251717); Klimovic, Ana (56039431800); Chaterji, Somali (18133715900); Bagchi, Saurabh (14821898700)","56878717700; 57215202832; 57197251717; 56039431800; 18133715900; 14821898700","Erratum: SONIC: Application-aware data passing for chained serverless applications (2021 USENIX Annual Technical Conference)","2021","2021 USENIX Annual Technical Conference","","","","","","","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111745708&partnerID=40&md5=40c48ce075f3f51c6701b4bf36c3eafb","In the paper ""SONIC: Application-aware Data Passing for Chained Serverless Applications"" by Ashraf Mahgoub, Purdue University; Karthick Shankar, Carnegie Mellon University; Subrata Mitra, Adobe Research; Ana Klimovic, ETH Zurich; Somali Chaterji and Saurabh Bagchi, Purdue University (Friday session, ""But You Played with Me Yesterday: Serverless Computing and Consistency,"" pp. 973-988 of the Proceedings), the authors have provided the following correction. 'Figure Presented'. Original text (page 980, section 5.4): For LightGBM and MapReduce Sort applications, we do not see a significant improvement in Perf/$ for AWS-? baselines with the latency-optimized configuration, since the memory footprint of these applications is close to the 3GB limit to begin with leaving very little room for overprovisioning. For AWS-?, using ElastiCache-Redis as the remote storage achieves 18% lower latency than using S3. However, ElastiCache-Redis increases the cost significantly, causing a reduction of Perf/$. Corrected text (page 980, section 5.4): For the MapReduce Sort application, we do not see a significant improvement in Perf/$ for AWS-? baselines with the latency-optimized configuration when using S3 as the remote storage. This is because the memory footprint of this application is close to the 3GB limit to begin with, leaving very little room for over-provisioning. However, for the LightGBM application, AWS-? + ElastiCache-Redis outperforms AWS-?+ S3 due to its lower latency. SONIC still achieves 3.5X and 2.8X in terms of the Perf/$ metric over AWS-?+ ElastiCache-Redis in memory-sized and latency-optimized configurations respectively, as Elasti-Cache-Redis increases the cost significantly. (Figure Presented). © 2021 USENIX Annual Technical Conference. All rights reserved.","","","USENIX Association","Facebook; Google; IBM; USENIX; VMware","2021 USENIX Annual Technical Conference, ATC 2021","14 July 2021 through 16 July 2021","Virtual, Online","170531","Erratum","Final","","Scopus","2-s2.0-85111745708"
"Agache A.; Brooker M.; Florescu A.; Iordache A.; Liguori A.; Neugebauer R.; Piwonka P.; Popa D.-M.","Agache, Alexandru (57194114875); Brooker, Marc (57225348685); Florescu, Andreea (57219243773); Iordache, Alexandra (57219247008); Liguori, Anthony (57212512525); Neugebauer, Rolf (59777312200); Piwonka, Phil (57219239442); Popa, Diana-Maria (57219239061)","57194114875; 57225348685; 57219243773; 57219247008; 57212512525; 59777312200; 57219239442; 57219239061","Firecracker: Lightweight virtualization for serverless applications","2020","Proceedings of the 17th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2020","","","","419","434","15","375","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086179991&partnerID=40&md5=0e81e717796ced6cef474bdf70ac5848","Serverless containers and functions are widely used for deploying and managing software in the cloud. Their popularity is due to reduced cost of operations, improved utilization of hardware, and faster scaling than traditional deployment methods. The economics and scale of serverless applications demand that workloads from multiple customers run on the same hardware with minimal overhead, while preserving strong security and performance isolation. The traditional view is that there is a choice between virtualization with strong security and high overhead, and container technologies with weaker security and minimal overhead. This tradeoff is unacceptable to public infrastructure providers, who need both strong security and minimal overhead. To meet this need, we developed Firecracker, a new open source Virtual Machine Monitor (VMM) specialized for serverless workloads, but generally useful for containers, functions and other compute workloads within a reasonable set of constraints. We have deployed Firecracker in two publically-available serverless compute services at Amazon Web Services (Lambda and Fargate), where it supports millions of production workloads, and trillions of requests per month. We describe how specializing for serverless informed the design of Firecracker, and what we learned from seamlessly migrating Lambda customers to Firecracker. © Proc. of the 17th USENIX Symposium on Networked Systems Design and Impl., NSDI 2020. All rights reserved.","","Containers; Open source software; Systems analysis; Virtualization; Web services; Amazon web services; Deployment methods; Open sources; Production workloads; Public infrastructures; Reduced cost; Strong securities; Virtual machine monitors; Computer hardware","USENIX Association","Amazon; et al.; Facebook; Google; NSF; USENIX Association","17th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2020","25 February 2020 through 27 February 2020","Santa Clara","162496","Conference paper","Final","","Scopus","2-s2.0-85086179991"
"Liu D.H.; Levy A.; Noghabi S.; Burckhardt S.","Liu, David H. (58250326300); Levy, Amit (56253133900); Noghabi, Shadi (57189499571); Burckhardt, Sebastian (8671920100)","58250326300; 56253133900; 57189499571; 8671920100","Doing More with Less: Orchestrating Serverless Applications without an Orchestrator","2023","Proceedings of the 20th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2023","","","","1505","1519","14","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159335366&partnerID=40&md5=18751a23cec95043a041dc9679c373e5","Standalone orchestrators simplify the development of serverless applications by providing higher-level programming interfaces, coordinating function interactions and ensuring exactly-once execution. However, they limit application flexibility and are expensive to use. We show that these specialized orchestration services are unnecessary. Instead, application-level orchestration, deployed as a library, can support the same programming interfaces, complex interactions and execution guarantees, utilizing only basic serverless components that are already universally supported and billed at a fine-grained per-use basis. Furthermore, application-level orchestration affords applications more flexibility and reduces costs for both providers and users. To demonstrate this, we present Unum, an application-level serverless orchestration system. Unum introduces an intermediate representation that partitions higher-level application definitions at compile-time and provides orchestration as a runtime library that executes in-situ with user-defined FaaS functions. On unmodified serverless infrastructures, Unum functions coordinate and ensure correctness in a decentralized manner by leveraging strongly consistent data stores. Compared with AWS Step Functions, a state-of-the-art standalone orchestrator, our evaluation shows that Unum performs well, costs significantly less and grants applications greater flexibility to employ application-specific patterns and optimizations. For a representative set of applications, Unum runs as much as 2x faster and costs 9x cheaper. © NSDI 2023.All rights reserved","","Application level; Compile time; Decentralised; Fine grained; High level applications; High-level programming; Intermediate representations; Programming interface; Reduce costs; Run-time library; Petroleum reservoir evaluation","USENIX Association","Amazon; et al.; Futurewei Technologies; Meta; NSF; USENIX Association","20th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2023","17 April 2023 through 19 April 2023","Boston","188341","Conference paper","Final","","Scopus","2-s2.0-85159335366"
"Yu M.; Cao T.; Wang W.; Chen R.","Yu, Minchen (57205201325); Cao, Tingjia (57284355300); Wang, Wei (57234263600); Chen, Ruichuan (23388102000)","57205201325; 57284355300; 57234263600; 23388102000","Following the Data, Not the Function: Rethinking Function Orchestration in Serverless Computing","2023","Proceedings of the 20th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2023","","","","1489","1504","15","36","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143252548&partnerID=40&md5=1f93269f9608e8499f6ea49bdcfa9add","Serverless applications are typically composed of function workflows in which multiple short-lived functions are triggered to exchange data in response to events or state changes. Current serverless platforms coordinate and trigger functions by following high-level invocation dependencies but are oblivious to the underlying data exchanges between functions. This design is neither efficient nor easy to use in orchestrating complex workflows - developers often have to manage complex function interactions by themselves, with customized implementation and unsatisfactory performance. In this paper, we argue that function orchestration should follow a data-centric approach. In our design, the platform provides a data bucket abstraction to hold the intermediate data generated by functions. Developers can use a rich set of data trigger primitives to control when and how the output of each function should be passed to the next functions in a workflow. By making data consumption explicit and allowing it to trigger functions and drive the workflow, complex function interactions can be easily and efficiently supported. We present Pheromone - a scalable, low-latency serverless platform following this data-centric design. Compared to well-established commercial and open-source platforms, Pheromone cuts the latencies of function interactions and data exchanges by orders of magnitude, scales to large workflows, and enables easy implementation of complex applications. © NSDI 2023.All rights reserved","","Digital storage; 'current; Complex functions; Complex workflows; Coordinate functions; Data-centric approaches; Low latency; Performance; States change; Trigger functions; Work-flows; Electronic data interchange","USENIX Association","Amazon; et al.; Futurewei Technologies; Meta; NSF; USENIX Association","20th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2023","17 April 2023 through 19 April 2023","Boston","188341","Conference paper","Final","","Scopus","2-s2.0-85143252548"
"Li Z.; Cheng J.; Chen Q.; Guan E.; Bian Z.; Tao Y.; Zha B.; Wang Q.; Han W.; Guo M.","Li, Zijun (57278389100); Cheng, Jiagan (57393989800); Chen, Quan (36623232500); Guan, Eryu (57949460700); Bian, Zizheng (57209605096); Tao, Yi (57949222200); Zha, Bin (57948855200); Wang, Qiang (57949460800); Han, Weidong (57948735000); Guo, Minyi (7201564780)","57278389100; 57393989800; 36623232500; 57949460700; 57209605096; 57949222200; 57948855200; 57949460800; 57948735000; 7201564780","RunD: A Lightweight Secure Container Runtime for High-density Deployment and High-concurrency Startup in Serverless Computing","2022","Proceedings of the 2022 USENIX Annual Technical Conference, ATC 2022","","","","53","68","15","63","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140961990&partnerID=40&md5=d5cbdb16593cfd66ce2dee1888cb3b16","The secure container that hosts a single container in a micro virtual machine (VM) is now used in serverless computing, as the containers are isolated through the microVMs. There are high demands on the high-density container deployment and high-concurrency container startup to improve both the resource utilization and user experience, as user functions are fine-grained in serverless platforms. Our investigation shows that the entire software stacks, containing the cgroups in the host operating system, the guest operating system, and the container rootfs for the function workload, together result in low deployment density and slow startup performance at high-concurrency. We propose and implement a lightweight secure container runtime, named RunD, to resolve the above problems through a holistic guest-tohost solution. With RunD, over 200 secure containers can be started in a second, and over 2,500 secure containers can be deployed on a node with 384GB of memory. RunD is adopted as Alibaba serverless container runtime to support high-density deployment and high-concurrency startup. © 2022 USENIX Annual Technical Conference, ATC 2022.All rights reserved.","","Fine grained; Guest operating systems; High concurrencies; High demand; Resources utilizations; Runtimes; Software stacks; Start-up performance; Users' experiences","USENIX Association","Alibaba Group; Apple; et al.; PagerDuty; USENIX Association; VMware University Research Fund","2022 USENIX Annual Technical Conference, ATC 2022","11 July 2022 through 13 July 2022","Carlsbad","183226","Conference paper","Final","","Scopus","2-s2.0-85140961990"
"Zhang H.; Cardoza A.; Chen P.B.; Angel S.; Liu V.","Zhang, Haoran (57219048671); Cardoza, Adney (57220066952); Chen, Peter Baile (59442045800); Angel, Sebastian (55841208800); Liu, Vincent (54684321900)","57219048671; 57220066952; 59442045800; 55841208800; 54684321900","Fault-tolerant and transactional stateful serverless workflows","2020","Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2020","","","","1187","1204","17","66","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096802786&partnerID=40&md5=8c06c0db4f42a51607bbacbd9dd15aa2","This paper introduces Beldi, a library and runtime system for writing and composing fault-tolerant and transactional stateful serverless functions. Beldi runs on existing providers and lets developers write complex stateful applications that require fault tolerance and transactional semantics without the need to deal with tasks such as load balancing or maintaining virtual machines. Beldi's contributions include extending the log-based fault-tolerant approach in Olive (OSDI 2016) with new data structures, transaction protocols, function invocations, and garbage collection. They also include adapting the resulting framework to work over a federated environment where each serverless function has sovereignty over its own data. We implement three applications on Beldi, including a movie review service, a travel reservation system, and a social media site. Our evaluation on 1,000 AWS Lambdas shows that Beldi's approach is effective and affordable. © 2020 Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2020. All rights reserved.","","Balancing; Fault tolerance; Reservation systems; Semantics; Systems analysis; Fault-tolerant; Garbage collection; Movie reviews; Runtime systems; Social media; Transaction protocol; Travel reservations; Work-flows; Fault tolerant computer systems","USENIX Association","Alibaba Group; Alipay; Amazon; Ant Group; et al.; USENIX","14th USENIX Symposium on Operating Systems Design and Implementation,OSDI 2020","4 November 2020 through 6 November 2020","Virtual, Online","164991","Conference paper","Final","","Scopus","2-s2.0-85096802786"
"Rausch T.; Hummer W.; Muthusamy V.; Dustdar S.; Rashed A.","Rausch, Thomas (8582596600); Hummer, Waldemar (36716249700); Muthusamy, Vinod (7801671473); Dustdar, Schahram (6701473617); Rashed, Alexander (57218219414)","8582596600; 36716249700; 7801671473; 6701473617; 57218219414","Towards a serverless platform for edge AI","2019","2nd USENIX Workshop on Hot Topics in Edge Computing, HotEdge 2019, co-located with USENIX ATC 2019","","","","","","","72","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086444901&partnerID=40&md5=7763b1b45d283c0da7669815841496c7","This paper proposes a serverless platform for building and operating edge AI applications. We analyze edge AI use cases to illustrate the challenges in building and operating AI applications in edge cloud scenarios. By elevating concepts from AI lifecycle management into the established serverless model, we enable easy development of edge AI workflow functions. We take a deviceless approach, i.e., we treat edge resources transparently as cluster resources, but give developers fine-grained control over scheduling constraints. Furthermore, we demonstrate the limitations of current serverless function schedulers, and present the current state of our prototype. © 2019 USENIX Association. All rights reserved.","","Edge computing; Life cycle; AI applications; Edge clouds; Edge resources; Fine-grained control; In-buildings; Life-cycle management; Scheduling constraints; Scheduling","USENIX Association","","2nd USENIX Workshop on Hot Topics in Edge Computing, HotEdge 2019, co-located with USENIX ATC 2019","9 July 2019","Renton","155932","Conference paper","Final","","Scopus","2-s2.0-85086444901"
"Boucher S.; Kalia A.; Andersen D.G.; Kaminsky M.","Boucher, Sol (57215283637); Kalia, Anuj (56367885300); Andersen, David G. (57210522272); Kaminsky, Michael (35233511800)","57215283637; 56367885300; 57210522272; 35233511800","Putting the “micro” back in microservice","2020","Proceedings of the 2018 USENIX Annual Technical Conference, USENIX ATC 2018","","","","645","650","5","70","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060929156&partnerID=40&md5=01b3057f899a9e50397254abe5b659ed","Modern cloud computing environments strive to provide users with fine-grained scheduling and accounting, as well as seamless scalability. The most recent face to this trend is the “serverless” model, in which individual functions, or microservices, are executed on demand. Popular implementations of this model, however, operate at a relatively coarse granularity, occupying resources for minutes at a time and requiring hundreds of milliseconds for a cold launch. In this paper, we describe a novel design for providing “functions as a service” (FaaS) that attempts to be truly micro: cold launch times in microseconds that enable even finer-grained resource accounting and support latency-critical applications. Our proposal is to eschew much of the traditional serverless infrastructure in favor of language-based isolation. The result is microsecond-granularity launch latency, and microsecond-scale preemptive scheduling using high-precision timers. © Proceedings of the 2018 USENIX Annual Technical Conference, USENIX ATC 2018. All rights reserved.","","Cloud computing environments; Critical applications; Fine grained; High-precision; Novel design; On demands; Pre-emptive scheduling; Resource accountings; Scheduling","USENIX Association","et al.; Facebook; NetApp; NSF; Oracle; The USENIX Association","2018 USENIX Annual Technical Conference, USENIX ATC 2018","11 July 2018 through 13 July 2018","Boston","155411","Conference paper","Final","","Scopus","2-s2.0-85060929156"
"Liu Q.; Yang Y.; Du D.; Xia Y.; Zhang P.; Feng J.; Larus J.R.; Chen H.","Liu, Qingyuan (57219510222); Yang, Yanning (58955147700); Du, Dong (57200438686); Xia, Yubin (7403027696); Zhang, Ping (58743503200); Feng, Jia (58955147800); Larus, James R. (7003695813); Chen, Haibo (55743141500)","57219510222; 58955147700; 57200438686; 7403027696; 58743503200; 58955147800; 7003695813; 55743141500","Harmonizing Efficiency and Practicability: Optimizing Resource Utilization in Serverless Computing with JIAGU","2024","Proceedings of the 2024 USENIX Annual Technical Conference, ATC 2024","","","","1","17","16","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201231038&partnerID=40&md5=f95872f429ee11537c15864ca5fa2037","Current serverless platforms struggle to optimize resource utilization due to their dynamic and fine-grained nature. Conventional techniques like overcommitment and autoscaling fall short, often sacrificing utilization for practicability or incurring performance trade-offs. Overcommitment requires predicting performance to prevent QoS violation, introducing trade-off between prediction accuracy and overheads. Autoscaling requires scaling instances in response to load fluctuations quickly to reduce resource wastage, but more frequent scaling also leads to more cold start overheads. This paper introduces JIAGU to harmonize efficiency with practicability through two novel techniques. First, pre-decision scheduling achieves accurate prediction while eliminating overheads by decoupling prediction and scheduling. Second, dual-staged scaling achieves frequent adjustment of instances with minimum overhead. We have implemented a prototype and evaluated it using real-world applications and traces from the public cloud platform. Our evaluation shows a 54.8% improvement in deployment density over commercial clouds (with Kubernetes) while maintaining QoS, and 81.0%–93.7% lower scheduling costs and a 57.4%–69.3% reduction in cold start latency compared to existing QoS-aware schedulers. © 2024 Proceedings of the 2024 USENIX Annual Technical Conference, ATC 2024. All rights reserved.","","Cost reduction; Economic and social effects; Forecasting; 'current; Autoscaling; Cold-start; Conventional techniques; Fine grained; Performance; Performance tradeoff; Resources utilizations; Scalings; Trade off; Efficiency","USENIX Association","Futurewei Technologies; IBM; Meta; NetApp; NSF; USENIX Association","2024 USENIX Annual Technical Conference, ATC 2024","10 July 2024 through 12 July 2024","Santa Clara","201530","Conference paper","Final","","Scopus","2-s2.0-85201231038"
"Arutyunyan M.; Berestovskyy A.; Bratschi-Kaye A.; Degenbaev U.; Drijvers M.; El-Ashi I.; Kaestle S.; Kashitsyn R.; Kot M.; Pignolet Y.-A.; Rumenov R.; Sarlis D.; Sinpalean A.; Uta A.; Warinschi B.; Zapuc A.","Arutyunyan, Maksym (58774800200); Berestovskyy, Andriy (58774952700); Bratschi-Kaye, Adam (58775046500); Degenbaev, Ulan (35089859500); Drijvers, Manu (55843275200); El-Ashi, Islam (58774849200); Kaestle, Stefan (57203514837); Kashitsyn, Roman (58774849300); Kot, Maciej (58775000100); Pignolet, Yvonne-Anne (32668030600); Rumenov, Rostislav (58774900200); Sarlis, Dimitris (58774800300); Sinpalean, Alin (58774952800); Uta, Alexandru (56440094500); Warinschi, Bogdan (23390939200); Zapuc, Alexandra (58774952900)","58774800200; 58774952700; 58775046500; 35089859500; 55843275200; 58774849200; 57203514837; 58774849300; 58775000100; 32668030600; 58774900200; 58774800300; 58774952800; 56440094500; 23390939200; 58774952900","Decentralized and Stateful Serverless Computing on the Internet Computer Blockchain","2023","Proceedings of the 2023 USENIX Annual Technical Conference, ATC 2023","","","","329","343","14","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177697845&partnerID=40&md5=213c1068f91618515f2f23abf5d70406","The Internet Computer (IC) is a fast and efficient decentralized blockchain-based platform for the execution of general-purpose applications in the form of smart contracts. In other words, the IC service is the antithesis of current serverless computing. Instead of ephemeral, stateless functions operated by a single entity, the IC offers decentralized stateful serverless computation over untrusted, independent datacenters. Developers deploy stateful canisters that serve calls either to end-users or other canisters. The IC programming model is similar to serverless clouds, with applications written in modern languages such as Rust or Python, yet simpler: state is maintained automatically, without developer intervention. In this paper, we identify and address significant systems challenges to enable efficient decentralized stateful serverless computation: scalability, stateful execution through orthogonal persistence, and deterministic scheduling. We describe the design of the IC and characterize its operational data gathered over the past 1.5 years, and its performance. © 2023 by The USENIX Association All Rights Reserved.","","Integrated circuits; 'current; Block-chain; Computer services; Datacenter; Decentralised; Deterministic scheduling; End-users; Modern languages; Programming models; Simple++; Blockchain","USENIX Association","Akamai; et al.; Futurewei Technologies; NSF; USENIX Association; VMWare University Research Fund","2023 USENIX Annual Technical Conference, ATC 2023","10 July 2023 through 12 July 2023","Boston","195092","Conference paper","Final","","Scopus","2-s2.0-85177697845"
"Oakes E.; Yang L.; Zhou D.; Houck K.; Harter T.; Arpaci-Dusseau A.C.; Arpaci-Dusseau R.H.","Oakes, Edward (59793753900); Yang, Leon (59608403100); Zhou, Dennis (57212865338); Houck, Kevin (57195371768); Harter, Tyler (54415799500); Arpaci-Dusseau, Andrea C. (6602169729); Arpaci-Dusseau, Remzi H. (6602342083)","59793753900; 59608403100; 57212865338; 57195371768; 54415799500; 6602169729; 6602342083","SOCK: Rapid task provisioning with serverless-optimized containers","2020","Proceedings of the 2018 USENIX Annual Technical Conference, USENIX ATC 2018","","","","57","69","12","285","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077468687&partnerID=40&md5=4e3d486e4ba7e8e882f63bf763e28a81","Serverless computing promises to provide applications with cost savings and extreme elasticity. Unfortunately, slow application and container initialization can hurt common-case latency on serverless platforms. In this work, we analyze Linux container primitives, identifying scalability bottlenecks related to storage and network isolation. We also analyze Python applications from GitHub and show that importing many popular libraries adds about 100 ms to startup. Based on these findings, we implement SOCK, a container system optimized for serverless workloads. Careful avoidance of kernel scalability bottlenecks gives SOCK an 18× speedup over Docker. A generalized-Zygote provisioning strategy yields an additional 3× speedup. A more sophisticated three-tier caching strategy based on Zygotes provides a 45× speedup over SOCK without Zygotes. Relative to AWS Lambda and OpenWhisk, OpenLambda with SOCK reduces platform overheads by 2.8× and 5.3× respectively in an image processing case study. © Proceedings of the 2018 USENIX Annual Technical Conference, USENIX ATC 2018. All rights reserved.","","Computer operating systems; Image processing; Scalability; Caching strategy; Cost saving; Network isolation; Containers","USENIX Association","et al.; Facebook; NetApp; NSF; Oracle; The USENIX Association","2018 USENIX Annual Technical Conference, USENIX ATC 2018","11 July 2018 through 13 July 2018","Boston","155411","Conference paper","Final","","Scopus","2-s2.0-85077468687"
"Mahgoub A.; Chaterji S.; Bagchi S.; Shankar K.; Mitra S.; Klimovic A.","Mahgoub, Ashraf (56878717700); Chaterji, Somali (18133715900); Bagchi, Saurabh (14821898700); Shankar, Karthick (57215202832); Mitra, Subrata (57197251717); Klimovic, Ana (56039431800)","56878717700; 18133715900; 14821898700; 57215202832; 57197251717; 56039431800","SONIC: Application-aware data passing for chained serverless applications","2021","2021 USENIX Annual Technical Conference","","","","973","988","15","31","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110765384&partnerID=40&md5=c26a32e6adf16f14186a0635df125f1c","Data analytics applications are increasingly leveraging serverless execution environments for their ease-of-use and pay-as-you-go billing. The structure of such applications is usually composed of multiple functions that are chained together to form a workflow. The current approach of exchanging intermediate (ephemeral) data between functions is through a remote storage (such as S3), which introduces significant performance overhead. We compare three datapassing methods, which we call VM-Storage, Direct-Passing, and state-of-practice Remote-Storage. Crucially, we show that no single data-passing method prevails under all scenarios and the optimal choice depends on dynamic factors such as the size of input data, the size of intermediate data, the application's degree of parallelism, and network bandwidth. We propose SONIC, a data-passing manager that optimizes application performance and cost, by transparently selecting the optimal data-passing method for each edge of a serverless workflow DAG and implementing communication-aware function placement. SONIC monitors application parameters and uses simple regression models to adapt its hybrid data passing accordingly. We integrate SONIC with Open- Lambda and evaluate the system on Amazon EC2 with three analytics applications, popular in the serverless environment. SONIC provides lower latency (raw performance) and higher performance/$ across diverse conditions, compared to four baselines: SAND, vanilla OpenLambda, OpenLambda with Pocket, and AWS Lambda. © 2021 USENIX Annual Technical Conference. All rights reserved.","","Data Analytics; Regression analysis; Application parameters; Application performance; Communication-aware; Degree of parallelism; Execution environments; Multiple function; Network bandwidth; State of practice; Digital storage","USENIX Association","Facebook; Google; IBM; USENIX; VMware","2021 USENIX Annual Technical Conference, ATC 2021","14 July 2021 through 16 July 2021","Virtual, Online","170531","Conference paper","Final","","Scopus","2-s2.0-85110765384"
"","","","Proceedings of the 2024 USENIX Annual Technical Conference, ATC 2024","2024","Proceedings of the 2024 USENIX Annual Technical Conference, ATC 2024","","","","","","1290","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201232280&partnerID=40&md5=ea60c5580095724f1bddf9d0747e53e6","The proceedings contain 77 papers. The topics discussed include: harmonizing efficiency and practicability: optimizing resource utilization in serverless computing with Jiagu; Alps: an adaptive learning, priority OS scheduler for serverless functions; starburst: a cost-aware scheduler for hybrid cloud; StreamBox: a lightweight GPU SandBox for serverless inference workflow; fast inference for probabilistic graphical models; cost-efficient large language model serving for multi-turn conversations with CachedAttention; ScalaAFA: constructing user-space all-flash array engine with holistic designs; FastCommit: resource-efficient, performant and cost-effective file system journaling; ZMS: zone abstraction for mobile flash storage; and ZMS: zone abstraction for mobile flash storage.","","","USENIX Association","Futurewei Technologies; IBM; Meta; NetApp; NSF; USENIX Association","2024 USENIX Annual Technical Conference, ATC 2024","10 July 2024 through 12 July 2024","Santa Clara","201530","Conference review","Final","","Scopus","2-s2.0-85201232280"
"Elhemali M.; Gallagher N.; Gordon N.; Idziorek J.; Krog R.; Lazier C.; Mo E.; Mritunjai A.; Perianayagam S.; Rath T.; Sivasubramanian S.; Sorenson J.C.; Sosothikul S.; Terry D.; Vig A.","Elhemali, Mostafa (22937823800); Gallagher, Niall (59432146800); Gordon, Nicholas (57949300800); Idziorek, Joseph (36975519700); Krog, Richard (57948812600); Lazier, Colin (57948812700); Mo, Erben (57949055000); Mritunjai, Akhilesh (57948812800); Perianayagam, Somu (18434540400); Rath, Tim (56573713200); Sivasubramanian, Swami (7006311951); Sorenson, James Christopher Iii (57948687600); Sosothikul, Sroaj (57948931800); Terry, Doug (7102494175); Vig, Akshat (57949421200)","22937823800; 59432146800; 57949300800; 36975519700; 57948812600; 57948812700; 57949055000; 57948812800; 18434540400; 56573713200; 7006311951; 57948687600; 57948931800; 7102494175; 57949421200","Amazon DynamoDB: A Scalable, Predictably Performant, and Fully Managed NoSQL Database Service","2022","Proceedings of the 2022 USENIX Annual Technical Conference, ATC 2022","","","","1037","1048","11","42","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140970753&partnerID=40&md5=1244e2c2e9584f9033bd508b43090146","Amazon DynamoDB is a NoSQL cloud database service that provides consistent performance at any scale. Hundreds of thousands of customers rely on DynamoDB for its fundamental properties: consistent performance, availability, durability, and a fully managed serverless experience. In 2021, during the 66-hour Amazon Prime Day shopping event, Amazon systems - including Alexa, the Amazon.com sites, and Amazon fulfillment centers, made trillions of API calls to DynamoDB, peaking at 89.2 million requests per second, while experiencing high availability with single-digit millisecond performance. Since the launch of DynamoDB in 2012, its design and implementation have evolved in response to our experiences operating it. The system has successfully dealt with issues related to fairness, traffic imbalance across partitions, monitoring, and automated system operations without impacting availability or performance. Reliability is essential, as even the slightest disruption can significantly impact customers. This paper presents our experience operating DynamoDB at a massive scale and how the architecture continues to evolve to meet the ever-increasing demands of customer workloads. © 2022 USENIX Annual Technical Conference, ATC 2022.All rights reserved.","","Automation; API calls; Cloud database; Consistent performance; Database service; Design and implementations; Fundamental properties; High availability; Partition systems; Performance; Traffic imbalance; Sales","USENIX Association","Alibaba Group; Apple; et al.; PagerDuty; USENIX Association; VMware University Research Fund","2022 USENIX Annual Technical Conference, ATC 2022","11 July 2022 through 13 July 2022","Carlsbad","183226","Conference paper","Final","","Scopus","2-s2.0-85140970753"
"Kumar N.; Ruiz P.M.; Menon V.; Kabiljo I.; Pundir M.; Newell A.; Lee D.; Wang L.; Tang C.","Kumar, Neeraj (57217716370); Ruiz, Pol Mauri (57215321820); Menon, Vijay (58493940200); Kabiljo, Igor (57190984397); Pundir, Mayank (54403600100); Newell, Andrew (57215316993); Lee, Daniel (59261743500); Wang, Liyuan (59261197300); Tang, Chunqiang (59844235800)","57217716370; 57215321820; 58493940200; 57190984397; 54403600100; 57215316993; 59261743500; 59261197300; 59844235800","Optimizing Resource Allocation in Hyperscale Datacenters: Scalability, Usability, and Experiences","2024","Proceedings of the 18th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2024","","","","507","528","21","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201319254&partnerID=40&md5=d5f0f1d7256bbce047580e7a448c7744","Meta’s private cloud uses millions of servers to host tens of thousands of services that power multiple products for billions of users. This complex environment has various optimization problems involving resource allocation, including hardware placement, server allocation, ML training & inference placement, traffic routing, database & container migration for load balancing, grouping serverless functions for locality, etc. The main challenges for a reusable resource-allocation framework are its usability and scalability. Usability is impeded by practitioners struggling to translate real-life policies into precise mathematical formulas required by formal optimization methods, while scalability is hampered by NP-hard problems that cannot be solved efficiently by commercial solvers. These challenges are addressed by Rebalancer, Meta’s resource-allocation framework. It has been applied to dozens of large-scale use cases over the past seven years, demonstrating its usability, scalability, and generality. At the core of Rebalancer is an expression graph that enables its optimization algorithm to run more efficiently than past algorithms. Moreover, Rebalancer offers a high-level specification language to lower the barrier for adoption by systems practitioners. © OSDI 2024.All rights reserved.","","NP-hard; Resource allocation; Reusability; Scalability; Complex environments; Datacenter; Load-Balancing; Multiple products; Optimization problems; Power; Private clouds; Resources allocation; Reusable resources; Traffic routing; High level languages","USENIX Association","Amazon; Databricks; et al.; Futurewei Technologies; Roblox; USENIX Association","18th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2024","10 July 2024 through 12 July 2024","Santa Clara","201531","Conference paper","Final","","Scopus","2-s2.0-85201319254"
"","","","Proceedings of the 2018 USENIX Annual Technical Conference, USENIX ATC 2018","2018","Proceedings of the 2018 USENIX Annual Technical Conference, USENIX ATC 2018","","","","","","1025","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085266048&partnerID=40&md5=95bfb99235a002fa026854d09f7d4b47","The proceedings contain 76 papers. The topics discussed include: FastTrack: foreground app-aware I/O management for improving user experience  of android smartphones; mainstream: dynamic stem-sharing for multi-tenant video processing; SOCK: rapid task provisioning with serverless-optimized containers; DynaMix: dynamic mobile device integration for efficient cross-device resource sharing; the design and implementation of hyperupcalls; aiql: enabling efficient attack investigation from system monitoring data; application memory isolation on ultra-low-power MCUs; peeking behind the curtains of serverless platforms; KylinX: a dynamic library operating system for simplified and efficient cloud virtualization; and throwhammer: rowhammer attacks over the network and defenses.","","","USENIX Association","et al.; Facebook; NetApp; NSF; Oracle; The USENIX Association","2018 USENIX Annual Technical Conference, USENIX ATC 2018","11 July 2018 through 13 July 2018","Boston","155411","Conference review","Final","","Scopus","2-s2.0-85085266048"
"Zhang Z.; Jin C.; Jin X.","Zhang, Zili (57818122600); Jin, Chao (57799003600); Jin, Xin (57189270771)","57818122600; 57799003600; 57189270771","Jolteon: Unleashing the Promise of Serverless for Serverless Workflows","2024","Proceedings of the 21st USENIX Symposium on Networked Systems Design and Implementation, NSDI 2024","","","","167","183","16","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194132930&partnerID=40&md5=d8b50bcbfc5eb3720777afa2c8a02578","Serverless computing promises automatic resource provisioning to relieve the burden of developers. Yet, developers still have to manually configure resources on current serverless platforms to satisfy application-level requirements. This is because cloud applications are orchestrated as serverless workflows with multiple stages, exhibiting a complex relationship between resource configuration and application requirements. We propose Jolteon, an orchestrator to unleash the promise of automatic resource provisioning for serverless workflows. At the core of Jolteon is a stochastic performance model that combines the benefits of whitebox modeling to capture the execution characteristics of serverless computing and blackbox modeling to accommodate the inherent performance variability. We formulate a chance constrained optimization problem based on the performance model, and exploit sampling and convexity to find optimal resource configurations that satisfy user-defined cost or latency bounds. We implement a system prototype of Jolteon and evaluate it on AWS Lambda with a variety of serverless workflows. The experimental results show that Jolteon outperforms the state-of-the-art solution, Orion, by up to 2.3× on cost and 2.1× on latency. © 2024 Proceedings of the 21st USENIX Symposium on Networked Systems Design and Implementation, NSDI 2024. All rights reserved.","","Stochastic models; Stochastic systems; Systems analysis; Application level; Application requirements; Cloud applications; Complex relationships; Multiple stages; On currents; On-currents; Performance Modeling; Resource configurations; Work-flows; Constrained optimization","USENIX Association","ByteDance; et al.; Futurewei Technologies; Meta; NSF; USENIX Association","21st USENIX Symposium on Networked Systems Design and Implementation, NSDI 2024","16 April 2024 through 18 April 2024","Santa Clara","199493","Conference paper","Final","","Scopus","2-s2.0-85194132930"
"Lazarev N.; Gohil V.; Tsai J.; Anderson A.; Chitlur B.; Zhang Z.; Delimitrou C.","Lazarev, Nikita (57212142647); Gohil, Varun (57204718090); Tsai, James (59261185700); Anderson, Andy (58556921600); Chitlur, Bhushan (36986786800); Zhang, Zhiru (57211081307); Delimitrou, Christina (38361424300)","57212142647; 57204718090; 59261185700; 58556921600; 36986786800; 57211081307; 38361424300","Sabre: Hardware-Accelerated Snapshot Compression for Serverless MicroVMs","2024","Proceedings of the 18th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2024","","","","1","18","17","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201303664&partnerID=40&md5=126d0b91995a9e6af698e2c6ad0dce9d","MicroVM snapshotting significantly reduces the cold start overheads in serverless applications. Snapshotting enables storing part of the physical memory of a microVM guest into a file, and later restoring from it to avoid long cold start-up times. Prefetching memory pages from snapshots can further improve the effectiveness of snapshotting. However, the efficacy of prefetching depends on the size of the memory that needs to be restored. Lossless page compression is therefore a great way to improve the coverage of the memory footprint that snapshotting with prefetching achieves. Unfortunately, the high overhead and high CPU cost of software-based (de)compression make this impractical. We introduce Sabre, a novel approach to snapshot page prefetching based on hardware-accelerated (de)compression. Sabre leverages an increasingly pervasive near-memory analytics accelerator available in modern datacenter processors. We show that by appropriately leveraging such accelerators, microVM snapshots of serverless applications can be compressed up to a factor of 4.5×, with nearly negligible decompression costs. We use this insight to build an efficient page prefetching library capable of speeding up memory restoration from snapshots by up to 55%. We integrate the library with the production-grade Firecracker microVMs and evaluate its end-to-end performance on a wide set of serverless applications. © OSDI 2024.All rights reserved.","","Aspect oriented programming; Ferroelectric RAM; Program debugging; Cold-start; Datacenter; Hardware-accelerated; Lossless; Memory footprint; Memory pages; Page compressions; Physical memory; Prefetching; Startup time; Restoration","USENIX Association","Amazon; Databricks; et al.; Futurewei Technologies; Roblox; USENIX Association","18th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2024","10 July 2024 through 12 July 2024","Santa Clara","201531","Conference paper","Final","","Scopus","2-s2.0-85201303664"
"","","","Proceedings of the 21st USENIX Symposium on Networked Systems Design and Implementation, NSDI 2024","2024","Proceedings of the 21st USENIX Symposium on Networked Systems Design and Implementation, NSDI 2024","","","","","","2069","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194163818&partnerID=40&md5=756bfcdd7bc522e4e3bc6a9f587e892b","The proceedings contain 112 papers. The topics discussed include: Horus: granular in-network task scheduler for cloud datacenters; making kernel bypass practical for the cloud with junction; flow scheduling with imprecise knowledge; Pudica: toward near-zero queuing delay in congestion control for cloud gaming; autothrottle: a practical bi-level approach to resource management for SLO-targeted microservices; Jolteon: unleashing the promise of serverless for serverless workflows; can’t be late: optimizing spot instance savings under deadlines; towards intelligent automobile cockpit via a new container architecture; TECC: towards efficient QUIC tunneling via collaborative transmission control; iStack: a general and stateful name-based protocol stack for named data networking; and the bedrock of byzantine fault tolerance: a unified platform for BFT protocols analysis, implementation, and experimentation.","","","USENIX Association","ByteDance; et al.; Futurewei Technologies; Meta; NSF; USENIX Association","21st USENIX Symposium on Networked Systems Design and Implementation, NSDI 2024","16 April 2024 through 18 April 2024","Santa Clara","199493","Conference review","Final","","Scopus","2-s2.0-85194163818"
"Razavi K.; Trivedi A.","Razavi, Kaveh (55532049200); Trivedi, Animesh (55014115500)","55532049200; 55014115500","Stratus: Clouds with microarchitectural resource management","2020","HotCloud 2020 - 12th USENIX Workshop on Hot Topics in Cloud Computing, co-located with USENIX ATC 2020","","","","","","","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091892505&partnerID=40&md5=c14f7c8fec11d4dcefff2e994aefa6ec","The emerging next generation of cloud services like Granular and Serverless computing are pushing the boundaries of the current cloud infrastructure. In order to meet the performance objectives, researchers are now leveraging low-level microarchitectural resources in clouds. At the same time these resources are also a major source of security problems that can compromise the confidentiality and integrity of sensitive data in multi-tenant shared cloud infrastructures. The core of the problem is the lack of isolation due to the unsupervised sharing of microarchitectural resources across different performance and security boundaries. In this paper, we introduce Stratus clouds that treat the isolation on microarchitectural elements as the key design principle when allocating cloud resources. This isolation improves both performance and security, but at the cost of reducing resource utilization. Stratus captures this trade-off using a novel abstraction that we call isolation credit, and show how it can help both providers and tenants when allocating microarchitectural resources using Stratus's declarative interface. We conclude by discussing the challenges of realizing Stratus clouds today. © HotCloud 2020 - 12th USENIX Workshop on Hot Topics in Cloud Computing, co-located with USENIX ATC 2020. All rights reserved.","","Cloud computing; Cloud infrastructures; Cloud services; Design Principles; Performance objective; Resource management; Resource utilizations; Security problems; Sensitive datas; Economic and social effects","USENIX Association","Google; NetApp; VMware","12th USENIX Workshop on Hot Topics in Cloud Computing, HotCloud 2020, co-located with USENIX ATC 2020","13 July 2020 through 14 July 2020","Virtual, Online","162465","Conference paper","Final","","Scopus","2-s2.0-85091892505"
"Klimovic A.; Wang Y.; Stuedi P.; Trivedi A.; Pfefferle J.; Kozyrakis C.","Klimovic, Ana (56039431800); Wang, Yawen (57212459829); Stuedi, Patrick (8966357900); Trivedi, Animesh (55014115500); Pfefferle, Jonas (56429712500); Kozyrakis, Christos (6602525246)","56039431800; 57212459829; 8966357900; 55014115500; 56429712500; 6602525246","Pocket: Elastic ephemeral storage for serverless analytics","2007","Proceedings of the 13th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2018","","","","427","444","17","238","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066898920&partnerID=40&md5=da326ee1022f305db162aea4ee9f568e","Serverless computing is becoming increasingly popular, enabling users to quickly launch thousands of short-lived tasks in the cloud with high elasticity and fine-grain billing. These properties make serverless computing appealing for interactive data analytics. However exchanging intermediate data between execution stages in an analytics job is a key challenge as direct communication between serverless tasks is difficult. The natural approach is to store such ephemeral data in a remote data store. However, existing storage systems are not designed to meet the demands of serverless applications in terms of elasticity, performance, and cost. We present Pocket, an elastic, distributed data store that automatically scales to provide applications with desired performance at low cost. Pocket dynamically rightsizes resources across multiple dimensions (CPU cores, network bandwidth, storage capacity) and leverages multiple storage technologies to minimize cost while ensuring applications are not bottlenecked on I/O. We show that Pocket achieves similar performance to ElastiCache Redis for serverless analytics applications while reducing cost by almost 60%. © Proceedings of NSDI 2010: 7th USENIX Symposium on Networked Systems Design and Implementation. All rights reserved.","","Data Analytics; Elasticity; Systems analysis; Direct communications; Distributed data stores; Multiple dimensions; Natural approaches; Network bandwidth; Storage capacity; Storage systems; Storage technology; Digital storage","USENIX Association","Alibaba Group; Ant Financial; Apple; et al.; Google; The USENIX Association","13th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2018","8 October 2018 through 10 October 2018","Carlsbad","155341","Conference paper","Final","","Scopus","2-s2.0-85066898920"
"Haeberlen A.; Mislove A.; Druschel P.","Haeberlen, Andreas (6507228730); Mislove, Alan (8693841200); Druschel, Peter (6701688597)","6507228730; 8693841200; 6701688597","Glacier: Highly durable, decentralized storage despite massive correlated failures","2005","2nd Symposium on Networked Systems Design and Implementation, NSDI 2005","","","","143","158","15","175","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85001706405&partnerID=40&md5=766018b0860c35e322f720da143023f5","Decentralized storage systems aggregate the available disk space of participating computers to provide a large storage facility. These systems rely on data redundancy to ensure durable storage despite of node failures. However, existing systems either assume independent node failures, or they rely on introspection to carefully place redundant data on nodes with low expected failure correlation. Unfortunately, node failures are not independent in practice and constructing an accurate failure model is difficult in large-scale systems. At the same time, malicious worms that propagate through the Internet pose a real threat of large-scale correlated failures. Such rare but potentially catastrophic failures must be considered when attempting to provide highly durable storage. In this paper, we describe Glacier, a distributed storage system that relies on massive redundancy to mask the effect of large-scale correlated failures. Glacier is designed to aggressively minimize the cost of this redundancy in space and time: Erasure coding and garbage collection reduces the storage cost; aggregation of small objects and a loosely coupled maintenance protocol for redundant fragments minimizes the messaging cost. In one configuration, for instance, our system can provide six-nines durable storage despite correlated failures of up to 60% of the storage nodes, at the cost of an eleven-fold storage overhead and an average messaging overhead of only 4 messages per node and minute during normal operation. Glacier is used as the storage layer for an experimental serverless email system. © NSDI 2005.","","Large scale systems; Multiprocessing systems; Redundancy; Systems analysis; Catastrophic failures; Correlated failures; Distributed storage system; Failure correlation; Garbage collection; Normal operations; Storage facilities; Storage overhead; Digital storage","USENIX Association","HP; Microsoft Research; Nation Science Foundation; USENIX Association","2nd Symposium on Networked Systems Design and Implementation, NSDI 2005","2 May 2005 through 4 May 2005","Boston","163485","Conference paper","Final","","Scopus","2-s2.0-85001706405"
"Wu H.; Yu Y.; Deng J.; Ibrahim S.; Wu S.; Fan H.; Cheng Z.; Jin H.","Wu, Hao (59614512300); Yu, Yue (59259620300); Deng, Junxiao (58521928600); Ibrahim, Shadi (58118076000); Wu, Song (56931730600); Fan, Hao (57215688890); Cheng, Ziyue (59259727300); Jin, Hai (56434989100)","59614512300; 59259620300; 58521928600; 58118076000; 56931730600; 57215688890; 59259727300; 56434989100","StreamBox: A Lightweight GPU SandBox for Serverless Inference Workflow","2024","Proceedings of the 2024 USENIX Annual Technical Conference, ATC 2024","","","","59","73","14","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201228188&partnerID=40&md5=44bbfdf72202f2b861a4eee567ce7767","The dynamic workload and latency sensitivity of DNN inference drive a trend toward exploiting serverless computing for scalable DNN inference serving. Usually, GPUs are spatially partitioned to serve multiple co-located functions. However, existing serverless inference systems isolate functions in separate monolithic GPU runtimes (e.g., CUDA context), which is too heavy for short-lived and fine-grained functions, leading to a high startup latency, a large memory footprint, and expensive inter-function communication. In this paper, we present StreamBox, a new lightweight GPU sandbox for serverless inference workflow. StreamBox unleashes the potential of streams and efficiently realizes them for serverless inference by implementing fine-grain and auto-scaling memory management, allowing transparent and efficient intra-GPU communication across functions, and enabling PCIe bandwidth sharing among concurrent streams. Our evaluations over real-world workloads show that StreamBox reduces the GPU memory footprint by up to 82% and improves throughput by 6.7X compared to state-of-the-art serverless inference systems. © 2024 Proceedings of the 2024 USENIX Annual Technical Conference, ATC 2024. All rights reserved.","","Program processors; Co-located; Fine grained; Finer grains; Inference systems; Memory footprint; Memory-management; Monolithics; Runtimes; Scalings; Work-flows; Graphics processing unit","USENIX Association","Futurewei Technologies; IBM; Meta; NetApp; NSF; USENIX Association","2024 USENIX Annual Technical Conference, ATC 2024","10 July 2024 through 12 July 2024","Santa Clara","201530","Conference paper","Final","","Scopus","2-s2.0-85201228188"
"Fu Y.; Shi R.; Wang H.; Chen S.; Cheng Y.","Fu, Yuqi (58733678000); Shi, Ruizhe (59152970100); Wang, Haoliang (57209143273); Chen, Songqing (9338029700); Cheng, Yue (56022559100)","58733678000; 59152970100; 57209143273; 9338029700; 56022559100","ALPS: An Adaptive Learning, Priority OS Scheduler for Serverless Functions","2024","Proceedings of the 2024 USENIX Annual Technical Conference, ATC 2024","","","","19","36","17","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201208988&partnerID=40&md5=9f95e370382067a6a2eb52fd774d9e91","FaaS (Function-as-a-Service) workloads feature unique patterns. Serverless functions are ephemeral, highly concurrent, and bursty, with an execution duration ranging from a few milliseconds to a few seconds. The workload behaviors pose new challenges to kernel scheduling. Linux CFS (Completely Fair Scheduler) is workload-oblivious and optimizes long-term fairness via proportional sharing. CFS neglects the short-term demands of CPU time from short-lived serverless functions, severely impacting the performance of short functions. Preemptive shortest job first—shortest remaining process time (SRPT)—prioritizes shorter functions in order to satisfy their short-term demands of CPU time, and therefore, serves as a best-case baseline for optimizing the turnaround time of short functions. A significant downside of approximating SRPT, however, is that longer functions might be starved. In this paper, we propose a novel application-aware kernel scheduler, ALPS (Adaptive Learning, Priority Scheduler), based on two key insights. First, approximating SRPT can largely benefit short functions but may inevitably penalize long functions. Second, CFS provides necessary infrastructure support to implement user-defined priority scheduling. To this end, we design ALPS to have a novel, decoupled scheduler frontend and backend architecture, which unifies approximate SRPT and proportional-share scheduling. ALPS’ frontend sits in the user space and approximates SRPT-inspired priority scheduling by adaptively learning from an SRPT simulation on recent past workload. ALPS’ backend uses eBPF functions hooked to CFS to carry out the continuously learned policies sent from the frontend to inform scheduling decisions in the kernel. This design adds workload intelligence to workload-oblivious OS scheduling while retaining desirable properties of OS schedulers. We evaluate ALPS extensively using two production FaaS workloads (Huawei and Azure) and results show that ALPS achieves a reduction of 57.2% in average function execution duration, compared to CFS. © 2024 Proceedings of the 2024 USENIX Annual Technical Conference, ATC 2024. All rights reserved.","","Adaptive learning; CPU time; Kernel schedulers; Long term fairness; Novel applications; Performance; Priority scheduling; Process time; Proportional sharing; Turn-around time; Computer operating systems","USENIX Association","Futurewei Technologies; IBM; Meta; NetApp; NSF; USENIX Association","2024 USENIX Annual Technical Conference, ATC 2024","10 July 2024 through 12 July 2024","Santa Clara","201530","Conference paper","Final","","Scopus","2-s2.0-85201208988"
"Chen X.; Li Z.; Jain T.; Narayanan V.; Burtsev A.","Chen, Xiangdong (58712322700); Li, Zhaofeng (57220078031); Jain, Tirth (59259727700); Narayanan, Vikram (57209220520); Burtsev, Anton (57203233225)","58712322700; 57220078031; 59259727700; 57209220520; 57203233225","Limitations and Opportunities of Modern Hardware Isolation Mechanisms","2024","Proceedings of the 2024 USENIX Annual Technical Conference, ATC 2024","","","","349","368","19","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201197006&partnerID=40&md5=659720d2e84769d09d8523ad4d1b320d","A surge in the number, complexity, and automation of targeted security attacks has triggered a wave of interest in hardware support for isolation. Intel memory protection keys (MPK), ARM pointer authentication (PAC), ARM memory tagging extensions (MTE), and ARM Morello capabilities are just a few hardware mechanisms aimed at supporting low-overhead isolation in recent CPUs. These new mechanisms aim to bring practical isolation to a broad range of systems, e.g., browser plugins, device drivers and kernel extensions, user-defined database and network functions, serverless cloud platforms, and many more. However, as these technologies are still nascent, their advantages and limitations are yet unclear. In this work, we do an in-depth look at modern hardware isolation mechanisms with the goal of understanding their suitability for the isolation of subsystems with the tightest performance budgets. Our analysis shows that while a huge step forward, the isolation mechanisms in commodity CPUs are still lacking implementation of several design principles critical for supporting low-overhead enforcement of isolation boundaries, zero-copy exchange of data, and secure revocation of access permissions. © 2024 Proceedings of the 2024 USENIX Annual Technical Conference, ATC 2024. All rights reserved.","","ARM processors; Budget control; Computer hardware; Cloud platforms; Device Driver; Hardware mechanism; Hardware supports; Low overhead; Memory protection; Network functions; New mechanisms; Plug-ins; Security attacks; Program processors","USENIX Association","Futurewei Technologies; IBM; Meta; NetApp; NSF; USENIX Association","2024 USENIX Annual Technical Conference, ATC 2024","10 July 2024 through 12 July 2024","Santa Clara","201530","Conference paper","Final","","Scopus","2-s2.0-85201197006"
"Sadeghian G.; Elsakhawy M.; Shahrad M.; Hattori J.; Shahrad M.","Sadeghian, Ghazal (58774859900); Elsakhawy, Mohamed (57199514528); Shahrad, Mohanna (58775156400); Hattori, Joe (58041984900); Shahrad, Mohammad (56943394900)","58774859900; 57199514528; 58775156400; 58041984900; 56943394900","UnFaaSener: Latency and Cost Aware Offloading of Functions from Serverless Platforms","2023","Proceedings of the 2023 USENIX Annual Technical Conference, ATC 2023","","","","879","896","17","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178516316&partnerID=40&md5=d76d2cfbd34c2fedb0c464e83450d2a4","We present UnFaaSener, a lightweight framework that enables serverless users to reduce their bills by harvesting non-serverless compute resources such as their VMs, on-premise servers, or personal computers. UnFaaSener is not a new serverless platform, nor does it require any support from today’s production serverless platforms. It uses existing pub/sub services as the glue between the serverless application and offloading hosts. UnFaaSener’s asynchronous scheduler takes into consideration the projected resource availability of the offloading hosts, various latency and cost components of serverless versus offloaded execution, the structure of the serverless application, and the developer’s QoS expectations to find the most optimal offloading decisions. These decisions are then stored to be retrieved and propagated through the execution flow of the serverless application. The system supports partial offloading at the resolution of each function and utilizes several design choices to establish confidence and adaptiveness. We evaluate the effectiveness of UnFaaSener for serverless applications with various structures. UnFaaSener was able to deliver cost savings of up to 89.8% based on the invocation pattern and the structure of the application, when we limited the offloading cap to 90% in our experiments. © 2023 by The USENIX Association All Rights Reserved.","","Adaptiveness; Compute resources; Cost components; Cost saving; Cost-aware; Latency-aware; Lightweight frameworks; Pub/sub; Resource availability; Systems support; Personal computers","USENIX Association","Akamai; et al.; Futurewei Technologies; NSF; USENIX Association; VMWare University Research Fund","2023 USENIX Annual Technical Conference, ATC 2023","10 July 2023 through 12 July 2023","Boston","195092","Conference paper","Final","","Scopus","2-s2.0-85178516316"
"Li J.; Zhao L.; Yang Y.; Zhan K.; Li K.","Li, Jie (56313960700); Zhao, Laiping (35243865000); Yang, Yanan (57208671879); Zhan, Kunlin (37111519700); Li, Keqiu (57204189178)","56313960700; 35243865000; 57208671879; 37111519700; 57204189178","TETRIS: Memory-efficient Serverless Inference through Tensor Sharing","2022","Proceedings of the 2022 USENIX Annual Technical Conference, ATC 2022","","","","473","488","15","49","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140978442&partnerID=40&md5=b65b5d61bb2631ee1b9056a8b663dcfa","Executing complex, memory-intensive deep learning inference services poses a major challenge for serverless computing frameworks, which would densely deploy and maintain inference models at high throughput. We observe the excessive memory consumption problem in serverless inference systems, due to the large-sized models and high data redundancy. We present TETRIS, a serverless platform catered to inference services with an order of magnitude lower memory footprint. TETRIS's design carefully considers the extensive memory sharing of runtime and tensors. It supports minimizing the runtime redundancy through a combined optimization of batching and concurrent execution and eliminates tensor redundancy across instances from either the same or different functions using a lightweight and safe tensor mapping mechanism. Our comprehensive evaluation demonstrates that TETRIS saves up to 93% memory footprint for inference services, and increases the function density by 30× without impairing the latency. © 2022 USENIX Annual Technical Conference, ATC 2022.All rights reserved.","","Deep learning; Redundancy; Computing frameworks; Data-redundancy; High-throughput; Inference models; Inference systems; Large-sized; Memory consumption; Memory efficient; Memory footprint; Runtimes; Tensors","USENIX Association","Alibaba Group; Apple; et al.; PagerDuty; USENIX Association; VMware University Research Fund","2022 USENIX Annual Technical Conference, ATC 2022","11 July 2022 through 13 July 2022","Carlsbad","183226","Conference paper","Final","","Scopus","2-s2.0-85140978442"
"Klimovic A.; Wang Y.; Kozyrakis C.; Stuedi P.; Pfefferle J.; Trivedi A.","Klimovic, Ana (56039431800); Wang, Yawen (57212459829); Kozyrakis, Christos (6602525246); Stuedi, Patrick (8966357900); Pfefferle, Jonas (56429712500); Trivedi, Animesh (55014115500)","56039431800; 57212459829; 6602525246; 8966357900; 56429712500; 55014115500","Understanding ephemeral storage for serverless analytics","2020","Proceedings of the 2018 USENIX Annual Technical Conference, USENIX ATC 2018","","","","789","794","5","80","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077459554&partnerID=40&md5=0a5cbfe37397bdbe9359b7d1b113557b","Serverless computing frameworks allow users to launch thousands of concurrent tasks with high elasticity and fine-grain resource billing without explicitly managing computing resources. While already successful for IoT and web microservices, there is increasing interest in leveraging serverless computing to run data-intensive jobs, such as interactive analytics. A key challenge in running analytics workloads on serverless platforms is enabling tasks in different execution stages to efficiently communicate data between each other via a shared data store. In this paper, we explore the suitability of different cloud storage services (e.g., object stores and distributed caches) as remote storage for serverless analytics. Our analysis leads to key insights to guide the design of an ephemeral cloud storage system, including the performance and cost efficiency of Flash storage for serverless application requirements and the need for a pay-what-you-use storage service that can support the high throughput demands of highly parallel applications. © Proceedings of the 2018 USENIX Annual Technical Conference, USENIX ATC 2018. All rights reserved.","","Application requirements; Cloud storage services; Cloud storage systems; Computing frameworks; Computing resource; Distributed cache; Fine grain resources; Storage services; Digital storage","USENIX Association","et al.; Facebook; NetApp; NSF; Oracle; The USENIX Association","2018 USENIX Annual Technical Conference, USENIX ATC 2018","11 July 2018 through 13 July 2018","Boston","155411","Conference paper","Final","","Scopus","2-s2.0-85077459554"
"Cao T.; Arpaci-Dusseau A.C.; Arpaci-Dusseau R.H.; Caraza-Harter T.","Cao, Tingjia (57284355300); Arpaci-Dusseau, Andrea C. (6602169729); Arpaci-Dusseau, Remzi H. (6602342083); Caraza-Harter, Tyler (57213268759)","57284355300; 6602169729; 6602342083; 57213268759","Making Serverless Pay-For-Use a Reality with Leopard","2025","Proceedings of the 22nd USENIX Symposium on Networked Systems Design and Implementation, NSDI 2025","","","","189","204","15","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006422436&partnerID=40&md5=2c00d496cdedd01e394259f8f81b7f79","Serverless computing has gained traction due to its event-driven architecture and “pay for use” (PFU) billing model. However, our analysis reveals that current billing practices do not align with true resource consumption. This paper challenges the prevailing SLIM (static, linear, interactive-only model) assumptions that underpin existing billing models, demonstrating that current billing does not realize PFU for realistic workloads. We introduce the Nearly Pay-for-Use (NPFU) billing model, which accommodates varying CPU and memory demands, spot cores, and preemptible memory. We also introduce Leopard, an NPFU-based serverless platform that integrates billing awareness into several major subsystems: CPU scheduler, OOM killer, admission controller, and cluster scheduler. Experimental results indicate that Leopard benefits both providers and users, increasing throughput by more than 2x and enabling cost reductions. © 2025 by The USENIX Association All Rights Reserved.","","'current; Admission controllers; Costs reduction; Event-driven architectures; Model assumptions; Resources consumption; Memory architecture","USENIX Association","ByteDance; et al.; Futurewei Technologies; Google; Meta; NSF","22nd USENIX Symposium on Networked Systems Design and Implementation, NSDI 2025","28 April 2025 through 30 April 2025","Philadelphia","208686","Conference paper","Final","","Scopus","2-s2.0-105006422436"
