"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"A Structured Literature Review Approach to Define Serverless Computing and Function as a Service","J. Manner","Distributed Systems Group, Otto-Friedrich-University, Bamberg, Germany",2023 IEEE 16th International Conference on Cloud Computing (CLOUD),"25 Sep 2023","2023","","","516","522","Nearly a decade ago, the emergence of an event-driven computing model attracted a lot of attention in industry and academia. This new computing concept has been described with a plethora of terms: Serverless Computing, Function as a Service (FaaS), Serverless functions, cloud functions, server-aware vs. server-less, to name only a few of the most frequently used terms. This enumeration showcases a problem of the domain: the lack of clear terminology, a conceptualization of Serverless and FaaS and a differentiation of the two. To overcome this limitation we present a Structured Literature Review (SLR) and usage trends which help to understand the current usage of the terms. Based on characteristics extracted from several definitions and validated in the course of the SLR we demarcate Serverless and FaaS. Our SLR shows that 90% of the considered papers predomi-nantly use the term Serverless whereas most of the authors intend to write about FaaS in the sense of our definition. Another insight is that the viewpoint on Serverless and FaaS evolved over time and that since 2021 most authors see Serverless as an umbrella term for FaaS and Backend as a Service (BaaS). Some of them also include further cloud service models, i.e. parts of PaaS and SaaS, providing a server-less experience for an application developer. This is an interpretation we share.","2159-6190","979-8-3503-0481-7","10.1109/CLOUD60044.2023.00068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10255020","Serverless Computing;Function as a Service;FaaS;Structured Literature Review;SLR","Industries;Terminology;Computational modeling;Bibliographies;Software as a service;Serverless computing;Search engines","","3","","59","IEEE","25 Sep 2023","","","IEEE","IEEE Conferences"
"Towards Scalable k-out-of-n Models for Assessing the Reliability of Large-Scale Function-as-a-Service Systems with Bayesian Networks","O. Bibartiu; F. Dürr; K. Rothermel; B. Ottenwälder; A. Grau","University of Stuttgart, Institute for Parallel and Distributed Systems (IPVS), Stuttgart, Germany; University of Stuttgart, Institute for Parallel and Distributed Systems (IPVS), Stuttgart, Germany; University of Stuttgart, Institute for Parallel and Distributed Systems (IPVS), Stuttgart, Germany; Robert Bosch GmbH, Bosch IoT Cloud, Stuttgart, Germany; Robert Bosch GmbH, Bosch IoT Cloud, Stuttgart, Germany",2019 IEEE 12th International Conference on Cloud Computing (CLOUD),"29 Aug 2019","2019","","","514","516","Typically, Function-as-a-Service (FaaS) involves state-less replication with very large numbers of instances. The reliability of such services can be evaluated using Bayesian Networks and k-out-of-n models. However, existing k-out-of-n models do not scale to the larger number of hosts of FaaS services. Therefore, we propose a scalable k-out-of-n model in this paper with the same semantics as the standard k-out-of-n voting gates in fault trees, enabling the reliability analysis of FaaS services.","2159-6190","978-1-7281-2705-7","10.1109/CLOUD.2019.00095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8814557","Reliability;Function as a Service;Bayesian Networks;Replication;k out of n;causal independence","","","","","9","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"QoS aware FaaS for Heterogeneous Edge-Cloud continuum","K. R. Sheshadri; J. Lakshmi","Cloud Systems Lab Indian Institute of Science, Bangalore, India; Cloud Systems Lab Indian Institute of Science, Bangalore, India",2022 IEEE 15th International Conference on Cloud Computing (CLOUD),"24 Aug 2022","2022","","","70","80","Function as a Service (FaaS) is one of the widely used serverless computing service offerings to build and deploy applications on the Cloud. The platform is popular for its ""pay-as-you-go"" billing model, microservice-based design, event-driven executions, and autonomous scaling. Although it has its firm roots in Cloud computing service offerings, it is considerably explored in the Edge computing layer. The efficient resource management of FaaS is attractive to Edge computing because of the limited nature of resources. Existing literature on Edge-Cloud FaaS platforms orchestrates compute workloads based on factors such as data locality, resource availability, network costs, and bandwidth. However, the state-of-the-art platforms lack a comprehensive way to address the challenges of managing heterogeneous resources in the FaaS platform. The resource specification in a heterogeneous setting, lack of Quality of Service (QoS) driven resource provisioning, and function deployment exacerbate the problem of resource selection, and function deployment in FaaS platforms with a heterogeneous resource pool. To address these gaps, the current work presents a novel heterogeneous FaaS platform that deduces function resource specification using Machine Learning (ML) methods, performs smart function placement on Edge/Cloud based on a user-specified QoS requirement, and exploit data locality by caching appropriate data for function executions. Experimental results based on real-world workloads on a video surveillance application show that the proposed platform brings efficient resource utilization and cost savings at the Cloud by reducing the resource usage by up to 30%, while improving the performance of function executions by up to 25% at Edge and Cloud.","2159-6190","978-1-6654-8137-3","10.1109/CLOUD55607.2022.00023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9860835","FaaS;Function as a Service;serverless computing;Quality of Service;QoS;Edge Cloud;Edge Cloud FaaS;Edge Cloud continuum;Heterogeneous FaaS platforms;Heterogeneous Edge Cloud platforms;QoS aware FaaS for Heterogeneous Edge Cloud continuum","Cloud computing;Costs;Computational modeling;Serverless computing;Microservice architectures;FAA;Quality of service","","9","","26","IEEE","24 Aug 2022","","","IEEE","IEEE Conferences"
"QoS aware FaaS platform","S. K. R; J. Lakshmi","Cloud Systems Lab, Indian Institute of Science, Bangalore, INDIA; Cloud Systems Lab, Indian Institute of Science, Bangalore, INDIA","2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","2 Aug 2021","2021","","","812","819","Function as a Service (FaaS), a form of serverless computing, is one of the recent cloud computing service offerings, which abstracts and automates the management and provisioning of resources, and deployment of applications. It provides powerful abstractions to compose applications as stateless functions and triggers their executions through events. The platform offers autonomous scaling for applications and pay-as-you-go sub-second billing model. However, contemporary FaaS platforms provide limited features in stating resource requirements. They often lack specifications to express application specificities and resource requirements associated with Quality of Service (QoS). Such specifications can effectively guide the resource provisioning and function deployment at the resource provider, leading to efficient resource utilization and cost savings. This research exploration motivates the need for a QoS specification framework for FaaS and proposes ideas for realizing an initial QoS aware FaaS platform. Experimental results based on real-world workload trace show the cost savings and efficient resource utilization that QoS can bring in FaaS platforms.","","978-1-7281-9586-5","10.1109/CCGrid51090.2021.00099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9499608","FaaS;Function as a Service;Serverless computing;QoS;Quality of Service;QoS aware FaaS platform;FaaS platform","Cloud computing;Computational modeling;Quality of service;FAA;Resource management","","7","","29","IEEE","2 Aug 2021","","","IEEE","IEEE Conferences"
"SCOOP: A Scalable Object-Oriented Serverless Platform","N. Shahidi; J. R. Gunasekaran; M. T. Kandemir; B. Urgaonkar","Computer Science and Engineering, Pennsylvania State University, University Park, PA; Adobe Research; Computer Science and Engineering, Pennsylvania State University, University Park, PA; Computer Science and Engineering, Pennsylvania State University, University Park, PA",2023 IEEE 16th International Conference on Cloud Computing (CLOUD),"25 Sep 2023","2023","","","1","3","Function-as-a-Service (FaaS) has been the primary component to drive the movement toward serverless computing. These lightweight and scalable components, though attractive, are non-trivial to accommodate the needs of long-running stateful applications. In this paper, we highlight the drawbacks of existing stateful FaaS proposals, in turn motivating the need to rethink the stateful serverless model for building general-purpose applications, while maintaining its benefits such as auto-scaling and pay-per-use cost model. We present a novel serverless model based on the object-oriented (OO) programming paradigm, with Object-as-a-Service (OaaS), acting as the only component of the serverless design. Through our experimental evaluations, we demonstrate that the proposed architecture, named SCOOP, can improve the end-to-end latency of applications by 52% and 58%, compared to the state-of-the-art stateless and stateful FaaS implementations, respectively, while reducing the SLO violations by up to 14% by scaling resources based on the traffic fluctuations in the WITS and Berkeley traces.","2159-6190","979-8-3503-0481-7","10.1109/CLOUD60044.2023.00069","NSF(grant numbers:2211018,2122155,1931531,1763681,1908793); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10254975","Serverless Computing;Object-Oriented Programming;FaaS;Stateful;AWS;Azure;AWS Step Function;Azure Durable Function","Fluctuations;Costs;Object oriented modeling;Computational modeling;Architecture;Buildings;Serverless computing","","1","","6","IEEE","25 Sep 2023","","","IEEE","IEEE Conferences"
"Evaluating Concurrent Executions of Multiple Function-as-a-Service Runtimes with MicroVM","J. Park; H. Kim; K. Lee","Dept. of Computer Science, Kookmin University, Seoul, South Korea; Dept. of Computer Science, Kookmin University, Seoul, South Korea; Dept. of Computer Science, Kookmin University, Seoul, South Korea",2020 IEEE 13th International Conference on Cloud Computing (CLOUD),"18 Dec 2020","2020","","","532","536","Serverless computing and public Function-as-a-Service (FaaS) systems are gaining significant attention because they help easily build a highly available system. With recent advances in micro virtual machines (microVM), the internal architecture of FaaS systems substantially changes. This paper focuses on a thorough investigation of the recent improvement in public FaaS systems concerning numerous concurrent executions. The adoption of microVM has changed the nature of FaaS, especially for runtime reservations. As a result, the performance degradation has decreased significantly compared to the previous generation FaaS, as shown in the experiment.","2159-6190","978-1-7281-8780-8","10.1109/CLOUD49709.2020.00080","National Research Foundation of Korea (NRF)(grant numbers:NRF-2015R1A5A7037615); ICT R&D program of IITP(grant numbers:2017-0-00396); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284320","FaaS;Serverless;microVM","Degradation;Cloud computing;Runtime;Conferences;FAA;Computer architecture;Virtual machining","","7","","14","IEEE","18 Dec 2020","","","IEEE","IEEE Conferences"
"Enabling Serverless Sky Computing","R. Cordingly; W. Lloyd","School of Engineering and Technology, University of Washington Tacoma, Washington, USA; School of Engineering and Technology, University of Washington Tacoma, Washington, USA",2023 IEEE International Conference on Cloud Engineering (IC2E),"6 Nov 2023","2023","","","232","235","The Sky Computing vision represents a unified multi-cloud environment where applications can be deployed to utilize resources from different cloud regions, resource configurations, and cloud providers. Serverless computing platforms have recently emerged, offering automatic elastic scaling, high performance, and reduced costs but often utilize proprietary deployment tools and services locking users into platform-specific services. This research aims to apply Sky Computing to serverless computing platforms offered by major cloud providers such as Amazon Web Services, Google Cloud, Azure, and more. This research will build a serverless sky architecture to enable the aggregation of serverless resources to achieve service-level objectives such as low hosting costs, high performance, fault tolerance, high throughput, and low carbon footprint. The research will focus on evaluating the performance implications of serverless aggregation (Thrust-1), design and evaluation of Sky Computing architectures and aggregation strategies (Thrust-2), and finally autonomous resource aggregation for intelligent self-management of applications deployed to the sky (Thrust-3).","2694-0825","979-8-3503-4394-6","10.1109/IC2E59103.2023.00038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10305821","Sky Computing;Serverless Computing;Function as a Service;Cloud Computing;Hybrid Cloud","Fault tolerance;Costs;Web services;Fault tolerant systems;Ecosystems;Serverless computing;Computer architecture","","1","","18","IEEE","6 Nov 2023","","","IEEE","IEEE Conferences"
"Scheduling Containers Rather Than Functions for Function-as-a-Service","D. K. Kim; H. -G. Roh","NAVER Corporation, Seongnam-si, Republic of Korea; NAVER Cloud Corporation, Seongnam-si, Republic of Korea","2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","2 Aug 2021","2021","","","465","474","Function-as-a-Service (FaaS) is a compelling technology that allows users to run functions in an event-driven way without concerns about server management. Container-based virtualization enables functions to run in a lightweight and isolated run-time environment, but frequent function executions accompanied with container initialization (cold starts) make the platform busy and unresponsive. For performance sake, warm starts, which is to execute functions on already initialized containers, are encouraged, and thus FaaS platforms make efforts to schedule functions to warm containers.From our experience operating an on-premise FaaS platform, we found that the existing scheduler showed poor performance and unstable behavior against multi-tenant and highly concurrent workloads. This paper proposes a novel FaaS scheduling algorithm, named FPCSch, that schedules Function-Pulling-Containers instead of scheduling functions to containers. As FPCSch lets containers continuously pull functions of the same type, cold starts decrease dramatically. Our evaluations show that Apache OpenWhisk equipped with FPCSch has many desirable features for FaaS platforms; (1) quite stable throughput against the multi-tenant workloads mixed by the increasing numbers of function types, (2) growing throughput for increasing concurrency, (3) uniformly load-balancing resource-intensive workloads, and (4) nearly proportional performance for scale-out.","","978-1-7281-9586-5","10.1109/CCGrid51090.2021.00056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9499544","FaaS;Servereless;Scheduling;Container;Function as a Service;OpenWhisk;Load balancing;Scale out;Scalability;Apache OpenWhisk;Cold Starts;Cloud;Cloud computing;FaaS platform;Virtualization","Concurrent computing;Schedules;Cloud computing;Scheduling algorithms;FAA;Containers;Throughput","","6","","44","IEEE","2 Aug 2021","","","IEEE","IEEE Conferences"
"FedLess: Secure and Scalable Federated Learning Using Serverless Computing","A. Grafberger; M. Chadha; A. Jindal; J. Gu; M. Gerndt","Chair of Computer Architecture and Parallel Systems, Technische Universität München Garching (Near Munich), Germany; Chair of Computer Architecture and Parallel Systems, Technische Universität München Garching (Near Munich), Germany; Chair of Computer Architecture and Parallel Systems, Technische Universität München Garching (Near Munich), Germany; Chair of Computer Architecture and Parallel Systems, Technische Universität München Garching (Near Munich), Germany; Chair of Computer Architecture and Parallel Systems, Technische Universität München Garching (Near Munich), Germany",2021 IEEE International Conference on Big Data (Big Data),"13 Jan 2022","2021","","","164","173","The traditional cloud-centric approach for Deep Learning (DL) requires training data to be collected and processed at a central server which is often challenging in privacy-sensitive domains like healthcare. Towards this, a new learning paradigm called Federated Learning (FL) has been proposed that brings the potential of DL to these domains while addressing privacy and data ownership issues. FL enables clients to learn a shared ML model while keeping the data local. However, conventional FL systems face challenges such as scalability, complex infrastructure management, and wasted compute and incurred costs due to idle clients. These challenges of FL systems closely align with the core problems that serverless computing and Function-as-a-Service (FaaS) platforms aim to solve. These include rapid scalability, no infrastructure management, automatic scaling to zero for idle clients, and a pay-per-use billing model. To this end, we present a novel system and framework for serverless FL, called FedLess. Our system supports multiple commercial and self-hosted FaaS providers and can be deployed in the cloud, on-premise in institutional data centers, and on edge devices. To the best of our knowledge, we are the first to enable FL across a large fabric of heterogeneous FaaS providers while providing important features like security and Differential Privacy. We demonstrate with comprehensive experiments that the successful training of DNNs for different tasks across up to 200 client functions and more is easily possible using our system. Furthermore, we demonstrate the practical viability of our methodology by comparing it against a traditional FL system and show that it can be cheaper and more resource-efficient.","","978-1-6654-3902-2","10.1109/BigData52589.2021.9672067","Google; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9672067","Function-as-a-service (FaaS);serverless computing;federated learning;deep learning","Training;Scalability;Computational modeling;Serverless computing;Training data;FAA;Big Data","","35","","71","IEEE","13 Jan 2022","","","IEEE","IEEE Conferences"
"The Ifs and Buts of Less is More: A Serverless Computing Reality Check","J. Kuhlenkamp; S. Werner; S. Tai","Information Systems Engineering, Technische Universität Berlin, Berlin, Germany; Information Systems Engineering, Technische Universität Berlin, Berlin, Germany; Information Systems Engineering, Technische Universität Berlin, Berlin, Germany",2020 IEEE International Conference on Cloud Engineering (IC2E),"19 May 2020","2020","","","154","161","Serverless computing defines a pay-as-you-go cloud execution model, where the unit of computation is a function that a cloud provider executes and auto-scales on behalf of a cloud consumer. Serverless suggests not (or less) caring about servers but focusing (more) on business logic expressed in functions. Server'less' may be `more' when getting developer expectations and platform propositions right and when engineering solutions that take specific behavior and constraints of (current) Function-as-a-Service platforms into account. To this end, in this invited paper, we present a summary of findings and lessons learned from a series of research experiments conducted over the past two years. We argue that careful attention must be placed on the promises associated with the serverless model, provide a reality-check for five common assumptions, and suggest ways to mitigate unwanted effects. Our findings focus on application workload distribution and computational processing complexity, the specific auto-scaling mechanisms in place, the behavior and strategies implemented with operational tasks, the constraints and limitations existing when composing functions, and the costs of executing functions.","","978-1-7281-1099-8","10.1109/IC2E48712.2020.00023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9096486","serverless computing;function-as-a-service (FaaS);FaaS platforms;FaaS experimentation","FAA;Computational modeling;Runtime;Cloud computing;Complexity theory;Task analysis;Load modeling","","16","","31","IEEE","19 May 2020","","","IEEE","IEEE Conferences"
"Resource Scaling Strategies for Open-Source FaaS Platforms compared to Commercial Cloud Offerings","J. Manner; G. Wirtz","Distributed Systems Group, Otto-Friedrich-University, Bamberg, Germany; Distributed Systems Group, Otto-Friedrich-University, Bamberg, Germany",2022 IEEE 15th International Conference on Cloud Computing (CLOUD),"24 Aug 2022","2022","","","40","48","Open-source offerings are often investigated when comparing their features to commercial cloud offerings. However, performance benchmarking is rarely executed for open-source tools hosted on-premise nor is it possible to conduct a fair cost comparison due to a lack of resource settings equivalent to cloud scaling strategies.Therefore, we firstly list implemented resource scaling strategies for public and open-source FaaS platforms. Based on this we propose a methodology to calculate an abstract performance measure to compare two platforms with each other. Since all open-source platforms suggest a Kubernetes deployment, we use this measure for a configuration of open-source FaaS platforms based on Kubernetes limits. We tested our approach with CPU intensive functions, considering the difference between single-threaded and multi-threaded functions to avoid wasting resources. With regard to this, we also address the noisy neighbor problem for open-source FaaS platforms by conducting an instance parallelization experiment. Our approach to limit resources leads to consistent results while avoiding an overbooking of resources.","2159-6190","978-1-6654-8137-3","10.1109/CLOUD55607.2022.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9860370","Serverless;Function as a Service;FaaS;Open-FaaS;Kubernetes;Benchmarking","Cloud computing;Costs;FAA;Benchmark testing;Noise measurement;Open source software","","4","","44","IEEE","24 Aug 2022","","","IEEE","IEEE Conferences"
"SMART: Serverless Module Analysis and Recognition Technique for Managed Applications","A. Ashkenazi; E. Grolman; A. Elyashar; D. Mimran; O. Brodt; Y. Elovici; A. Shabtai","Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev, Israel; Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev, Israel; Cyber Labs@BGU, Ben-Gurion University of the Negev, Israel; Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev, Israel; Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev, Israel; Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev, Israel; Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev, Israel","2024 IEEE 24th International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","8 Oct 2024","2024","","","442","452","Serverless Function-as-a-Service (FaaS) environments enable developers to build and run cloud applications without the need to manage the underlying servers and computing infrastructure, allowing them to focus on implementing the application logic. Such environments contain numerous functions and dynamic resources, e.g., APIs and databases, making it challenging to gain insight and context of internal events i.e., recognize modules. Module in a serverless application is a set of functions and resources, that represents a functional unit that shares logical context. This paper presents SMART, a method for automatic analysis and recognition of modules for managed serverless applications. The proposed method creates an event-based graph by analyzing the standard serverless logs that document events involving the application’s functions and resources and utilizes well-known community detection algorithms (such as Louvain), with graph centrality metrics (such as degree centrality) to recognize the modules. SMART enables high-level visibility of the application’s structure and logical context which can facilitate security analysis and contribute to improved decision-making of incident response handlers, who typically do not have direct access to the application’s design and code, which can lead to challenges in fully understanding the system’s intricacies. We focused on the popular Amazon Web Services (AWS) Lambda serverless computing platform and evaluated the proposed method on three different demo applications (Airline Booking, VOD, and E-commerce). We compared SMART’s performance to four overlapping community detection algorithms and showed that it outperformed them in the task of module recognition, with a maximum improvement of 61% on the omega index metric compared to the Speaker-Listener Label Propagation algorithm. In addition, we demonstrate that the use of large language models (LLMs) with the knowledge gained by SMART can enrich security analysis insights.","2993-2114","979-8-3503-9566-2","10.1109/CCGrid59990.2024.00057","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10701367","Serverless computing;Function-as-a-Service;Security analysis;Incident response;Serverless activity logs;Serverless application architecture","Measurement;Web services;Serverless computing;Computer architecture;Security;Indexes;Electronic commerce;Detection algorithms;Standards;Airline industry","","1","","30","IEEE","8 Oct 2024","","","IEEE","IEEE Conferences"
"Exploring the Impact of Serverless Computing on Peer To Peer Training Machine Learning","A. Barrak; R. Trabelsi; F. Jaafar; F. Petrillo","Department of Computer Science and Mathematics, University of Quebec at Chicoutimi, UQAC, Saguenay, Canada; Department of Computer Science and Mathematics, University of Quebec at Chicoutimi, UQAC, Saguenay, Canada; Department of Computer Science and Mathematics, University of Quebec at Chicoutimi, UQAC, Saguenay, Canada; Département de génie Logiciel, École de Technologie Supérieure, ÉTS, Montreal",2023 IEEE International Conference on Cloud Engineering (IC2E),"6 Nov 2023","2023","","","141","152","The increasing demand for computational power in big data and machine learning has driven the development of distributed training methodologies. Among these, peer-to-peer (P2P) networks provide advantages such as enhanced scalability and fault tolerance. However, they also encounter challenges related to resource consumption, costs, and communication overhead as the number of participating peers grows. In this paper, we introduce a novel architecture that combines serverless computing with P2P networks for distributed training and present a method for efficient parallel gradient computation under resource constraints.Our findings show a significant enhancement in gradient computation time, with up to a 97.34% improvement compared to conventional P2P distributed training methods. As for costs, our examination confirmed that the serverless architecture could incur higher expenses, reaching up to 5.4 times more than instance-based architectures. It is essential to consider that these higher costs are associated with marked improvements in computation time, particularly under resource-constrained scenarios.Despite the cost-time trade-off, the serverless approach still holds promise due to its pay-as-you-go model. Utilizing dynamic resource allocation, it enables faster training times and optimized resource utilization, making it a promising candidate for a wide range of machine learning applications.","2694-0825","979-8-3503-4394-6","10.1109/IC2E59103.2023.00024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10305831","Serverless;FaaS;Function as a Service;P2P;peer-to-peer architecture;Distributed Training;Machine Learning","Training;Costs;Scalability;Serverless computing;Computer architecture;Machine learning;Peer-to-peer computing","","3","","46","IEEE","6 Nov 2023","","","IEEE","IEEE Conferences"
"GeoFaaS: An Edge-to-Cloud FaaS Platform","M. Malekabbasi; T. Pfandzelter; T. Schirmer; D. Bermbach","Scalable Software Systems Research Group, Technische Universität Berlin & Einstein Center Digital Future; Scalable Software Systems Research Group, Technische Universität Berlin & Einstein Center Digital Future; Scalable Software Systems Research Group, Technische Universität Berlin & Einstein Center Digital Future; Scalable Software Systems Research Group, Technische Universität Berlin & Einstein Center Digital Future",2024 IEEE International Conference on Cloud Engineering (IC2E),"14 Nov 2024","2024","","","66","71","The massive growth of mobile and IoT devices demands geographically distributed computing systems for optimal performance, privacy, and scalability. However, existing edge-to-cloud serverless platforms lack location awareness, resulting in inefficient network usage and increased latency. In this paper, we propose GeoFaaS, a novel edge-to-cloud Function-as-a-Service (FaaS) platform that leverages real-time client location information for transparent request execution on the nearest available FaaS node. If needed, GeoFaaS transparently offloads requests to the cloud when edge resources are overloaded, thus, ensuring consistent execution without user intervention. GeoFaaS has a modular and decentralized architecture: building on the single-node FaaS system tinyFaaS, GeoFaaS works as a stand-alone edge-to-cloud FaaS platform but can also integrate and act as a routing layer for existing FaaS services, e.g., in the cloud. To evaluate our approach, we implemented an open-source proof-of-concept prototype and studied performance and fault-tolerance behavior in experiments.","2694-0825","979-8-3315-2869-0","10.1109/IC2E61754.2024.00014","Bundesministerium für Bildung und Forschung; Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10749738","Function-as-a-Service;Edge-to-Cloud;Serverless Computing","Performance evaluation;Location awareness;Privacy;Fault tolerance;Scalability;Fault tolerant systems;Prototypes;Routing;Real-time systems;Internet of Things","","1","","26","IEEE","14 Nov 2024","","","IEEE","IEEE Conferences"
"Data-driven scheduling in serverless computing to reduce response time","B. Przybylski; P. Żuk; K. Rzadca","Institute of Informatics, University of Warsaw, Warsaw, Poland; Institute of Informatics, University of Warsaw, Warsaw, Poland; Institute of Informatics, University of Warsaw, Warsaw, Poland","2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","2 Aug 2021","2021","","","206","216","In Function as a Service (FaaS), a serverless computing variant, customers deploy functions instead of complete virtual machines or Linux containers. It is the cloud provider who maintains the runtime environment for these functions. FaaS products are offered by all major cloud providers (e.g. Amazon Lambda, Google Cloud Functions, Azure Functions); as well as standalone open-source software (e.g. Apache OpenWhisk) with their commercial variants (e.g. Adobe I/O Runtime or IBM Cloud Functions). We take the bottom-up perspective of a single node in a FaaS cluster. We assume that all the execution environments for a set of functions assigned to this node have been already installed. Our goal is to schedule individual invocations of functions, passed by a load balancer, to minimize performance metrics related to response time. Deployed functions are usually executed repeatedly in response to multiple invocations made by end-users. Thus, our scheduling decisions are based on the information gathered locally: the recorded call frequencies and execution times. We propose a number of heuristics, and we also adapt some theoretically-grounded ones like SEPT or SERPT. Our simulations use a recently-published Azure Functions Trace. We show that, compared to the baseline FIFO or round-robin, our data-driven scheduling decisions significantly improve the performance.","","978-1-7281-9586-5","10.1109/CCGrid51090.2021.00030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9499596","scheduling;function as a service;FaaS;serverless;data center;cloud;latency;response time;flow time;stretch","Cloud computing;Time-frequency analysis;Schedules;Processor scheduling;Computational modeling;FAA;Time measurement","","6","","34","IEEE","2 Aug 2021","","","IEEE","IEEE Conferences"
"FedLesScan: Mitigating Stragglers in Serverless Federated Learning","M. Elzohairy; M. Chadha; A. Jindal; A. Grafberger; J. Gu; M. Gerndt; O. Abboud","Chair of Computer Architecture and Parallel Systems, Technische Universität München Garching (near Munich), Germany; Chair of Computer Architecture and Parallel Systems, Technische Universität München Garching (near Munich), Germany; Chair of Computer Architecture and Parallel Systems, Technische Universität München Garching (near Munich), Germany; Chair of Computer Architecture and Parallel Systems, Technische Universität München Garching (near Munich), Germany; Chair of Computer Architecture and Parallel Systems, Technische Universität München Garching (near Munich), Germany; Chair of Computer Architecture and Parallel Systems, Technische Universität München Garching (near Munich), Germany; Huawei Technologies, Munich, Germany",2022 IEEE International Conference on Big Data (Big Data),"26 Jan 2023","2022","","","1230","1237","Federated Learning (FL) is a machine learning paradigm that enables the training of a shared global model across distributed clients while keeping the training data local. While most prior work on designing systems for FL has focused on using stateful always running components, recent work has shown that components in an FL system can greatly benefit from the usage of serverless computing and Function-as-a-Service technologies. To this end, distributed training of models with severless FL systems can be more resource-efficient and cheaper than conventional FL systems. However, serverless FL systems still suffer from the presence of stragglers, i.e., slow clients due to their resource and statistical heterogeneity. While several strategies have been proposed for mitigating stragglers in FL, most methodologies do not account for the particular characteristics of serverless environments, i.e., cold-starts, performance variations, and the ephemeral stateless nature of the function instances. Towards this, we propose FedLesScan, a novel clustering-based semi-asynchronous training strategy, specifically tailored for serverless F L. FedLesScan dynamically adapts to the behavior of clients and minimizes the effect of stragglers on the overall system. We implement our strategy by extending an open-source serverless FL system called FedLess. Moreover, we comprehensively evaluate our strategy using the 2nd generation Google Cloud Functions with four datasets and varying percentages of stragglers. Results from our experiments show that compared to other approaches FedLesScan reduces training time and cost by an average of 8% and 20% respectively while utilizing clients better with an average increase in the effective update ratio of 17.75%.","","978-1-6654-8045-1","10.1109/BigData55660.2022.10021037","Deutsche Forschungsgemeinschaft; Google; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10021037","Federated learning;Deep learning;Serverless computing;Function-as-a-service;FaaS","Training;Costs;Federated learning;Computational modeling;Training data;Serverless computing;Big Data","","4","","25","IEEE","26 Jan 2023","","","IEEE","IEEE Conferences"
"Software Resource Disaggregation for HPC with Serverless Computing","M. Copik; M. Chrapek; L. Schmid; A. Calotoiu; T. Hoefler","Department of Computer Science, ETH Zürich, Zürich, Switzerland; Department of Computer Science, ETH Zürich, Zürich, Switzerland; Karlsruhe Institute of Technology, Germany; Department of Computer Science, ETH Zürich, Zürich, Switzerland; Department of Computer Science, ETH Zürich, Zürich, Switzerland",2024 IEEE International Parallel and Distributed Processing Symposium (IPDPS),"8 Jul 2024","2024","","","139","156","Aggregated HPC resources have rigid allocation systems and programming models which struggle to adapt to diverse and changing workloads. Consequently, HPC systems fail to efficiently use the large pools of unused memory and increase the utilization of idle computing resources. Prior work attempted to increase the throughput and efficiency of supercomputing systems through workload co-location and resource disaggregation. However, these methods fall short of providing a solution that can be applied to existing systems without major hardware modifications and performance losses. In this paper, we improve the utilization of supercomputers by employing the new cloud paradigm of serverless computing. We show how serverless functions provide fine-grained access to the resources of batchmanaged cluster nodes. We present an HPC-oriented Functionas-a-Service (FaaS) that satisfies the requirements of high-performance applications. We demonstrate a software resource disaggregation approach where placing functions on unallocated and underutilized nodes allows idle cores and accelerators to be utilized while retaining near-native performance.Full Paper Version: https://arxiv.org/abs/2401.10852HPC FaaS Implementation: https://github.com/spcl/rFaaS","1530-2075","979-8-3503-8711-7","10.1109/IPDPS57955.2024.00021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10579231","serverless;high-performance computing;function-as-a-service;utilization","Distributed processing;Program processors;Computational modeling;Memory management;Serverless computing;Programming;Throughput","","5","","130","IEEE","8 Jul 2024","","","IEEE","IEEE Conferences"
"Energy-Aware Resource Scheduling for Serverless Edge Computing","M. S. Aslanpour; A. N. Toosi; M. A. Cheema; R. Gaire","Department of Software Systems and Cybersecurity, Faculty of Information Technology, Monash University, Clayton, Australia; Department of Software Systems and Cybersecurity, Faculty of Information Technology, Monash University, Clayton, Australia; Department of Software Systems and Cybersecurity, Faculty of Information Technology, Monash University, Clayton, Australia; CSIRO DATA61, Australia","2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","19 Jul 2022","2022","","","190","199","In this paper, we present energy-aware scheduling for Serverless edge computing. Energy awareness is critical since edge nodes, in many Internet of Things (IoT) domains, are meant to be powered by renewable energy sources that are variable, making low-powered and/or overloaded (bottleneck) nodes unavailable and not operating their services. This awareness is also required since energy challenges have not been previously addressed by Serverless, largely due to its origin in cloud computing. To achieve this, we formally model an energy-aware resource scheduling problem in Serverless edge computing, given a cluster of battery-operated and renewable-energy powered nodes. Then, we devise zone-oriented and priority-based algorithms to improve the operational availability of bottleneck nodes. As assets, our algorithm coins terms “sticky offloading” and “warm scheduling” in the interest of the Quality of Service (QoS). We evaluate our proposal against well-known benchmarks using real-world implementations on a cluster of Raspberry Pis enabled with container orchestration, Kubernetes, and Serverless computing, OpenFaaS, where edge nodes are powered by real-world solar irradiation. Experimental results achieve significant improvements, up to 33%, in helping bottleneck node's operational availability while preserving the QoS. With energy awareness, now Serverless can unconditionally offer its resource efficiency and portability at the edge.","","978-1-6654-9956-9","10.1109/CCGrid54584.2022.00028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9826101","edge computing;Serverless;function-as-a-service;energy-aware;scheduling","Wireless communication;Renewable energy sources;Computational modeling;Clustering algorithms;Serverless computing;Quality of service;Benchmark testing","","29","","23","IEEE","19 Jul 2022","","","IEEE","IEEE Conferences"
"Hybrid Serverless Platform for Service Function Chains","S. K. R; J. Lakshmi","Cloud Systems Lab, Indian Institute of Science, Bangalore, INDIA; Cloud Systems Lab, Indian Institute of Science, Bangalore, INDIA",2023 IEEE 16th International Conference on Cloud Computing (CLOUD),"25 Sep 2023","2023","","","493","504","Cloud Data Centres deal with dynamic changes all the time. Networks in particular, need to adapt their configurations to changing workloads. Given these expectations, Network Function Virtualization (NFV) using Software Defined Networks (SDNs) have realized the aspect of programmability in networks, bringing in the necessary fidelity to managing network resources. NFVs allows network services to be programmed as software entities that can be deployed on commodity clusters in the Cloud. Being software, they inherently carry the ability to be customized to specific tenants' requirements and thus support multi-tenant variations with ease. However, the ability to exploit scaling in alignment with changing demands with minimal loss of service and improving resource usage efficiency still remains a challenge. Several recent works in literature have proposed platforms to realize Virtual Network functions (VNFs) on the Cloud using different services such as Infrastructure as a Service (IaaS) and serverless computing. These approaches suffer from deployment difficulties (configuration and sizing) or adaptability to performance requirements or changing dynamics. In the current work, we propose a Hybrid Serverless Platform (HSP) to address these identified lacunae. The HSP is implemented using a combination of persistent IaaS and FaaS components. The IaaS components handle the steady state load, whereas the FaaS components activate during the dynamic change associated with scaling to minimize service loss. The HSP's controller takes provisioning decisions on Quality of Service (QoS) attributes derived from flow statistics, thereby alleviating sizing decisions for deployment. A proof-of-concept realization of HSP is presented in the paper and is evaluated for an example Service Function Chain (SFC) scenario for a dynamic workload, which shows minimal loss in flowlet service, up to 35% resource savings as compared to a pure IaaS deployment and up to 55% lower end-to-end SFC execution times as compared to a baseline FaaS implementation.","2159-6190","979-8-3503-0481-7","10.1109/CLOUD60044.2023.00066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10254966","FaaS;Serverless;Hybrid Serverless Platform;IaaS;SFC;Serverless computing;SDN","Cloud computing;Data centers;Service function chaining;Serverless computing;Quality of service;Dynamic scheduling;Software","","3","","45","IEEE","25 Sep 2023","","","IEEE","IEEE Conferences"
"GPU-enabled Function-as-a-Service for Machine Learning Inference","M. Zhao; K. Jha; S. Hong",Arizona State University; Arizona State University; Arizona State University,2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS),"18 Jul 2023","2023","","","918","928","Function-as-a-Service (FaaS) is emerging as an important cloud computing service model as it can improve the scalability and usability of a wide range of applications, especially Machine-Learning (ML) inference tasks that require scalable resources and complex software configurations. These inference tasks heavily rely on GPUs to achieve high performance; however, support for GPUs is currently lacking in the existing FaaS solutions. The unique event-triggered and short-lived nature of functions poses new challenges to enabling GPUs on FaaS, which must consider the overhead of transferring data (e.g., ML model parameters and inputs/outputs) between GPU and host memory. This paper proposes a novel GPU-enabled FaaS solution that enables ML inference functions to efficiently utilize GPUs to accelerate their computations. First, it extends existing FaaS frameworks such as OpenFaaS to support the scheduling and execution of functions across GPUs in a FaaS cluster. Second, it provides caching of ML models in GPU memory to improve the performance of model inference functions and global management of GPU memories to improve cache utilization. Third, it offers co-designed GPU function scheduling and cache management to optimize the performance of ML inference functions. Specifically, the paper proposes locality-aware scheduling, which maximizes the utilization of both GPU memory for cache hits and GPU cores for parallel processing. A thorough evaluation based on real-world traces and ML models shows that the proposed GPU-enabled FaaS works well for ML inference tasks, and the proposed locality-aware scheduler achieves a speedup of 48x compared to the default, load balancing only schedulers.","1530-2075","979-8-3503-3766-2","10.1109/IPDPS54959.2023.00096","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10177435","Function-as-a-Service;GPU scheduling;Caching;Machine learning inference","Computational modeling;Memory management;Graphics processing units;Machine learning;Data transfer;Data models;Software","","6","","29","IEEE","18 Jul 2023","","","IEEE","IEEE Conferences"
"rFaaS: Enabling High Performance Serverless with RDMA and Leases","M. Copik; K. Taranov; A. Calotoiu; T. Hoefler","Department of Computer Science, ETH Zürich, Zürich, Switzerland; Microsoft; Department of Computer Science, ETH Zürich, Zürich, Switzerland; Department of Computer Science, ETH Zürich, Zürich, Switzerland",2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS),"18 Jul 2023","2023","","","897","907","High performance is needed in many computing systems, from batch-managed supercomputers to general-purpose cloud platforms. However, scientific clusters lack elastic parallelism, while clouds cannot offer competitive costs for high-performance applications. In this work, we investigate how modern cloud programming paradigms can bring the elasticity needed to allocate idle resources, decreasing computation costs and improving overall data center efficiency. Function-as-a-Service (FaaS) brings the pay-as-you-go execution of stateless functions, but its performance characteristics cannot match coarse-grained cloud and cluster allocations. To make serverless computing viable for high-performance and latency-sensitive applications, we present rFaaS, an RDMA-accelerated FaaS platform. We identify critical limitations of serverless - centralized scheduling and inefficient network transport - and improve the FaaS architecture with allocation leases and microsecond invocations. We show that our remote functions add only negligible overhead on top of the fastest available networks, and we decrease the execution latency by orders of magnitude compared to contemporary FaaS systems. Furthermore, we demonstrate the performance of rFaaS by evaluating real-world FaaS benchmarks and parallel applications. Overall, our results show that new allocation policies and remote memory access help FaaS applications achieve high performance and bring serverless computing to HPC.","1530-2075","979-8-3503-3766-2","10.1109/IPDPS54959.2023.00094","European Research Council; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10177421","Serverless;Function-as-a-Service;High-Performance Computing;RDMA","Costs;Processor scheduling;High-speed networks;Granular computing;Scalability;Serverless computing;Parallel processing","","5","","70","IEEE","18 Jul 2023","","","IEEE","IEEE Conferences"
"RDOF: Deployment Optimization for Function as a Service","L. Zhu; G. Giotis; V. Tountopoulos; G. Casale","Department of Computing, Imperial College London, London, UK; Innovation Lab Athens Technology Center, Athens, Greece; Innovation Lab Athens Technology Center, Athens, Greece; Department of Computing, Imperial College London, London, UK",2021 IEEE 14th International Conference on Cloud Computing (CLOUD),"8 Nov 2021","2021","","","508","514","Function as a service (FaaS) simplifies the runtime resource management of cloud applications and enables fine-grained scaling and billing at the function level, thus becoming the most widespread serverless paradigm today. Cost-effective use of FaaS entails appropriately deploying individual functions. We propose in this paper RDOF1, a model-driven approach to deployment optimization for FaaS. RDOF predicts the performance of a FaaS-based application by instantiating a layered queueing network and finds the optimal configuration of each function such that the total operating cost is minimized under the specified performance requirements. We have validated RDOF on Amazon Web Services (AWS) and implemented it in an online tool that operates on TOSCA metamodels.","2159-6190","978-1-6654-0060-2","10.1109/CLOUD53861.2021.00066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9582257","deployment optimization;function as a service;layered queueing networks;TOSCA","Cloud computing;Costs;Runtime;Web services;Conferences;Pipelines;FAA","","3","","13","IEEE","8 Nov 2021","","","IEEE","IEEE Conferences"
"The State of FaaS: An Analysis of Public Functions-as-a-Service Providers","N. Ekwe-Ekwe; L. Amos","School of Computer Science, University of St Andrews, St Andrews, United Kingdom; lucasamos.dev, Scotland, United Kingdom",2024 IEEE 17th International Conference on Cloud Computing (CLOUD),"28 Aug 2024","2024","","","430","438","Serverless computing is a growing and maturing field that is the focus of much research, industry interest and adoption. Previous works exploring Functions-as-a-Service providers have focused primarily on the most well known providers AWS Lambda, Google Cloud Functions and Microsoft Azure Functions without exploring other providers in similar detail. In this work, we conduct the first detailed review of ten currently publicly available FaaS platforms exploring everything from their history, to their features and pricing to where they sit within the overall public FaaS landscape, before making a number of observations as to the state of the FaaS.","2159-6190","979-8-3503-6853-6","10.1109/CLOUD62652.2024.00055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10643918","serverless;faas;functions-as-a-service;serverless computing;state of serverless;state of faas;survey","Industries;Reviews;Serverless computing;Pricing;History","","","","107","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"Darly: Deep Reinforcement Learning for QoS-aware scheduling under resource heterogeneity Optimizing serverless video analytics","D. Giagkos; A. Tzenetopoulos; D. Masouros; D. Soudris; S. Xydis","Microprocessors Laboratory and Digital Systems Lab (MicroLab), School of Electrical and Computer Engineering, National Technical University of Athens; Microprocessors Laboratory and Digital Systems Lab (MicroLab), School of Electrical and Computer Engineering, National Technical University of Athens; Microprocessors Laboratory and Digital Systems Lab (MicroLab), School of Electrical and Computer Engineering, National Technical University of Athens; Microprocessors Laboratory and Digital Systems Lab (MicroLab), School of Electrical and Computer Engineering, National Technical University of Athens; Microprocessors Laboratory and Digital Systems Lab (MicroLab), School of Electrical and Computer Engineering, National Technical University of Athens",2023 IEEE 16th International Conference on Cloud Computing (CLOUD),"25 Sep 2023","2023","","","1","3","Today, video analytics are becoming extremely popular due to the increasing need for extracting valuable information from videos available in public sharing services through camera-driven streams. Typically, video analytics are organized as a set of separate tasks, each of which has different resource requirements (e.g., computational- vs. memory-intensive tasks). The serverless computing paradigm forms a very promising approach for mapping such types of applications, as it enables fine-grained deployment and management in a per-function manner. However, modern serverless frameworks suffer from performance variability issues, due to i) the interference introduced due to co-location of third-party workloads with the serverless funcations and ii) the increasing hardware heterogeneity introduced in public clouds. To this end, this work introduces Darly, a QoS- and heterogeneity-aware Deep Reinforcement Learning-based Scheduler for serverless video analytics deployments. The proposed framework incorporates a DRL agent which exploits low-level performance counters to identify the levels of interference and the degree of heterogeneity in the underlying infrastructure and combines this information along with user-defined QoS requirements to dynamically optimize resource allocations by deciding the placement, migration, or horizontal scaling of serverless functions. Promising results are produced withing our experiments, which are accompanied with the intent to further build upon this groundwork.","2159-6190","979-8-3503-0481-7","10.1109/CLOUD60044.2023.00079","Hellenic Foundation for Research and Innovation (HFRI); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10255024","Cloud computing;Serverless Computing;Deep Reinforcement Learning;Quality-of-Service;Dynamic Scheduling;Resource Management","Processor scheduling;Visual analytics;Serverless computing;Quality of service;Interference;Reinforcement learning;Streaming media","","6","","9","IEEE","25 Sep 2023","","","IEEE","IEEE Conferences"
"User-guided Page Merging for Memory Deduplication in Serverless Systems","W. Qiu; M. Copik; Y. Wang; A. Calotoiu; T. Hoefler","Huawei Technologies, China; Department of Computer Science, ETH Zürich, Zürich, Switzerland; Shanghai Jiao Tong University, China; Department of Computer Science, ETH Zürich, Zürich, Switzerland; Department of Computer Science, ETH Zürich, Zürich, Switzerland",2023 IEEE International Conference on Big Data (BigData),"22 Jan 2024","2023","","","159","169","Serverless computing is an emerging cloud paradigm that offers an elastic and scalable allocation of computing resources with pay-as-you-go billing. In the Function-as-a-Service (FaaS) programming model, applications comprise short-lived and stateless serverless functions executed in isolated containers or microVMs, which can quickly scale to thousands of instances and process terabytes of data. This flexibility comes at the cost of duplicated runtimes, libraries, and user data spread across many function instances, and cloud providers do not utilize this redundancy. The memory footprint of serverless forces removing idle containers to make space for new ones, which decreases performance through more cold starts and fewer data caching opportunities.We address this issue by proposing deduplicating memory pages of serverless workers with identical content, based on the content-based page-sharing concept of Linux Kernel Same-page Merging (KSM). We replace the background memory scanning process of KSM, as it is too slow to locate sharing candidates in short-lived functions. Instead, we design User-Guided Page Merging (UPM), a built-in Linux kernel module that leverages the madvise system call: we enable users to advise the kernel of memory areas that can be shared with others. We show that UPM reduces memory consumption by up to 55% on 16 concurrent containers executing a typical image recognition function, more than doubling the density for containers of the same function that can run on a system.","","979-8-3503-2445-7","10.1109/BigData59044.2023.10386487","European Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10386487","Serverless;Function-as-a-Service;Memory Deduplication;Inference","Runtime;Runtime library;Linux;Merging;Redundancy;Serverless computing;Containers","","3","","51","IEEE","22 Jan 2024","","","IEEE","IEEE Conferences"
"Apodotiko: Enabling Efficient Serverless Federated Learning in Heterogeneous Environments","M. Chadha; A. Jensen; J. Gu; O. Abboud; M. Gerndt","Chair of Computer Architecture and Parallel Systems, Technische Universität München, Garching (near Munich), Germany; Chair of Computer Architecture and Parallel Systems, Technische Universität München, Garching (near Munich), Germany; Chair of Computer Architecture and Parallel Systems, Technische Universität München, Garching (near Munich), Germany; Huawei Technologies, Munich, Germany; Chair of Computer Architecture and Parallel Systems, Technische Universität München, Garching (near Munich), Germany","2024 IEEE 24th International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","8 Oct 2024","2024","","","206","215","Federated Learning (FL) is an emerging machine learning paradigm that enables the collaborative training of a shared global model across distributed clients while keeping the data decentralized. Recent works on designing systems for efficient FL have shown that utilizing serverless computing technologies, particularly Function-as-a-Service (FaaS) for FL, can enhance resource efficiency, reduce training costs, and alleviate the complex infrastructure management burden on data holders. However, current serverless FL systems still suffer from the presence of stragglers, i.e., slow clients that impede the collaborative training process. While strategies aimed at mitigating stragglers in these systems have been proposed, they overlook the diverse hardware resource configurations among FL clients. To this end, we present Apodotiko, a novel asynchronous training strategy designed for serverless FL. Our strategy incorporates a scoring mechanism that evaluates each client’s hardware capacity and dataset size to intelligently prioritize and select clients for each training round, thereby minimizing the effects of stragglers on system performance. We comprehensively evaluate Apodotiko across diverse datasets, considering a mix of CPU and GPU clients, and compare its performance against five other FL training strategies. Results from our experiments demonstrate that Apodotiko outperforms other FL training strategies, achieving an average speedup of 2.75x and a maximum speedup of 7.03x. Furthermore, our strategy significantly reduces cold starts by a factor of four on average, demonstrating suitability in serverless environments.","2993-2114","979-8-3503-9566-2","10.1109/CCGrid59990.2024.00032","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10701376","Federated learning;Deep learning;Serverless computing;Function-as-a-service;Straggler mitigation","Training;Federated learning;System performance;Prevention and mitigation;Collaboration;Serverless computing;Graphics processing units;Distributed databases;Hardware;Data models","","","","38","IEEE","8 Oct 2024","","","IEEE","IEEE Conferences"
"Performance Evaluation of Asynchronous FaaS","D. Balla; M. Maliosz; C. Simon","Department of Telecommunications and Media Informatics, High Speed Networks Laboratory, Budapest University of Technology and Economics, Budapest, Hungary; Department of Telecommunications and Media Informatics, High Speed Networks Laboratory, Budapest University of Technology and Economics, Budapest, Hungary; Department of Telecommunications and Media Informatics, High Speed Networks Laboratory, Budapest University of Technology and Economics, Budapest, Hungary",2021 IEEE 14th International Conference on Cloud Computing (CLOUD),"8 Nov 2021","2021","","","147","156","Function as a Service (FaaS) is a novel but dynamically emerging field of cloud computing. The majority of the leading cloud service providers have their own FaaS platforms, however, the open source community has embraced this technology, therefore an increasing number of FaaS alternatives can be deployed for on-premise use-cases. FaaS systems support both synchronous and asynchronous function invocations. In this paper we examine the differences in performance and billing between the two invocation types in OpenFaaS, Kubeless, Fission and Knative by using a simple function chain containing echo functions and a more complex image and natural language processing chain, implemented in Python3. We also present our solution, the implementation of asynchronous function invocations by using the Redis key-value store. Finally, we show how asynchronous function invocations avoid the negative effects on the billing when functions are cold-started.","2159-6190","978-1-6654-0060-2","10.1109/CLOUD53861.2021.00028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9582259","Serverless;Function as a Service;FaaS;Cloud;Asynchronous","Performance evaluation;Cloud computing;Sentiment analysis;Costs;Image processing;Conferences;FAA","","6","","24","IEEE","8 Nov 2021","","","IEEE","IEEE Conferences"
"Architecture-Specific Performance Optimization of Compute-Intensive FaaS Functions","M. Chadha; A. Jindal; M. Gerndt","Chair of Computer Architecture and Parallel Systems, Technische Universität München, Garching (near Munich), Germany; Chair of Computer Architecture and Parallel Systems, Technische Universität München, Garching (near Munich), Germany; Chair of Computer Architecture and Parallel Systems, Technische Universität München, Garching (near Munich), Germany",2021 IEEE 14th International Conference on Cloud Computing (CLOUD),"8 Nov 2021","2021","","","478","483","FaaS allows an application to be decomposed into functions that are executed on a FaaS platform. The FaaS platform is responsible for the resource provisioning of the functions. Recently, there is a growing trend towards the execution of compute-intensive FaaS functions that run for several seconds. However, due to the billing policies followed by commercial FaaS offerings, the execution of these functions can incur significantly higher costs. Moreover, due to the abstraction of underlying processor architectures on which the functions are executed, the performance optimization of these functions is challenging. As a result, most FaaS functions use pre-compiled libraries generic to x86-64 leading to performance degradation. In this paper, we examine the underlying processor architectures for Google Cloud Functions (GCF) and determine their prevalence across the 19 available GCF regions. We modify, adapt, and optimize three compute-intensive FaaS workloads written in Python using Numba, a JIT compiler based on LLVM, and present results wrt performance, memory consumption, and costs on GCF. Results from our experiments show that the optimization of FaaS functions can improve performance by 12.8x (geometric mean) and save costs by 73.4% on average for the three functions. Our results show that optimization of the FaaS functions for the specific architecture is very important. We achieved a maximum speedup of 1.79x by tuning the function especially for the instruction set of the underlying processor architecture.","2159-6190","978-1-6654-0060-2","10.1109/CLOUD53861.2021.00062","BMBF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9582234","Function-as-a-service (FaaS);serverless computing;performance optimization;cost;heterogeneity;Numba;LLVM","Degradation;Cloud computing;Costs;Instruction sets;Memory management;FAA;Market research","","12","","20","IEEE","8 Nov 2021","","","IEEE","IEEE Conferences"
"High Performance Serverless Architecture for Deep Learning Workflows","D. Chahal; M. Ramesh; R. Ojha; R. Singhal","TCS Research, Mumbai, India; TCS Research, Mumbai, India; TCS Research, Mumbai, India; TCS Research, Mumbai, India","2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","2 Aug 2021","2021","","","790","796","Serverless architecture is a rapidly growing paradigm for deploying deep learning applications performing ephemeral computing and serving bursty workloads. Serverless architecture promises automatic scaling and cost efficiency for inferencing deep learning models while minimizing the operational logic. However, serverless computing is stateless with constraints on local resources. Hence, deploying complex deep learning applications containing large size models, frameworks, and libraries is a challenge.In this work, we discuss a methodology and architecture for migrating deep vision algorithms and model based applications to a serverless computing platform. We have tested our methodology using AWS infrastructure (AWS Lambda, Provisioned Concurrency, VPC endpoint, S3 and EFS) to mitigate the challenges in deploying composition of APIs containing large deep learning models and frameworks. We evaluate the performance and cost of our architecture for a real-life enterprise application used for document processing.","","978-1-7281-9586-5","10.1109/CCGrid51090.2021.00096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9499397","Serverless;FaaS;Cloud;AI","Deep learning;Concurrent computing;Cloud computing;Computational modeling;Scalability;NoSQL databases;Pipelines","","8","","17","IEEE","2 Aug 2021","","","IEEE","IEEE Conferences"
"Function Memory Optimization for Heterogeneous Serverless Platforms with CPU Time Accounting","R. Cordingly; S. Xu; W. Lloyd","School of Engineering and Technology, University of Washington, Tacoma, United States; School of Engineering and Technology, University of Washington, Tacoma, United States; School of Engineering and Technology, University of Washington, Tacoma, United States",2022 IEEE International Conference on Cloud Engineering (IC2E),"16 Nov 2022","2022","","","104","115","Serverless Function-as-a-Service (FaaS) platforms often abstract the underlying infrastructure configuration into the single option of specifying a function's memory reservation size. This resource abstraction of coupling configurations options (e.g. vCPUs, memory, disk), combined with the lack of profiling, leaves developers to make ad hoc decisions on how to configure functions. Solutions are needed to mitigate exhaustive brute force searches of large parameter input spaces to find optimal configurations which can incur high costs. To address these challenges, we propose CPU Time Accounting Memory Selection (CPU-TAMS). CPU-TAMS is a workload agnostic memory selection method that utilizes CPU time accounting principles and regression modeling to recommend memory settings that reduce function runtime and subsequently, cost. Comparing CPU-TAMS to eight existing selection methods, we find that CPU-TAMS finds maximum value memory settings with only 8% runtime and 5% cost error compared to brute force testing while only requiring a single profiling run to evaluate function resource requirements. We adapt CPU-TAMS for use on four commercial FaaS platforms demonstrating efficacy to optimize function memory configurations where platforms feature heterogeneous infrastructure management policies.","","978-1-6654-9115-0","10.1109/IC2E55432.2022.00019","NSF(grant numbers:OAC-1849970); NIH(grant numbers:R01GM126019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9946331","Serverless Computing;Function-as-a-Service;Performance Evaluation;Performance Modeling","Couplings;Costs;Runtime;Computational modeling;Memory management;Force;Throughput","","10","","38","IEEE","16 Nov 2022","","","IEEE","IEEE Conferences"
"Serving distributed inference deep learning models in serverless computing","K. Mahajan; R. Desai",Meta; Meta,2022 IEEE 15th International Conference on Cloud Computing (CLOUD),"24 Aug 2022","2022","","","109","111","Serverless computing (SC) in an attractive win-win paradigm for cloud providers and customers, simultaneously providing greater flexibility and control over resource utilization for cloud providers while reducing costs through pay-per-use model and no capacity management for customers. While SC has been shown effective for event-triggered web applications, the use of deep learning (DL) applications on SC is limited due to latency-sensitive DL applications and stateless SC. In this paper, we focus on two key problems impacting deployment of distributed inference (DI) models on SC: resource allocation and cold start latency. To address the two problems, we propose a hybrid scheduler for identifying the optimal server resource allocation policy. The hybrid scheduler identifies container allocation based on candidate allocations from greedy strategy as well as deep reinforcement learning based allocation model.","2159-6190","978-1-6654-8137-3","10.1109/CLOUD55607.2022.00029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9860735","Serverless Computing;Distributed Inference;Deep Learning;Cold start","Deep learning;Costs;Computational modeling;Serverless computing;Reinforcement learning;Containers","","3","","24","IEEE","24 Aug 2022","","","IEEE","IEEE Conferences"
"Beyond Load Balancing: Package-Aware Scheduling for Serverless Platforms","G. Aumala; E. Boza; L. Ortiz-Avilés; G. Totoy; C. Abad","Escuela Superior Politécnica del Litoral, ESPOL; Escuela Superior Politécnica del Litoral, ESPOL; Escuela Superior Politécnica del Litoral, ESPOL; Escuela Superior Politécnica del Litoral, ESPOL; Escuela Superior Politécnica del Litoral, ESPOL","2019 19th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)","8 Jul 2019","2019","","","282","291","Fast deployment and execution of cloud functions in Function-as-a-Service (FaaS) platforms is critical, for example, for microservices architectures. However, functions that require large packages or libraries are bloated and start slowly. An optimization is to cache packages at the worker nodes instead of bundling them with the functions. However, existing FaaS schedulers are vanilla load balancers, agnostic of packages cached in response to prior function executions, and cannot properly reap the benefits of package caching. We study the case of package-aware scheduling and propose PASch, a novel scheduling algorithm that seeks package affinity during scheduling so that worker nodes can re-use execution environments with preloaded packages. PASch leverages consistent hashing and the power of 2 choices, while actively avoiding worker overload. We implement PASch in a new scheduler for the OpenLambda framework and evaluate it using simulations and real experiments. When using PASch instead of a least loaded balancer, tasks perceive an average speedup of 1.29×, and 80th percentile latency that is 23× faster. Furthermore, for the workload studied in this paper, PASch outperforms consistent hashing with bounded loads-a state-of-the-art load balancing algorithm-yielding a 1.3× average speedup, and a speedup of 1.5× at the 80th percentile.","","978-1-7281-0912-1","10.1109/CCGRID.2019.00042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8752939","serverless computing;cloud computing;function as a service;scheduling;load balancing;data locality;performance;OpenLambda","","","43","","31","IEEE","8 Jul 2019","","","IEEE","IEEE Conferences"
"Serverless Computing: An Investigation of Factors Influencing Microservice Performance","W. Lloyd; S. Ramesh; S. Chinthalapati; L. Ly; S. Pallickara","Institute of Technology, University of Washington, Tacoma, Washington, USA; Microsoft, Redmond, Washington, USA; Institute of Technology, University of Washington, Tacoma, Washington, USA; Institute of Technology, University of Washington, Tacoma, Washington, USA; Department of Computer Science, Colorado State University, Fort Collins, Colorado, USA",2018 IEEE International Conference on Cloud Engineering (IC2E),"17 May 2018","2018","","","159","169","Serverless computing platforms provide function(s)-as-a-Service (FaaS) to end users while promising reduced hosting costs, high availability, fault tolerance, and dynamic elasticity for hosting individual functions known as microservices. Serverless Computing environments, unlike Infrastructure-as-a-Service (IaaS) cloud platforms, abstract infrastructure management including creation of virtual machines (VMs), operating system containers, and request load balancing from users. To conserve cloud server capacity and energy, cloud providers allow hosting infrastructure to go COLD, deprovisioning containers when service demand is low freeing infrastructure to be harnessed by others. In this paper, we present results from our comprehensive investigation into the factors which influence microservice performance afforded by serverless computing. We examine hosting implications related to infrastructure elasticity, load balancing, provisioning variation, infrastructure retention, and memory reservation size. We identify four states of serverless infrastructure including: provider cold, VM cold, container cold, and warm and demonstrate how microservice performance varies up to 15x based on these states.","","978-1-5386-5008-0","10.1109/IC2E.2018.00039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8360324","Resource Management and Performance;Serverless Computing;Function-as-a-Service;Provisioning Variation","Containers;Cloud computing;Load management;Elasticity;Servers;Operating systems;Fault tolerance","","185","","38","IEEE","17 May 2018","","","IEEE","IEEE Conferences"
"Serverless Computing: Behind the Scenes of Major Platforms","D. Kelly; F. Glavin; E. Barrett","School of Computer Science National University of Ireland, Galway (NUIG), Galway, Ireland; School of Computer Science National University of Ireland, Galway (NUIG), Galway, Ireland; School of Computer Science National University of Ireland, Galway (NUIG), Galway, Ireland",2020 IEEE 13th International Conference on Cloud Computing (CLOUD),"18 Dec 2020","2020","","","304","312","Serverless computing offers an event driven pay-as-you-go framework for application development. A key selling point is the concept of no back-end server management, allowing developers to focus on application functionality. This is achieved through severe abstraction of the underlying architecture the functions run on. We examine the underlying architecture and report on the performance of serverless functions and how they are effected by certain factors such as memory allocation and interference caused by load induced by other users on the platform. Specifically, we focus on the serverless offerings of the four largest platforms; AWS Lambda, Google Cloud Functions, Microsoft Azure Functions and IBM Cloud Functions. In this paper, we observe and contrast between these platforms in their approach to the common issue of “cold starts”, we devise a means to unveil the underlying architecture serverless functions execute on and we investigate the effects of interference from load on the platform over the time span of one month.","2159-6190","978-1-7281-8780-8","10.1109/CLOUD49709.2020.00050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284261","Serverless Computing;Cloud Computing;Function-as-a-Service;Performance Measurement;Benchmarking","Cloud computing;Conferences;Computer architecture;Interference;Servers;Resource management","","29","","17","IEEE","18 Dec 2020","","","IEEE","IEEE Conferences"
"MicroFaaS on OpenFaaS: An Embedded Platform for Running Cloud Functions","A. B. George; A. Byrne; A. K. Coskun","Boston University, Boston, MA; Boston University, Boston, MA; Boston University, Boston, MA",2023 IEEE International Conference on Cloud Engineering (IC2E),"6 Nov 2023","2023","","","230","231","Function-as-a-Service (FaaS) platforms present a new cloud computing paradigm by enabling serverless deployment and execution of functions. MicroFaaS, a cost-effective and energy-efficient datacenter architecture, replaces x86-based rack servers with ARM-based single-board computers (SBCs). This paper focuses on enhancing MicroFaaS by incorporating OpenFaaS, a popular secure function building and deployment framework. Through extensive experimentation, MicroFaaS on OpenFaaS showcases improved energy efficiency, ease-of-use, and scalability over traditional cloud systems.","2694-0825","979-8-3503-4394-6","10.1109/IC2E59103.2023.00037","Boston University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10305822","serverless;function-as-a-service;single-board computers;OpenFaaS","Computers;Cloud computing;Scalability;Architecture;Buildings;Prototypes;Computer architecture","","1","","5","IEEE","6 Nov 2023","","","IEEE","IEEE Conferences"
"Making Serverless Computing More Serverless","Z. Al-Ali; S. Goodarzy; E. Hunter; S. Ha; R. Han; E. Keller; E. Rozner",University of Colorado Boulder; University of Colorado Boulder; University of Colorado Boulder; University of Colorado Boulder; University of Colorado Boulder; University of Colorado Boulder; IBM Research,2018 IEEE 11th International Conference on Cloud Computing (CLOUD),"11 Sep 2018","2018","","","456","459","In serverless computing, developers define a function to handle an event, and the serverless framework horizontally scales the application as needed. The downside of this function-based abstraction is it limits the type of application supported and places a bound on the function to be within the physical resource limitations of the server the function executes on. In this paper we propose a new abstraction for serverless computing: a developer supplies a process and the serverless framework seamlessly scales out the process's resource usage across the datacenter. This abstraction enables processing to not only be more general purpose, but also allows a process to break out of the limitations of a single server – making serverless computing more serverless. To realize this abstraction, we propose ServerlessOS, comprised of three key components: (i) a new disaggregation model, which leverages disaggregation for abstraction, but enables resources to move fluidly between servers for performance; (ii) a cloud orchestration layer which manages fine-grained resource allocation and placement throughout the application's lifetime via local and global decision making; and (iii) an isolation capability that enforces data and resource isolation across disaggregation, effectively extending Linux cgroup functionality to span servers.","2159-6190","978-1-5386-7235-8","10.1109/CLOUD.2018.00064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8457832","serverless;cloud;virtualization;isolation;orchestration;resource disaggregation","Servers;Instruction sets;Cloud computing;Sockets;Couplings;Micromechanical devices;Memory management","","22","","13","IEEE","11 Sep 2018","","","IEEE","IEEE Conferences"
"Towards Serverless Sky Computing: An Investigation on Global Workload Distribution to Mitigate Carbon Intensity, Network Latency, and Cost","R. Cordingly; J. Kaur; D. Dwivedi; W. Lloyd","School of Engineering and Technology, University of Washington, Tacoma, Washington, USA; School of Engineering and Technology, University of Washington, Tacoma, Washington, USA; School of Engineering and Technology, University of Washington, Tacoma, Washington, USA; School of Engineering and Technology, University of Washington, Tacoma, Washington, USA",2023 IEEE International Conference on Cloud Engineering (IC2E),"6 Nov 2023","2023","","","59","69","The high demand for energy consumption and the resulting carbon footprint of the cloud pose significant sustainability challenges, as cloud data centers consume vast amounts of energy. The emergence of serverless cloud computing platforms has opened up new avenues for more sustainable cloud computing. Serverless Function-as-a-Service (FaaS) cloud computing platforms facilitate deploying applications as decoupled microservices to leverage automatic rapid scaling, high availability, fault tolerance, and on-demand pricing. The absence of always-on hosting costs associated with virtual machines enables serverless functions to be deployed with many different function configurations and cloud regions to achieve high performance, low network latency, and reduced costs. In this paper, we investigate the utility of a global sky computing platform where serverless resources are aggregated between up to 19 distinct cloud regions. We prototype a serverless load distribution system to distribute client requests across serverless aggregations to minimize performance objectives, including network latency, runtime, hosting costs, and carbon footprint. To evaluate our serverless distribution system's ability to meet performance objectives, we continuously executed large experiments across 19 regions around the world from November 2022 through March 2023. Our serverless load distribution approach using aggregated resources reduced the carbon intensity of a globally distributed serverless application by up to 99.8%, network latency by 65%, or hosting costs by 58% by optimizing function routing to deployments with optimal hardware configurations.","2694-0825","979-8-3503-4394-6","10.1109/IC2E59103.2023.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10305816","Sky Computing;Serverless Computing;Function-as-a-Service;Green Computing","Cloud computing;Costs;Runtime;Prototypes;Pricing;Routing;Virtual machining","","7","","26","IEEE","6 Nov 2023","","","IEEE","IEEE Conferences"
"FaaSCTDO: Collaborative Task-Data Orchestration for Serverless Workflows","N. Yang; H. Zhang; Y. Zhang","Beijing Key Lab of Intelligent Telecommunication Software and Multimedia, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Lab of Intelligent Telecommunication Software and Multimedia, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Lab of Intelligent Telecommunication Software and Multimedia, Beijing University of Posts and Telecommunications, Beijing, China",2023 IEEE 16th International Conference on Cloud Computing (CLOUD),"25 Sep 2023","2023","","","526","528","The use of Function-as-a-Service (FaaS) platforms for executing complex serverless workflows has gained significant popularity. However, the stateless nature of FaaS requires functions to rely on remote storage to store their state, leading to performance overhead and reduced efficiency. Previous approaches using excess memory resources as caches can cause resource contention and decreased task execution performance. Moreover, the impact of workflow orchestration approaches on latency has not been fully explored. To address these challenges, we propose FaaSCTDO, a framework for collaborative task and data orchestration in serverless workflows. FaaSCTDO provides developers with comprehensive lifecycle management capabilities, treating data as orchestratable objects. It groups tasks and data with high data correlation together and allocates them on the same server node, leveraging data locality to expedite function access to data.","2159-6190","979-8-3503-0481-7","10.1109/CLOUD60044.2023.00070","National Natural Science Foundation of China (NSFC)(grant numbers:62172050); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10254972","FaaS;serverless workflows;orchestration;data locality;workflow grouping","Cloud computing;Correlation;Collaboration;Servers;Task analysis","","","","8","IEEE","25 Sep 2023","","","IEEE","IEEE Conferences"
"An Auto-Scaling Framework for Predictable Open Source FaaS Function Chains","D. Balla; M. Maliosz; C. Simon","Department of Telecommunications and Media Informatics Budapest, High Speed Networks Laboratory, University of Technology and Economics, Budapest, Hungary; Department of Telecommunications and Media Informatics Budapest, High Speed Networks Laboratory, University of Technology and Economics, Budapest, Hungary; Department of Telecommunications and Media Informatics Budapest, High Speed Networks Laboratory, University of Technology and Economics, Budapest, Hungary",2023 IEEE 16th International Conference on Cloud Computing (CLOUD),"25 Sep 2023","2023","","","229","237","Function as a Service (FaaS) is a novel approach for application virtualization in the cloud. Complex cloud native applications may be implemented as chains of functions. Due to the inter-dependencies between the functions of such chains, their performance is harder to predict. This also means that the design of a resource efficient scaling mechanism is a challenging task, the goal is to avoid wasteful scaling strategies. In this paper, we propose an auto-scaling framework that keeps the amount of resources, allocated for the functions in the chain, at a minimum level, while meeting the latency requirements of an application implemented as a function chain. Then, we show how to integrate our auto-scaling framework with a general open source FaaS system, as well as with, OpenFaaS. We also introduce our modifications on OpenFaaS that enable to use various load-balancing algorithms for distributing the requests between the function instances. We compare our auto-scaling framework to the auto-scaler of OpenFaaS by the completion time of our python3 based image processing test function chain, as well as, by the amount of allocated resources.","2159-6190","979-8-3503-0481-7","10.1109/CLOUD60044.2023.00034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10255010","FaaS;Function as a Service;Function chain;Auto-Scaling;Simulation","Cloud computing;Image processing;Computer architecture;Prediction algorithms;Load management;Task analysis;Edge computing","","1","","17","IEEE","25 Sep 2023","","","IEEE","IEEE Conferences"
"TriggerBench: A Performance Benchmark for Serverless Function Triggers","J. Scheuner; M. Bertilsson; O. Grönqvist; H. Tao; H. Lagergren; J. -P. Steghöfer; P. Leitner","Department of Computer Science and Engineering, Chalmers | University of Gothenburg, Gothenburg, Sweden; Department of Computer Science and Engineering, Chalmers | University of Gothenburg, Gothenburg, Sweden; Department of Computer Science and Engineering, Chalmers | University of Gothenburg, Gothenburg, Sweden; Department of Computer Science and Engineering, Chalmers | University of Gothenburg, Gothenburg, Sweden; Department of Computer Science and Engineering, Chalmers | University of Gothenburg, Gothenburg, Sweden; Department of Computer Science and Engineering, Chalmers | University of Gothenburg, Gothenburg, Sweden; Department of Computer Science and Engineering, Chalmers | University of Gothenburg, Gothenburg, Sweden",2022 IEEE International Conference on Cloud Engineering (IC2E),"16 Nov 2022","2022","","","96","103","Serverless computing offers a scalable event-based paradigm for deploying managed cloud-native applications. Function triggers are essential building blocks in serverless, as they initiate any function execution. However, function triggering is insufficiently studied and inherently hard to measure given the distributed, ephemeral, and asynchronous nature of event-based function coordination. To address this gap, we present TriggerBench, a cross-provider benchmark for evaluating serverless function triggers based on distributed tracing. We evaluate the trigger latency (i.e., time to transition between two functions) of eight types of triggers in Microsoft Azure and three in AWS. Our results show that all triggers suffer from long tail latency, storage triggers introduce variable multi-second delays, and HTTP triggers are most suitable for interactive applications. Our insights can guide developers in choosing optimal event or messaging triggers for latency-sensitive applications. Researchers can extend TriggerBench to study the latency, scalability, and reliability of further trigger types and cloud providers.","","978-1-6654-9115-0","10.1109/IC2E55432.2022.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9946324","serverless;FaaS;triggers;distributed tracing;observability;performance;benchmarking","Runtime;Scalability;Buildings;Serverless computing;Tail;Benchmark testing;Delays","","5","","40","IEEE","16 Nov 2022","","","IEEE","IEEE Conferences"
"ElastiBench: Scalable Continuous Benchmarking on Cloud FaaS Platforms","T. Schirmer; T. Pfandzelter; D. Bermbach","Scalable Software Systems Research Group, Technische Universität Berlin & Einstein Center Digital Future; Scalable Software Systems Research Group, Technische Universität Berlin & Einstein Center Digital Future; Scalable Software Systems Research Group, Technische Universität Berlin & Einstein Center Digital Future",2024 IEEE International Conference on Cloud Engineering (IC2E),"14 Nov 2024","2024","","","83","92","Running microbenchmark suites often and early in the development process enables developers to identify performance issues in their application. Microbenchmark suites of complex applications can comprise hundreds of individual benchmarks and take multiple hours to evaluate meaningfully, making running those benchmarks as part of CI/CD pipelines infeasible. In this paper, we reduce the total execution time of microbenchmark suites by leveraging the massive scalability and elasticity of FaaS (Function-as-a-Service) platforms. While using FaaS enables users to quickly scale up to thousands of parallel function instances to speed up microbenchmarking, the performance variation and low control over the underlying computing resources complicate reliable benchmarking. We present ElastiBench, an architecture for executing microbenchmark suites on cloud FaaS platforms, and evaluate it on code changes from an open-source time series database. Our evaluation shows that our prototype can produce reliable results ($\sim 95 \%$ of performance changes accurately detected) in a quarter of the time ($\leq 15 \mathrm{~min}$ vs. $\sim 4 \mathrm{~h}$) and at lower cost ($\$0.49$ vs. $\$ 1.18$) compared to cloud-based virtual machines.","2694-0825","979-8-3315-2869-0","10.1109/IC2E61754.2024.00016","Bundesministerium für Bildung und Forschung; Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10749788","Serverless;FaaS;Microbenchmarks","Cloud computing;Costs;Scalability;Time series analysis;Pipelines;Prototypes;Computer architecture;Benchmark testing;Parallel processing;Robustness","","1","","52","IEEE","14 Nov 2024","","","IEEE","IEEE Conferences"
"CASY: A CPU Cache Allocation System for FaaS Platform","A. Jeatsa; B. Teabe; D. Hagimont","IRIT, Universite de Toulouse, CNRS, Toulouse INP, UT3, Toulouse, France; IRIT, Universite de Toulouse, CNRS, Toulouse INP, UT3, Toulouse, France; IRIT, Universite de Toulouse, CNRS, Toulouse INP, UT3, Toulouse, France","2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","19 Jul 2022","2022","","","494","503","Function as a Service (FaaS) has become a key service in the cloud. It enables customers to conceive their appli-cation as a collection of minimal serverless functions interacting with each other. FaaS platforms abstract all the management complexity to the client. This emerging paradigm is also attractive because of its billing model. Clients are charged based on the execution time of functions, allowing finer-grained pricing. There-fore, executing functions as fast as possible is very important to lower the cost. Several research studies have investigated runtime optimization in FaaS environments, but none have explored CPU cache allocation. Indeed, CPU cache contention is a well-known issue in software and FaaS is not exempt from this issue. Various hardware improvements have been made to address the CPU cache partitioning problem. Among other things, Intel has implemented a new technology in their new processors that allows cache partitioning: Cache Allocation Technology (CAT). This technology allows allocating cache ways to processes, and the usage of the cache by each process will be limited to the allocated amount. In this paper, we propose CASY (CPU Cache Allocation SYstem), a system that performs CPU cache allocation for serverless functions using the Intel CAT technology. CASY uses machine learning to build a cache usage profile for functions and uses this profile to predict the cache requirements based on the function's input data. Because the CPU's cache size is small, CASY integrates an allocation algorithm which ensures that the cache loads are balanced on all cache ways. We implemented our system and integrated it into the Open Whisk FaaS platform. Our evaluations show a 11 % decrease in execution time for some serverless functions without degrading the performance of other functions.","","978-1-6654-9956-9","10.1109/CCGrid54584.2022.00059","Agence nationale de la recherche(grant numbers:ANR-20-CE25-0005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9825963","FaaS;CPU Cache;Allocation","Runtime;Program processors;FAA;Pricing;Predictive models;Prediction algorithms;Software","","2","","31","IEEE","19 Jul 2022","","","IEEE","IEEE Conferences"
"KneeScale: Efficient Resource Scaling for Serverless Computing at the Edge","X. Li; P. Kang; J. Molone; W. Wang; P. Lama","Department of Computer Science, University of Texas at San Antonio, San Antonio, Texas; Department of Computer Science, University of Texas at San Antonio, San Antonio, Texas; Department of Computer Science, University of Texas at San Antonio, San Antonio, Texas; Department of Computer Science, University of Texas at San Antonio, San Antonio, Texas; Department of Computer Science, University of Texas at San Antonio, San Antonio, Texas","2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","19 Jul 2022","2022","","","180","189","Serverless computing is a promising paradigm for delivering services to the Internet of Things (IoT) applications at the edge of the network. Its event-triggered computation, as well as fine-grained and agile resource scaling, is well-suited for a resource-constrained edge computing environment. However, general-purpose auto-scalers that are predominant in the cloud settings perform poorly for serverless computing at the Edge. This is mainly due to the difficulty in quickly determining the optimal resource allocation under resource-budget constraints and dynamic workloads. In this paper, we present an adaptive auto-scaler, KneeScale, that dynamically adjusts the number of replicas for serverless functions to reach a point at which the relative cost to increase resource allocation is no longer worth the corresponding performance benefit. We have designed and implemented KneeScale as lightweight system software that utilizes Kubernetes for resource management. Experimental results with a function-as-a-service (FaaS) benchmark, FunetionBeneh, and an open-source serverless computing platform, OpenFaaS, demonstrate the superior performance and resource efficiency of KneeScale. It outperforms Kubernetes Horizontal Pod AutoScaler (HPA) and OpenFaaS built-in scheduler in terms of cumulative performance under a given resource budget by up to 32 % and 106 % respectively. KneeScale achieves higher cumulative throughput than both competing techniques, lower latencies than OpenFaaS built-in scheduler, and similar latencies compared to HPA for a variety of serverless functions.","","978-1-6654-9956-9","10.1109/CCGrid54584.2022.00027","NSF(grant numbers:CNS 1911012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9826051","Resource Scaling;Edge;Serverless Computing","Costs;Serverless computing;FAA;Benchmark testing;Throughput;Dynamic scheduling;System software","","15","","22","IEEE","19 Jul 2022","","","IEEE","IEEE Conferences"
"A Case for Function-as-a-Service with Disaggregated FPGAs","B. Ringlein; F. Abel; D. Diamantopoulos; B. Weiss; C. Hagleitner; M. Reichenbach; D. Fey",IBM Research Europe; IBM Research Europe; IBM Research Europe; IBM Research Europe; IBM Research Europe; Friedrich-Alexander University Erlangen- Nürnberg; Friedrich-Alexander University Erlangen- Nürnberg,2021 IEEE 14th International Conference on Cloud Computing (CLOUD),"8 Nov 2021","2021","","","333","344","The slowdown of Moore's law and the end of Dennard scaling created a demand for specialized accelerators, including Field Programmable Gate Arrays (FPGAs), in cloud data centers. At the same time, compute resources are increasingly consumed via public and private clouds and traditional applications are modernized using scalable microservices and Function-as-a-Service (FaaS) offerings. Nonetheless, true FaaS based on FPGAs or other accelerators is virtually absent from the offering catalogs of all major cloud providers. In addition, FPGA applications are typically coded in a monolithic fashion, due to device and vendor specific dependencies, which reduces the portability and usability of FPGA cloud offerings further. However, FPGA-based FaaS can improve execution efficiency and minimize (tail-) latencies while decreasing costs. We propose a novel system architecture, called Mantle, that uses disaggregated FPGAs to enable scalable, usable, portable and efficient FaaS offerings for FPGAs. Our experimental results demonstrate a significant reduction of end-to-end service provisioning time to below 7 seconds and an increase in execution efficiency by a factor of 4 with negligible overhead.","2159-6190","978-1-6654-0060-2","10.1109/CLOUD53861.2021.00047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9582275","cloud computing;reconfigurable computing;disaggregated FPGAs;Shell Role architecture;partial reconfiguration;FaaS;forward compatibility;dynamic ISA extensions","Semiconductor device modeling;Cloud computing;Data centers;Moore's Law;Systems architecture;FAA;Computer architecture","","8","","62","IEEE","8 Nov 2021","","","IEEE","IEEE Conferences"
"Pushing Serverless to the Edge with WebAssembly Runtimes","P. Gackstatter; P. A. Frangoudis; S. Dustdar","Distributed Systems Group, TU Wien, Vienna, Austria; Distributed Systems Group, TU Wien, Vienna, Austria; Distributed Systems Group, TU Wien, Vienna, Austria","2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","19 Jul 2022","2022","","","140","149","Serverless computing has become a popular part of the cloud computing model, thanks to abstracting away infrastructure management and enabling developers to write functions that auto-scale in a polyglot environment, while only paying for the used compute time. While this model is ideal for handling unpredictable and bursty workloads, cold-start latencies of hundreds of milliseconds or more still hinder its support for latency-critical IoT services, and may cancel the latency benefits that come with proximity, when serverless functions are deployed at the edge. Moreover, CPU power and memory limitations which often characterize edge hosts drive latencies even higher. The root of the problem lies in the de facto runtime environments for serverless functions, namely container technologies such as Docker. A radical approach is thus to replace them with a more light-weight alternative. For this purpose, we examine WebAssembly's suitability for use as a serverless container runtime, with a focus on edge computing settings, and present the design and implementation of a WebAssembly-based runtime environment for serverless edge computing. WOW, our prototype for WebAssembly execution in Apache QpenWhisk, reduces cold-start latency by up to 99.5%, can improve on memory consumption by more than 5×, and increases function execution throughput by up to 4.2× on low-end edge computing equipment compared to the standard Docker-based container runtime for various serverless workloads.","","978-1-6654-9956-9","10.1109/CCGrid54584.2022.00023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9826054","Function-as-a-Service;edge computing;server-less;WebAssembly","Runtime environment;Computational modeling;Memory management;Serverless computing;Prototypes;Containers;Throughput","","30","","36","IEEE","19 Jul 2022","","","IEEE","IEEE Conferences"
"Increasing Efficiency and Result Reliability of Continuous Benchmarking for FaaS Applications","T. C. Rese; N. Japke; S. Koch; T. Pfandzelter; D. Bermbach","Scalable Software Systems Group, Technische Universität Berlin & Einstein Center Digital Future, Berlin, Germany; Scalable Software Systems Group, Technische Universität Berlin & Einstein Center Digital Future, Berlin, Germany; Scalable Software Systems Group, Technische Universität Berlin & Einstein Center Digital Future, Berlin, Germany; Scalable Software Systems Group, Technische Universität Berlin & Einstein Center Digital Future, Berlin, Germany; Scalable Software Systems Group, Technische Universität Berlin & Einstein Center Digital Future, Berlin, Germany",2024 IEEE International Conference on Cloud Engineering (IC2E),"14 Nov 2024","2024","","","93","100","In a continuous deployment setting, Function-as-aService (FaaS) applications frequently receive updated releases, each of which can cause a performance regression. While continuous benchmarking, i.e., comparing benchmark results of the updated and the previous version, can detect such regressions, performance variability of FaaS platforms necessitates thousands of function calls, thus, making continuous benchmarking time intensive and expensive. In this paper, we propose DuetFaaS, an approach which adapts duet benchmarking to FaaS applications. With DuetFaaS, we deploy two versions of FaaS function in a single cloud function instance and execute them in parallel to reduce the impact of platform variability. We evaluate our approach against state-of-the-art approaches, running on AWS Lambda. Overall, DuetFaaS requires fewer invocations to accurately detect performance regressions than other state-of-the-art approaches. In $98.41 \%$ of evaluated cases, our approach provides equal or smaller confidence interval size. DuetFaaS achieves an interval size reduction in $59.06 \%$ of all evaluated sample sizes when compared to the competitive approaches.","2694-0825","979-8-3315-2869-0","10.1109/IC2E61754.2024.00017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10749743","Function-as-a-Service;serverless;benchmarking","Accuracy;Prototypes;Benchmark testing;Software;Software reliability","","","","31","IEEE","14 Nov 2024","","","IEEE","IEEE Conferences"
"Multi-Objective Workflow Scheduling to Serverless Architecture in a Multi-Cloud Environment","M. Ramesh; D. Chahal; C. Phalak; R. Singhal","TCS Research, Mumbai, India; TCS Research, Mumbai, India; TCS Research, Mumbai, India; TCS Research, New York, USA",2023 IEEE International Conference on Cloud Engineering (IC2E),"6 Nov 2023","2023","","","173","183","Many complex workflows consist of multiple tasks represented as a directed acyclic graph (DAG). Optimal deployment of such workflows on a cloud using multiple services requires a judicious selection of compute and storage services to minimize the makespan and the cost of deployment. Multi-cloud deployment is emerging as a preferred choice for the deployment of complex workflows for price competitiveness and freedom from vendor lock-in. However, finding an optimal mapping scheme for heterogeneous tasks of a workflow in a multi-cloud environment is a challenge. Furthermore, each participating cloud service provider (CSP) has a unique cost model and maximum deliverable performance. This makes exploration of the optimal configuration of the chosen service daunting. Many algorithms, frameworks, and tools have been proposed to schedule complex workflows on cloud using virtual machines (VMs) available as Infrastructure-as-a-Service (IaaS). However, the use of scalable and cost-effective serverless platforms offered as Function-as-a-Service (FaaS) is still in its infancy.In this work, we use particle swarm optimization (PSO) for mapping complex workflows to cloud services such as computing, and storage in a multi-cloud scenario. We map complex workflows to the serverless platforms and storage services from popular cloud vendors namely Amazon Web Services (AWS), Azure (AZR), and Google Cloud Platform (GCP). The experimental evaluation shows that our approach results in up to 61% improvement in makespan and 51% improvement in the cost of workflow deployment as compared to naive and intuition-based mapping in a cloud.","2694-0825","979-8-3503-4394-6","10.1109/IC2E59103.2023.00027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10305826","Multi-cloud;serverless;FaaS;PSO;makespan;cost","Cloud computing;Schedules;Costs;Web services;Perturbation methods;Optimal scheduling;Virtual machining","","","","43","IEEE","6 Nov 2023","","","IEEE","IEEE Conferences"
"XFBench: A Cross-Cloud Benchmark Suite for Evaluating FaaS Workflow Platforms","V. Kulkarni; N. Reddy; T. Khare; H. Mohan; J. Murali; M. A; R. B; S. Balajee; S. S; S. D; V. S; Y. V; C. Babu; A. S. Prasad; Y. Simmhan","Indian Institute of Science, Bangalore, India; Indian Institute of Science, Bangalore, India; Indian Institute of Science, Bangalore, India; Sri Sivasubramaniya Nadar College of Engineering, Chennai, India; Sri Sivasubramaniya Nadar College of Engineering, Chennai, India; Sri Sivasubramaniya Nadar College of Engineering, Chennai, India; Sri Sivasubramaniya Nadar College of Engineering, Chennai, India; Sri Sivasubramaniya Nadar College of Engineering, Chennai, India; Sri Sivasubramaniya Nadar College of Engineering, Chennai, India; Sri Sivasubramaniya Nadar College of Engineering, Chennai, India; Sri Sivasubramaniya Nadar College of Engineering, Chennai, India; Sri Sivasubramaniya Nadar College of Engineering, Chennai, India; Sri Sivasubramaniya Nadar College of Engineering, Chennai, India; Indian Institute of Technology, Ropar, India; Indian Institute of Science, Bangalore, India","2024 IEEE 24th International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","8 Oct 2024","2024","","","543","556","Functions-as-a-Service (FaaS) is a widely used serverless computing abstraction that helps developers build applications using event-driven, stateless functions that execute on the cloud. Commercial FaaS platforms such as AWS Lambda and Azure Functions offer elastic auto-scaling and invocation-level billing to ease operations. Applications are often composed as a dataflow of FaaS functions that are orchestrated by FaaS workflow platforms such as AWS Step Functions or Azure Durable Functions. However, the proprietary nature of FaaS platforms on public clouds means that their internals are less understood. While benchmarks to characterize FaaS platforms exist, none are available for a principled evaluation of FaaS workflow platforms. Further, they are less configurable and often limited to simple workloads and a single cloud provider. We address this limitation by proposing XFBench, an end-to-end automated benchmarking framework for FaaS workflows that works across clouds, and an accompanying function, workflow, and workload suite. The user provides a generic definition of the workflow and workload for benchmarking, and XFBench automatically deploys the workflows across multiple cloud platforms, generates client requests, and profiles the execution. We validate XFBench with realistic workflows and workloads on AWS and Azure platforms in different global regions to offer early insights into understanding the inter-function communication, function execution time, and cold start scaling.","2993-2114","979-8-3503-9566-2","10.1109/CCGrid59990.2024.00067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10701340","Faas;Serverless;Benchmarking;HybridCloud;Performance Characterisation","Measurement;Cloud computing;Costing;Serverless computing;Benchmark testing;Logic","","1","","36","IEEE","8 Oct 2024","","","IEEE","IEEE Conferences"
"OrcBench: A Representative Serverless Benchmark","R. Hancock; S. Udayashankar; A. J. Mashtizadeh; S. Al-Kiswany","University of Waterloo, Waterloo, Canada; University of Waterloo, Waterloo, Canada; University of Waterloo, Waterloo, Canada; University of Waterloo, Waterloo, Canada",2022 IEEE 15th International Conference on Cloud Computing (CLOUD),"24 Aug 2022","2022","","","103","108","Serverless computing is rapidly growing area of research. No standardized benchmark currently exists for evaluating orchestration level decisions or executing large serverless workloads because of the limited data provided by cloud providers. Current benchmarks focus on other aspects, such as the cost of running general types of functions and their runtimes.We introduce OrcBench, the first orchestration benchmark based on the recently published Microsoft Azure serverless data set. OrcBench categorizes 8622 serverless functions into 17 distinct models, which represent 5.6 million invocations from the original trace.OrcBench also incorporates a time-series analysis to identify function chains within the dataset. OrcBench can use these to create workloads that mimic complete serverless applications, which includes simulating CPU and memory usage. The modeling allows these workloads to be scaled according to the target hardware configuration.","2159-6190","978-1-6654-8137-3","10.1109/CLOUD55607.2022.00028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9860528","benchmark;serverless;cloud;modeling","Analytical models;Costs;Time series analysis;Serverless computing;Benchmark testing;Signal generators;Hardware","","2","","14","IEEE","24 Aug 2022","","","IEEE","IEEE Conferences"
"TrapShield: Enhancing Security and Privacy in Serverless Workflows using Honeypots by Robust Adversary Penalization","S. Garg; M. Suresh; M. S. D. Thakur; M. A. Rajan; P. Sahu; M. Gharote; M. Ramesh; S. Lodha","TCS Innovation Labs Tata Consultancy Services, New Delhi, India; Department of Chemical Engineering, Indian Institute of Technology Bombay, Mumbai, India; TCS Innovation Labs Tata Consultancy Services, Bengaluru, India; TCS Innovation Labs Tata Consultancy Services, Bengaluru, India; TCS Innovation Labs Tata Consultancy Services, New Delhi, India; TCS Innovation Labs Tata Consultancy Services, Pune, India; TCS Innovation Labs Tata Consultancy Services, Mumbai, India; TCS Innovation Labs Tata Consultancy Services, Pune, India",2024 IEEE International Conference on Cloud Engineering (IC2E),"14 Nov 2024","2024","","","245","246","The marked shift of application developers to serverless computing has led to an increase in the number of cyberattacks and privacy concerns, thus prompting the need for secure serverless workflows. We propose TrapShield, a honeypots-based, secure and privacy preserving framework to protect serverless computing applications from insider and outsider attacks. It utilizes honeypots to deceive the attackers and penalize them by redirecting to a random set of dummy functions forming a cycle. Evaluations on Google Cloud Platform and Amazon Web Services for three popular serverless applications show TrapShield’s effectiveness in reducing costs for thwarting attacks while maintaining high runtime performance (approximately 1.3 seconds for an airline booking application).","2694-0825","979-8-3315-2869-0","10.1109/IC2E61754.2024.00034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10749767","Serverless Cloud Security;Workflows;Insider attacks;Honeypots;Dummy nodes;Penalize;Cost Optimization","Privacy;Costs;Runtime;Web services;Serverless computing;Information filtering;Security;Optimization;Airline industry;Information integrity","","","","2","IEEE","14 Nov 2024","","","IEEE","IEEE Conferences"
"GlobalFlow: A Cross-Region Orchestration Service for Serverless Computing Services","G. Zheng; Y. Peng","University of Washington Bothell, Bothell, WA; University of Washington Bothell, Bothell, WA",2019 IEEE 12th International Conference on Cloud Computing (CLOUD),"29 Aug 2019","2019","","","508","510","With the development of serverless computing, orchestration of multiple serverless computing services is highly desired by many cloud-based applications. In this paper, we present GlobalFlow, an orchestration service that can coordinate various geographically distributed but logically dependent serverless computing services through copy-based or connector-based strategy. Through preliminary evaluation, the proposed service has demonstrated its effectiveness in orchestrating various AWS Lambda functions in different regions without significant overhead.","2159-6190","978-1-7281-2705-7","10.1109/CLOUD.2019.00093","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8814505","serverless computing;orchestration;geo distributed;AWS Lambda;cloud computing","","","8","","8","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"Optimizing Cloud Function Configuration via Local Simulations","J. Manner; M. Endreβ; S. Böhm; G. Wirtz","Distributed Systems Group Otto-Friedrich-University, Bamberg, Germany; Distributed Systems Group Otto-Friedrich-University, Bamberg, Germany; Distributed Systems Group Otto-Friedrich-University, Bamberg, Germany; Distributed Systems Group Otto-Friedrich-University, Bamberg, Germany",2021 IEEE 14th International Conference on Cloud Computing (CLOUD),"8 Nov 2021","2021","","","168","178","Function as a Service (FaaS) - the reason why so many practitioners and researchers talk about Serverless Computing - claims to hide all operational concerns. The promise when using FaaS is that users only have to focus on the core business functionality in form of cloud functions. However, a few configuration options remain within the developer's responsibility. Most of the currently available cloud function offerings force the user to choose a memory or other resource setting and a timeout value. CPU is scaled based on the chosen options. At a first glance, this seems like an easy task, but the tradeoff between performance and cost has implications on the quality of service of a cloud function. Therefore, in this paper we present a local simulation approach for cloud functions and support developers in choosing a suitable configuration. The methodology we propose simulates the execution behavior of cloud functions locally, makes the cloud and local environment comparable and maps the local profiling data to a cloud platform. This reduces time during the development and enables developers to work with their familiar tools. This is especially helpful when implementing multi-threaded cloud functions.","2159-6190","978-1-6654-0060-2","10.1109/CLOUD53861.2021.00030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9582177","Serverless;Function as a Service;FaaS;Benchmarking;Simulation;Profiling","Cloud computing;Costs;Conferences;Computational modeling;Force;FAA;Quality of service","","10","","62","IEEE","8 Nov 2021","","","IEEE","IEEE Conferences"
"HARDLESS: A Generalized Serverless Compute Architecture for Hardware Processing Accelerators","S. Werner; T. Schirmer","ISE, TU Berlin, Germany; MCC & ECDF, TU Berlin, Germany",2022 IEEE International Conference on Cloud Engineering (IC2E),"16 Nov 2022","2022","","","79","84","The increasing use of hardware processing accelerators tailored for specific applications, such as the Vision Processing Unit (VPU) for image recognition, further increases developers' configuration, development, and management over-head. Developers have successfully used fully automated elastic cloud services such as serverless computing to counter these additional efforts and shorten development cycles for applications running on CPUs. Unfortunately, current cloud solutions do not yet provide these simplifications for applications that require hardware acceleration. However, as the development of special-ized hardware acceleration continues to provide performance and cost improvements, it will become increasingly important to enable ease of use in the cloud. In this paper, we present an initial design and implemen-tation of Hardless, an extensible and generalized serverless computing architecture that can support workloads for arbitrary hardware accelerators. We show how Hardless can scale across different commodity hardware accelerators and support a variety of workloads using the same execution and programming model common in serverless computing today.","","978-1-6654-9115-0","10.1109/IC2E55432.2022.00016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9946318","serverless engineering;accelerated computing","Image recognition;Costs;Processor scheduling;Computational modeling;Serverless computing;Graphics processing units;Computer architecture","","4","","18","IEEE","16 Nov 2022","","","IEEE","IEEE Conferences"
"Serving Deep Learning Models in a Serverless Platform","V. Ishakian; V. Muthusamy; A. Slominski",Bentley University; IBM T.J. Watson Research Center; IBM T.J. Watson Research Center,2018 IEEE International Conference on Cloud Engineering (IC2E),"17 May 2018","2018","","","257","262","Serverless computing has emerged as a compelling paradigm for the development and deployment of a wide range of event based cloud applications. At the same time, cloud providers and enterprise companies are heavily adopting machine learning and Artificial Intelligence to either differentiate themselves, or provide their customers with value added services. In this work we evaluate the suitability of a serverless computing environment for the inferencing of large neural network models. Our experimental evaluations are executed on the AWS Lambda environment using the MxNet deep learning framework. Our experimental results show that while the inferencing latency can be within an acceptable range, longer delays due to cold starts can skew the latency distribution and hence risk violating more stringent SLAs.","","978-1-5386-5008-0","10.1109/IC2E.2018.00052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8360337","AWS Lambda;Cloud Computing;Serverless Computing;Deep Learningm","Machine learning;Cloud computing;Neural networks;Containers;Computational modeling;Delays;Training","","97","","13","IEEE","17 May 2018","","","IEEE","IEEE Conferences"
"Exploiting Serverless Runtimes for Large-Scale Optimization","A. Aytekin; M. Johansson","School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden",2019 IEEE 12th International Conference on Cloud Computing (CLOUD),"29 Aug 2019","2019","","","499","501","Serverless runtimes provide efficient and cost-effective environments for scalable computations, thanks to their event-driven and elastic nature. So far, they have mostly been used for stateless, data parallel and sporadic computations. In this work, we propose exploiting serverless runtimes to solve generic, large-scale optimization problems. To this end, we implement a parallel optimization algorithm for solving a regularized logistic regression problem, and use AWS Lambda for the compute-intensive work. We show that relative speedups up to 256 workers and efficiencies above 70% up to 64 workers can be expected.","2159-6190","978-1-7281-2705-7","10.1109/CLOUD.2019.00090","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8814497","serverless;optimization;alternating direction method of multipliers;distributed optimization","","","9","","13","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"Towards Anti-Collision Coordination for UAVs with Serverless Edge Computing","T. Pfandzelter; D. Bermbach; R. Vilter; I. Friese; S. Melnyk; Q. Zhou; H. D. Schotten","Scalable Software Systems Group, Technische Universität Berlin & Einstein Center Digital Future, Berlin, Germany; Scalable Software Systems Group, Technische Universität Berlin & Einstein Center Digital Future, Berlin, Germany; Aviation Engineering Group, Technical University of Applied Sciences Wildau, Wildau, Germany; Deutsche Telekom AG, Berlin, Germany; Intelligent Networks, German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany; Intelligent Networks, German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany; Intelligent Networks, German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany",2024 IEEE International Conference on Cloud Engineering (IC2E),"14 Nov 2024","2024","","","247","248","Autonomously flying unmanned aerial vehicles in logistics, agriculture, and other domains will take up considerable airspace in the future. In areas with a high amount of air traffic, ground coordination to detect and mitigate encounters with collision risks is required. We propose a serverless approach to build and deploy such a system using edge computing resources. Serverless abstractions allow domain experts to focus on implementing the application logic while the underlying platform manages constrained resources and low-latency service access.","2694-0825","979-8-3315-2869-0","10.1109/IC2E61754.2024.00035","Bundesministerium für Bildung und Forschung; Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10749766","serverless;edge computing;UAVs","Autonomous aerial vehicles;Agriculture;Logic;Low latency communication;Edge computing;Logistics","","","","9","IEEE","14 Nov 2024","","","IEEE","IEEE Conferences"
"Skedulix: Hybrid Cloud Scheduling for Cost-Efficient Execution of Serverless Applications","A. Das; A. Leaf; C. A. Varela; S. Patterson","Department of Computer Science, Rensselaer Polytechnic Institute, Troy, NY, USA; Department of Computer Science, Rensselaer Polytechnic Institute, Troy, NY, USA; Department of Computer Science, Rensselaer Polytechnic Institute, Troy, NY, USA; Department of Computer Science, Rensselaer Polytechnic Institute, Troy, NY, USA",2020 IEEE 13th International Conference on Cloud Computing (CLOUD),"18 Dec 2020","2020","","","609","618","We present a framework for scheduling multifunction serverless applications over a hybrid public-private cloud. A set of serverless jobs is input as a batch, and the objective is to schedule function executions over the hybrid platform to minimize the cost of public cloud use, while completing all jobs by a specified deadline. As this scheduling problem is NP-Hard, we propose a greedy algorithm that dynamically determines both the order and placement of each function execution using predictive models of function execution time and network latencies. We present a prototype implementation of our framework that uses AWS Lambda and OpenFaaS, for the public and private cloud, respectively. We evaluate our prototype in live experiments using a mixture of compute and I/O heavy serverless applications. Our results show that our framework can achieve a speedup in batch processing of up to 1.92 times that of an approach that uses only the private cloud, at 40.5% the cost of an approach that uses only the public cloud.","2159-6190","978-1-7281-8780-8","10.1109/CLOUD49709.2020.00090","National Science Foundation(grant numbers:CNS 1553340,CNS 1816307); Air Force Office of Scientific Research(grant numbers:FA9550-19-1-0054); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284265","Hybrid Cloud;Serverless Computing;Cloud Scheduling;Performance Optimization;Computational Offloading;Function as a Service","Greedy algorithms;Cloud computing;Schedules;Processor scheduling;Prototypes;Predictive models;Servers","","39","","26","IEEE","18 Dec 2020","","","IEEE","IEEE Conferences"
"Automatic Tuning of Hyperparameters for Neural Networks in Serverless Cloud","A. Kaplunovich; Y. Yesha","Department of Computer Science, University of Maryland, Baltimore, USA; Department of Computer Science, University of Maryland, Baltimore, USA",2020 IEEE International Conference on Big Data (Big Data),"19 Mar 2021","2020","","","2751","2756","Deep Neural Networks are used to solve the most challenging world problems. In spite of the numerous advancements in the field, most of the models are being tuned manually. Experienced Data Scientists have to manually optimize hyperparameters, such as dropout rate, learning rate or number of neurons for Big Data applications. We have implemented a flexible automatic real-time hyperparameter tuning methodology. It works for arbitrary models written in Python and Keras. We also utilized state of the art Cloud services such as trigger based serverless computing (Lambda), and advanced GPU instances to implement automation, reliability and scalability.The existing tuning libraries, such as hyperopt, Scikit-Optimize or SageMaker, require developers to provide a list of hyperparameters and the range of their values manually. Our novel approach detects potential hyperparameters automatically from the source code, updates the original model to tune the parameters, runs the evaluation in the Cloud on spot instances, finds the optimal hyperparameters, and saves the results in the No-SQL database. The methodology can be applied to numerous Big Data Machine Learning systems.","","978-1-7281-6251-5","10.1109/BigData50022.2020.9378280","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9378280","Neural Networks;Hyperparameter;Automation;Optimization;Big Data;Cloud;Serverless;Machine Learning;AWS","Machine learning;Tools;Data models;Planning;Reliability;Tuning;Optimization","","8","","27","IEEE","19 Mar 2021","","","IEEE","IEEE Conferences"
"Application Deployment Strategies for Reducing the Cold Start Delay of AWS Lambda","J. Dantas; H. Khazaei; M. Litoiu","Department of Electrical Engineering & Computer Science, York University, Toronto, Canada; Department of Electrical Engineering & Computer Science, York University, Toronto, Canada; Department of Electrical Engineering & Computer Science, York University, Toronto, Canada",2022 IEEE 15th International Conference on Cloud Computing (CLOUD),"24 Aug 2022","2022","","","1","10","Serverless computing has emerged in recent years as the new computing paradigm adopted by key players in the industry for software development. This new paradigm has seen rapid growth in adoption due to its unique billing model and scaling characteristics. Public cloud providers such as Amazon Web Services (AWS) offer several configurations and language runtimes for their serverless functions. Although extensively explored by the research community, this field still lacks current studies that address the many challenges developers face when leveraging serverless functions for real-world applications. One of these challenges that are often overseen by many programmers is the cold start problem which is present in any serverless application. For this reason, we propose the first study to characterize the underlying cold start impacts caused by the choice of language runtime, application size, memory size and deployment type on AWS Lambda. In this paper, we analyze the performance of the container-based deployment and ZIP-based deployment of AWS Lambda using a variety of language runtimes and applications running with different function configurations; then we propose guidelines for developers and cloud managers to consider when deploying/managing the workloads on the cloud.","2159-6190","978-1-6654-8137-3","10.1109/CLOUD55607.2022.00016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9860368","AWS Lambda;Cold Start;Function as a Service;Serverless Computing;Performance Benchmark","Industries;Runtime;Web services;Computational modeling;Serverless computing;Benchmark testing;Software","","6","","31","IEEE","24 Aug 2022","","","IEEE","IEEE Conferences"
"Sequence Clock: A Dynamic Resource Orchestrator for Serverless Architectures","I. Fakinos; A. Tzenetopoulos; D. Masouros; S. Xydis; D. Soudris","School of Electrical and Computer Engineering, National Technical University of Athens; School of Electrical and Computer Engineering, National Technical University of Athens; School of Electrical and Computer Engineering, National Technical University of Athens; School of Electrical and Computer Engineering, National Technical University of Athens; School of Electrical and Computer Engineering, National Technical University of Athens",2022 IEEE 15th International Conference on Cloud Computing (CLOUD),"24 Aug 2022","2022","","","81","90","Function-as-a-service (FaaS) represents the next frontier in the evolution of cloud computing being an emerging paradigm that removes the burden of configuration and management issues from users. This is achieved by replacing the well-established monolithic approach with graphs of standalone, small, stateless, event-driven components called functions. At the same time, from the cloud providers’ perspective, problems such as availability, load balancing and scalability need to be resolved without being aware of the functionality, behavior or resource requirements of their tenants’ code. However, in this context, functions’ containers coexist with others inside a host of finite resources, where a passive resource allocation technique does not guarantee a well-defined quality of service (QoS) in regards to time latency. In this paper, we present Sequence Clock, an expandable latency targeting tool that actively monitors serverless invocations in a cluster and offers execution of a sequential chain of functions, also known as pipelines or sequences, while achieving the targeted time latency. Two regulation methods were utilized, with one of them achieving up to 82% decrease in the severity of time violations and in some cases even eliminating them completely.","2159-6190","978-1-6654-8137-3","10.1109/CLOUD55607.2022.00024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9860832","serverless computing;faas;QoS;OpenWhisk;Kubernetes","Cloud computing;Scalability;Pipelines;Quality of service;FAA;Load management;Regulation","","3","","19","IEEE","24 Aug 2022","","","IEEE","IEEE Conferences"
"MISO: A CRDT-based Middleware for Stateful Objects in the Serverless Edge-Cloud Continuum","V. Goronjic; S. Nastic","Distributed Systems Group, TU Wien, Austria; Distributed Systems Group, TU Wien, Austria",2024 IEEE International Conference on Cloud Engineering (IC2E),"14 Nov 2024","2024","","","55","65","Serverless functions typically depend on external services to manage the application state, which can be difficult at the Edge due to high latency and network costs. Current solutions for stateful serverless functions at the Edge either have limited support for data locality or require mutual consensus for write operations which is hard to achieve at the Edge. This paper introduces MISO, a novel middleware for serverless computing that enables stateful serverless functions across the Edge-Cloud continuum. The middleware provides MISO Objects offering data locality. It is interoperable with existing serverless platforms and allows concurrent state modifications in a decentralized manner. The main contributions of our work include: i) A novel conceptual model to maintain application state in serverless functions called MISO Objects, ii) MISO middleware and an SDK for serverless functions, and iii) The asynchronous state replication mechanism of MISO Objects using an overlay network to optimize data transfer and resource consumption. Our evaluation demonstrated that MISO outperforms the state-of-theart by up to $243 \%$ in terms of total execution time for AllReducetype operations. Furthermore, the state replication exhibits $O(n)$ scalability regarding time, throughput, memory usage, and data volume. We further demonstrate that our work can seamlessly be integrated into an open-source serverless platform and that our SDK requires up to $150 \%$ fewer lines of code and exhibits up to $75 \%$ less cognitive complexity than the state-of-the-art.","2694-0825","979-8-3315-2869-0","10.1109/IC2E61754.2024.00013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10749737","serverless;faas;middleware;CRDT;stateful objects;edge-cloud continuum","Codes;Overlay networks;Scalability;Serverless computing;Transforms;MISO communication;Programming;Throughput;Complexity theory;Middleware","","1","","25","IEEE","14 Nov 2024","","","IEEE","IEEE Conferences"
"FIRST: Exploiting the Multi-Dimensional Attributes of Functions for Power-Aware Serverless Computing","L. Zhang; C. Li; X. Wang; W. Feng; Z. Yu; Q. Chen; J. Leng; M. Guo; P. Yang; S. Yue","Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Tencent; Tencent",2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS),"18 Jul 2023","2023","","","864","874","Emerging cloud-native development models raise new challenges for managing server performance and power at microsecond scale. Compared with traditional cloud workloads, serverless functions exhibit unprecedented heterogeneity, variability, and dynamicity. Designing cloud-native power management schemes for serverless functions requires significant engineering effort. Current solutions remain sub-optimal since their orchestration process is often one-sided, lacking a systematic view. A key obstacle to truly efficient function deployment is the fundamental wide abstraction gap between the upper-layer request scheduling and the low-level hardware execution.In this work, we show that the optimal operating point (OOP) for energy efficiency cannot be attained without synthesizing the multi-dimensional attributes of functions. We present FIRST, a novel mechanism that enables servers to better orchestrate serverless functions. The key feature of FIRST is that it leverages a lightweight Internal Representation and meta-Scheduling (IRS) layer for collecting the maximum potential revenue from the servers. Specifically, FIRST follows a pipeline-style workflow. Its frontend components aim to analyze functions from different angles and expose their key features to the system. Meanwhile, its backend components are able to make informed function assignment decisions to avoid OOP divergence. We further demonstrate the way to create extensions based on FIRST to enable versatile cloud-native power management. In total, our design constitutes a flexible management layer that supports power-aware function deployment. We show that FIRST could allow 94% functions to be processed under the OOP, which brings up to 24% energy efficiency improvements.","1530-2075","979-8-3503-3766-2","10.1109/IPDPS54959.2023.00091","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10177391","FaaS;power management;multicore","Distributed processing;Systematics;Power system management;Serverless computing;Energy efficiency;Hardware;Behavioral sciences","","6","","40","IEEE","18 Jul 2023","","","IEEE","IEEE Conferences"
"On Realizing Efficient Deep Learning Using Serverless Computing","K. Assogba; M. Arif; M. M. Rafique; D. S. Nikolopoulos",Rochester Institute of Technology; Rochester Institute of Technology; Rochester Institute of Technology; Virginia Tech,"2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","19 Jul 2022","2022","","","220","229","Serverless computing is gaining rapid popularity as it enables quick application deployment and seamless application scaling without managing complex computing resources. Re-cently, it has been explored for running data-intensive, e.g., deep learning (DL), workloads for improving application performance and reducing execution cost. However, serverless computing imposes resource-level constraints, specifically fixed memory allocation and short task timeouts, that lead to job failures. In this paper, we address these constraints and develop an effective runtime framework, DiSDeL, that improves the performance of DL jobs by leveraging data splitting techniques, and ensuring that an appropriate amount of memory is allocated to containers for storing application data and a suitable timeout is selected for each job based on its complexity in serverless deployments. We implement our approach using Apache OpenWhisk and TensorFlow platforms and evaluate it using representative DL workloads to show that it eliminates DL job failures and reduces action memory consumption and total training time by up to 44% and 46%, respectively as compared to a default serverless computing framework. Our evaluation also shows that DiSDeL achieves a performance improvement of up to 29% as compared to bare-metal TensorFlow environment in a multi-tenant setting.","","978-1-6654-9956-9","10.1109/CCGrid54584.2022.00031","National Science Foundation(grant numbers:2106634,2106635); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9826021","Data-intensive Computing;Serverless Computing;Deep Learning;Data Parallelism;OpenWhisk;TensorFlow","Training;Deep learning;Runtime;Processor scheduling;Memory management;Serverless computing;Containers","","6","","46","IEEE","19 Jul 2022","","","IEEE","IEEE Conferences"
"Tackling Cold Start in Serverless Computing with Multi-Level Container Reuse","A. C. Zhou; R. Huang; Z. Ke; Y. Li; Y. Wang; R. Mao","Hong Kong Baptist University; Nankai University; Nankai University; Guangdong Provincial Key Lab of Popular High Performance Computers, Shenzhen University; Nankai University; Nankai University",2024 IEEE International Parallel and Distributed Processing Symposium (IPDPS),"8 Jul 2024","2024","","","89","99","In Serverless Computing, function cold-start is a major issue that causes delay of the system. Various solutions have been proposed to address function cold-start issue, among which keeping containers alive after function completion is an easy and commonly adopted way in real serverless clouds. However, when reusing warm containers for function warm starts, existing systems only match functions to containers with the same configurations. This greatly limits the warm resource utilization. Our analysis of real-world applications reveals that many serverless applications share the same operating system and language frameworks. Thus, we propose multi-level container reuse that tries to reduce the startup latency of functions using ""similar"" containers to greatly improve warm resource utilization. Due to the complexity of selecting the best container reuse solutions, we designed a Deep Reinforcement Learning (DRL) based scheduler to efficiently and effectively address the problem. Moreover, we released a new serverless benchmark named FStartBench that contains detailed package information for comparing the effectiveness of different function cold-start methods. Experiments based on FStartBench show that, given a warm resource pool with fixed size, our DRL-based scheduler can achieve up to 53% reduction on the average function startup latency compared to state-of-the-art solutions.","1530-2075","979-8-3503-8711-7","10.1109/IPDPS57955.2024.00017","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10579145","serverless computing;cold-start problem;warm container reuse","Distributed processing;Operating systems;Serverless computing;Containers;Benchmark testing;Deep reinforcement learning;Complexity theory","","2","","40","IEEE","8 Jul 2024","","","IEEE","IEEE Conferences"
"Improving the Efficiency of Serverless Computing via Core-Level Power Management","D. Liu; J. Wang; X. Wang; C. Li; L. Zhang; X. Hou; X. Shi; M. Guo","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China","2024 IEEE 24th International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","8 Oct 2024","2024","","","125","135","Serverless computing has recently become a significant application paradigm in data centers. However, existing power management methods focus on optimizations at the coarse-grained server level, making them unable to handle the characteristics of these short-lived, dynamic serverless functions. In this context, the unawareness of function-level characteristics by the existing power management systems can severely degrade the energy efficiency of the data centers. To address this challenge, we design a function-level power management system. Instead of relying on server-level schedulers, we propose a novel core-level scheduling policy for serverless functions that can efficiently allocate functions to the most suitable CPU core. Additionally, we propose a power management mechanism for serverless computing that can reduce system power consumption with functions’ QoS guaranteed. Our evaluation shows that our system achieves a maximum power saving of 8.5% and an average power saving of 8% across the majority of loads without incurring any loss in tail latency, as compared to the conventional server-level scheduling system.","2993-2114","979-8-3503-9566-2","10.1109/CCGrid59990.2024.00024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10701381","serverless computing;power efficiency;power management;core-level scheduling","Data centers;Power demand;Processor scheduling;Power system management;Scalability;Serverless computing;Quality of service;Tail;Energy efficiency;Optimization","","","","45","IEEE","8 Oct 2024","","","IEEE","IEEE Conferences"
"Paldia: Enabling SLO-Compliant and Cost-Effective Serverless Computing on Heterogeneous Hardware","V. M. Bhasi; A. Sharma; S. Mohanty; M. T. Kandemir; C. R. Das",The Pennsylvania State University; The Pennsylvania State University; The Pennsylvania State University; The Pennsylvania State University; The Pennsylvania State University,2024 IEEE International Parallel and Distributed Processing Symposium (IPDPS),"8 Jul 2024","2024","","","100","113","Among the variety of applications (apps) being deployed on serverless platforms, apps such as Machine Learning (ML) inference serving can achieve better performance from leveraging accelerators like GPUs. Yet, major serverless providers, despite having GPU-equipped servers, do not offer GPU support for their serverless functions. Given that serverless functions are deployed on various generations of CPUs already, extending this to various (typically more expensive) GPU generations can offer providers a greater range of hardware to serve incoming requests according to the functions and request traffic. Here, providers are faced with the challenge of selecting hardware to reach a well-proportioned trade-off point between cost and performance. While recent works have attempted to address this, they often fail to do so as they overlook optimization opportunities arising from intelligently leveraging existing GPU sharing mechanisms. To address this point, we devise a heterogeneous serverless framework, PALDIA, which uses a prudent Hardware selection policy to acquire capable, cost-effective hardware and perform intelligent request scheduling on it to yield high performance and cost savings. Specifically, our scheduling algorithm employs hybrid spatio-temporal GPU sharing that intelligently trades off job queueing delays and interference to allow the chosen cost-effective hardware to also be highly performant. We extensively evaluate PALDIA using 16 ML inference workloads with real-world traces on a 6 node heterogeneous cluster. Our results show that PALDIA significantly outperforms state-of-the-art works in terms of Service Level Objective (SLO) compliance (up to 13.3% more) and tail latency (up to ∼50% less), with cost savings up to 86%.","1530-2075","979-8-3503-8711-7","10.1109/IPDPS57955.2024.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10579260","serverless;heterogeneous;GPU;scheduling","Distributed processing;Costs;Scheduling algorithms;Graphics processing units;Serverless computing;Tail;Machine learning","","","","89","IEEE","8 Jul 2024","","","IEEE","IEEE Conferences"
"FunctionBench: A Suite of Workloads for Serverless Cloud Function Service","J. Kim; K. Lee","College of Computer Science, Kookmin University, South Korea; College of Computer Science, Kookmin University, South Korea",2019 IEEE 12th International Conference on Cloud Computing (CLOUD),"29 Aug 2019","2019","","","502","504","Serverless computing is attracting considerable attention recently, but many published papers use micro-benchmarks for evaluation that might result in impracticality. To address this, we present FunctionBench, a suite of practical function workloads for public services. It contains realistic data-oriented applications that utilize various resources during execution. The source codes customized for various cloud service providers are publicly available. We are positive that it suggests opportunities for new function applications with lessen experiment setup overheads.","2159-6190","978-1-7281-2705-7","10.1109/CLOUD.2019.00091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8814583","workload;benchmark;FaaS;serverless;cloud","Cloud computing;FAA;Computational modeling;Random access memory;Google;Training;Predictive models","","126","","8","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"GreenWhisk: Emission-Aware Computing for Serverless Platform","J. Serenari; S. Sreekumar; K. Zhao; S. Sarkar; S. Lee",University of Pittsburgh; University of Pittsburgh; University of Pittsburgh; TryCarbonara; University of Pittsburgh,2024 IEEE International Conference on Cloud Engineering (IC2E),"14 Nov 2024","2024","","","44","54","Serverless computing is an emerging cloud computing abstraction wherein the cloud platform transparently manages all resources, including explicitly provisioning resources and geographical load balancing when the demand for service spikes. Users provide code as functions, and the cloud platform runs these functions handling all aspects of function execution. While prior work has primarily focused on optimizing performance, this paper focuses on reducing the carbon footprint of these systems making variations in grid carbon intensity and intermittency from renewables transparent to the user. We introduce GreenWhisk, a carbon-aware serverless computing platform built upon Apache OpenWhisk, operating in two modes - grid-connected and grid-isolated - addressing intermittency challenges arising from renewables and the grid’s carbon footprint. Moreover, we develop carbon-aware load balancing algorithms that leverage energy and carbon information to reduce the carbon footprint. Our evaluation results show that GreenWhisk can easily incorporate carbon-aware algorithms, thereby reducing the carbon footprint of functions without significantly impacting the performance of function execution. In doing so, our system design enables the integration of new carbon-aware strategies into a serverless computing platform.","2694-0825","979-8-3315-2869-0","10.1109/IC2E61754.2024.00012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10749780","serverless;cloud;emissions;green computing","Renewable energy sources;Green products;Software algorithms;Serverless computing;Carbon dioxide;Load management;Software systems;Carbon;Carbon footprint;System analysis and design","","","","50","IEEE","14 Nov 2024","","","IEEE","IEEE Conferences"
"Operator as a Service: Stateful Serverless Complex Event Processing","M. Luthra; S. Hennig; K. Razavi; L. Wang; B. Koldehofe","Technical University of Darmstadt, Germany; Technical University of Darmstadt, Germany; Technical University of Darmstadt, Germany; Technical University of Darmstadt, Germany; Technical University of Darmstadt, Germany",2020 IEEE International Conference on Big Data (Big Data),"19 Mar 2021","2020","","","1964","1973","Complex Event Processing (CEP) is a powerful paradigm for scalable data management that is employed in many real-world scenarios such as detecting credit card fraud in banks. The so-called complex events are expressed using a specification language that is typically implemented and executed on a specific runtime system. While the tight coupling of these two components has been regarded as the key for supporting CEP at high performance, such dependencies pose several inherent challenges as follows. (1) Application development atop a CEP system requires extensive knowledge of how the runtime system operates, which is typically highly complex in nature. (2) The specification language dependence requires the need of domain experts and further restricts and steepens the learning curve for application developers.In this paper, we propose CEPless, a scalable data management system that decouples the specification from the runtime system by building on the principles of serverless computing. CEPless provides ""operator as a service"" and offers flexibility by enabling the development of CEP application in any specification language while abstracting away the complexity of the CEP runtime system. As part of CEPless, we designed and evaluated novel mechanisms for in-memory processing and batching that enable the stateful processing of CEP operators even under high rates of ingested events. Our evaluation demonstrates that CEPless can be easily integrated into existing CEP systems like Apache Flink while attaining similar throughput under high scale of events (up to 100K events per second) and dynamic operator update in ~238 ms.","","978-1-7281-6251-5","10.1109/BigData50022.2020.9378142","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9378142","Complex Event Processing;Serverless computing;Function as a Service;Internet of Things","Runtime;Big Data;Programming;Throughput;Credit cards;Specification languages;Internet of Things","","5","","29","IEEE","19 Mar 2021","","","IEEE","IEEE Conferences"
"TrIMS: Transparent and Isolated Model Sharing for Low Latency Deep Learning Inference in Function-as-a-Service","A. Dakkak; C. Li; S. Garcia de Gonzalo; J. Xiong; W. -m. Hwu","Department of Computer Science, University of Illinois, Urbana-Champaign; Department of Computer Science, University of Illinois, Urbana-Champaign; Department of Computer Science, University of Illinois, Urbana-Champaign; IBM Thomas J. Watson Research Center, Yorktown Heights, NY; Department of Electrical and Computer Engineering, University of Illinois, Urbana-Champaign",2019 IEEE 12th International Conference on Cloud Computing (CLOUD),"29 Aug 2019","2019","","","372","382","Deep neural networks (DNNs) have become core computation components within low latency Function as a Service (FaaS) prediction pipelines. Cloud computing, as the defacto backbone of modern computing infrastructure, has to be able to handle user-defined FaaS pipelines containing diverse DNN inference workloads while maintaining isolation and latency guarantees with minimal resource waste. The current solution for guaranteeing isolation and latency within FaaS is inefficient. A major cause of the inefficiency is the need to move large amount of data within and across servers. We propose TrIMS as a novel solution to address this issue. TrIMSis a generic memory sharing technique that enables constant data to be shared across processes or containers while still maintaining isolation between users. TrIMS consists of a persistent model store across the GPU, CPU, local storage, and cloud storage hierarchy, an efficient resource management layer that provides isolation, and a succinct set of abstracts, applicationAPIs, and container technologies for easy and transparent integration with FaaS, Deep Learning (DL) frameworks, and user code. We demonstrate our solution by interfacing TrIMS with the Apache MXNet framework and demonstrate up to 24x speedup in latency for image classification models, up to 210x speedup for large models, and up to8×system throughput improvement.","2159-6190","978-1-7281-2705-7","10.1109/CLOUD.2019.00067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8814494","Machine Learning;Inference;cloud;memory","","","12","","39","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"BeFaaS: An Application-Centric Benchmarking Framework for FaaS Platforms","M. Grambow; T. Pfandzelter; L. Burchard; C. Schubert; M. Zhao; D. Bermbach","TU Berlin & Einstein Center Digital Future Mobile Cloud Computing Research Group, Berlin, Germany; TU Berlin & Einstein Center Digital Future Mobile Cloud Computing Research Group, Berlin, Germany; TU Berlin & Einstein Center Digital Future Mobile Cloud Computing Research Group, Berlin, Germany; TU Berlin & Einstein Center Digital Future Mobile Cloud Computing Research Group, Berlin, Germany; TU Berlin & Einstein Center Digital Future Mobile Cloud Computing Research Group, Berlin, Germany; TU Berlin & Einstein Center Digital Future Mobile Cloud Computing Research Group, Berlin, Germany",2021 IEEE International Conference on Cloud Engineering (IC2E),"22 Nov 2021","2021","","","1","8","Following the increasing interest and adoption of FaaS systems, benchmarking frameworks for determining nonfunctional properties have also emerged. While existing (microbenchmark) frameworks only evaluate single aspects of FaaS platforms, a more holistic, application-driven approach is still missing. In this paper, we design and present BeFaaS, an extensible application-centric benchmarking framework for FaaS environments that focuses on the evaluation of FaaS platforms through realistic and typical examples of FaaS applications. BeFaaS includes a built-in e-commerce benchmark, is extensible for new workload profiles and new platforms, supports federated benchmark runs in which the benchmark application is distributed over multiple providers, and supports a fine-grained result analysis. Our evaluation compares three major FaaS providers in single cloud provider setups and shows that BeFaaS is capable of running each benchmark automatically with minimal configuration effort and providing detailed insights for each interaction.","","978-1-6654-4970-0","10.1109/IC2E52221.2021.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9610428","FaaS;Benchmarking;Fog Computing;Infrastructure Automation","Automation;Conferences;FAA;Benchmark testing;Tools","","28","","33","IEEE","22 Nov 2021","","","IEEE","IEEE Conferences"
"Algorithms for scheduling scientific workflows on serverless architecture","M. Majewski; M. Pawlik; M. Malawski","Institute of Computer Science, AGH Univ. of Science and Technology, Krakow, Poland; Institute of Computer Science, AGH Univ. of Science and Technology, Krakow, Poland; Institute of Computer Science, AGH Univ. of Science and Technology, Krakow, Poland","2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","2 Aug 2021","2021","","","782","789","Serverless computing is a novel cloud computing paradigm where the cloud provider manages the underlying infrastructure, while users are only required to upload the code of the application. Function as a Service (FaaS) is a serverless computing model where short-lived methods are executed in the cloud. One of the promising use cases for FaaS is running scientific workflow applications, which represent a scientific process composed of related tasks. Due to the distinctive features of FaaS, which include rapid resource provisioning, indirect infrastructure management, and fine-grained billing model a need arises to create dedicated scheduling methods to effectively use the novel infrastructures as an environment for workflow applications. In this paper we propose two novel scheduling algorithms SMOHEFT and SML, which are designed to create a schedule for executing scientific workflows on serverless infrastructures concerning time and cost constraints. We evaluated proposed algorithms by performing experiments, where we planned the execution of three applications: Ellipsoids, Vina and Montage. SDBWS and SDBCS algorithms were used as a baseline. SML achieved the best results when executing Ellipsoids workflow, with a success rate above 80%, while other algorithms were below 60%. In the case of Vina, all the algorithms, except SDBWS, had a success rate above 87.5% and in the case of Montage, the success rate of all algorithms was similar, over 87.5%. The proposed algorithms' success rate is comparable or better than offered by other studied solutions.","","978-1-7281-9586-5","10.1109/CCGrid51090.2021.00095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9499676","Serverless;Cloud functions;Algorithm;Scheduling;Workflow","Cloud computing;Schedules;Scheduling algorithms;Computational modeling;Clustering algorithms;FAA;Scheduling","","9","","17","IEEE","2 Aug 2021","","","IEEE","IEEE Conferences"
"PrivFlow: Secure and Privacy Preserving Serverless Workflows on Cloud","S. Garg; M. S. Dilip Thakur; R. M. A; L. P. Maddali; V. Ramachandran","TCS Innovation Labs, Tata Consultancy Services, New Delhi, INDIA; TCS Innovation Labs, Tata Consultancy Services, Bengaluru, INDIA; TCS Innovation Labs, Tata Consultancy Services, Bengaluru, INDIA; TCS Innovation Labs, Tata Consultancy Services, Hyderabad, INDIA; TCS Innovation Labs, Tata Consultancy Services, Chennai, INDIA","2023 IEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","10 Jul 2023","2023","","","447","458","The recent advancement of serverless computing in the widespread deployment of applications prompts the need to protect serverless workflows against cloud vulnerabilities and threats. We propose PrivFlow, a workflow-centric, privacy preserving framework to protect the information flow in serverless computing applications in semi-honest (S-PrivFlow) and malicious (M-PrivFlow) adversarial settings. An Authenticated Data Structure is used to store the valid workflows encoded in the proposed format. The validation of workflows is performed in a privacy preserving manner that leaks no sensitive information to any unauthorized user. We focus on the two most prevalent attacks on the serverless cloud platforms, namely the Denial-of-Wallet and Wrong Function Invocation attacks. We demonstrate that PrivFlow mitigates both of these attacks. Further, we evaluate PrivFlow on the popular benchmark application- Hello Retail, and a customized scaled application. Though the comparison with the state-of-the-art approaches in terms of the runtime performance shows a latency of 1.6 times for S-PrivFlow and 8 times for M-PrivFlow, the PrivFlow provides high security and privacy. PrivFlow acts as a wrapper to the application resulting in no change to the source code.","","979-8-3503-0119-9","10.1109/CCGrid57682.2023.00049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10171483","Serverless Cloud Security;Serverless Workflows;Privacy Preserving;Authenticated Data Structure;Consensus","Privacy;Data privacy;Runtime;Source coding;Serverless computing;Benchmark testing;Data structures","","2","","51","IEEE","10 Jul 2023","","","IEEE","IEEE Conferences"
"Evaluation of Production Serverless Computing Environments","H. Lee; K. Satyam; G. Fox","School of Informatics, Computing and Engineering Indiana University, Bloomington; School of Informatics, Computing and Engineering Indiana University, Bloomington; School of Informatics, Computing and Engineering Indiana University, Bloomington",2018 IEEE 11th International Conference on Cloud Computing (CLOUD),"11 Sep 2018","2018","","","442","450","Serverless computing provides a small runtime container to execute lines of codes without infrastructure management which is similar to Platform as a Service (PaaS) but a functional level. Amazon started the event-driven compute named Lambda functions in 2014 with a 25 concurrent limitation, but it now supports at least a thousand of concurrent invocation to process event messages generated by resources like databases, storage and system logs. Other providers, i.e., Google, Microsoft, and IBM offer a dynamic scaling manager to handle parallel requests of stateless functions in which additional containers are provisioning on new compute nodes for distribution. However, while functions are often developed for microservices and lightweight workload, they are associated with distributed data processing using the concurrent invocations. We claim that the current serverless computing environments can support dynamic applications in parallel when a partitioned task is executable on a small function instance. We present results of throughput, network bandwidth, a file I/O and compute performance regarding the concurrent invocations. We deployed a series of functions for distributed data processing to address the elasticity and then demonstrated the differences between serverless computing and virtual machines for cost efficiency and resource utilization.","2159-6190","978-1-5386-7235-8","10.1109/CLOUD.2018.00062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8457830","FaaS, Serverless, Event-driven Computing, Amazon Lambda, Google Functions, Microsoft Azure Functions, IBM OpenWhisk","Google;Throughput;Cloud computing;Databases;Containers;Runtime;Data processing","","106","","18","IEEE","11 Sep 2018","","","IEEE","IEEE Conferences"
"DGSF: Disaggregated GPUs for Serverless Functions","H. Fingler; Z. Zhu; E. Yoon; Z. Jia; E. Witchel; C. J. Rossbach","Department of Computer Science, The University of Texas at Austin; Department of Computer Science, The University of Texas at Austin; Department of Computer Science, The University of Texas at Austin; Department of Computer Science, The University of Texas at Austin; Department of Computer Science, The University of Texas at Austin; Department of Computer Science, The University of Texas at Austin",2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS),"15 Jul 2022","2022","","","739","750","Ease of use and transparent access to elastic resources have attracted many applications away from traditional platforms toward serverless functions. Many of these applications, such as machine learning, could benefit significantly from GPU acceleration. Unfortunately, GPUs remain inaccessible from serverless functions in modern production settings. We present DGSF, a platform that transparently enables serverless functions to use GPUs through general purpose APIs such as CUDA. DGSF solves provisioning and utilization challenges with disaggregation, serving the needs of a potentially large number of functions through virtual GPUs backed by a small pool of physical GPUs on dedicated servers. Disaggregation allows the provider to decouple GPU provisioning from other resources, and enables significant benefits through consolidation. We describe how DGSF solves GPU disaggregation challenges including supporting API transparency, hiding the latency of communication with remote GPUs, and load-balancing access to heavily shared GPUs. Evaluation of our prototype on six workloads shows that DGSF's API remoting optimizations can improve the runtime of a function by up to 50% relative to unoptimized DGSF. Such optimizations, which aggressively remove GPU runtime and object management latency from the critical path, can enable functions running over DGSF to have a lower end-to-end time than when running on a GPU natively. By enabling GPU sharing, DGSF can reduce function queueing latency by up to 53%. We use DGSF to augment AWS Lambda with GPU support, showing similar benefits.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00077","NSF(grant numbers:CNS-1846169,CNS-2006943,CNS-2008321,CNS-1900457); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820659","cloud computing;serverless;FaaS;GPU;API remoting","Distributed processing;Cloud computing;Runtime;Graphics processing units;Prototypes;Production;Machine learning","","8","","62","IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"AIBLOCK: Blockchain based Lightweight Framework for Serverless Computing using AI","M. Golec; D. Chowdhury; S. Jaglan; S. S. Gill; S. Uhlig","School of Electronic Engineering and Computer Science, Queen Mary University of London, United Kingdom; Department of Electronics and communication engineering, International Institute of Information Technology, Naya Raipur, India; Department of Electrical Engineering, Indian Institute of Technology (IIT), Delhi, India; School of Electronic Engineering and Computer Science, Queen Mary University of London, United Kingdom; School of Electronic Engineering and Computer Science, Queen Mary University of London, United Kingdom","2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","19 Jul 2022","2022","","","886","892","Artificial intelligence (AI)-based studies have been carried out recently for the early detection of COVID-19. The goal is to prevent the spread of the disease and the number of fatal cases. In AI-based COVID-19 diagnostic studies, the integrity of the data is critical to obtain reliable results. In this paper, we propose a Blockchain-based framework called AIBLOCK, to offer the data integrity required for applications such as Industry 4.0, healthcare, and online banking. In addition, the proposed framework is integrated with Google Cloud Platform (GCP)-Cloud Functions, a serverless computing platform that automatically manages resources by offering dynamic scalability. The performance of five different machine learning models is evaluated and compared in terms of Accuracy, Precision, Recall, F-Score and Area under the curve (AUC). The experimental results show that decision trees gives the best results in terms of accuracy (98.4 %). Further, it has been identified that utilization of Blockchain technology can increase the load on memory.","","978-1-6654-9956-9","10.1109/CCGrid54584.2022.00106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9826025","Blockchain;Artificial Intelligence;Serverless Computing;Data Integrity","COVID-19;Online banking;Scalability;Serverless computing;Medical services;Machine learning;Blockchains","","14","","28","IEEE","19 Jul 2022","","","IEEE","IEEE Conferences"
"Asset Modeling using Serverless Computing","S. Jayaraman; C. Reddy; E. Khabiri; D. Patel; A. Bhamidipaty; J. Kalagnanam","IBM Research, Yorktown Heights, USA; IBM Research, Yorktown Heights, USA; IBM Research, Yorktown Heights, USA; IBM Research, Yorktown Heights, USA; IBM Research, Yorktown Heights, USA; IBM Research, Yorktown Heights, USA",2021 IEEE International Conference on Big Data (Big Data),"13 Jan 2022","2021","","","4084","4090","Assets in the domain of Internet of Things (IoT) generate time-series data such as sensor readings and alerts. In addition, the assets have associated static data such as the make, model and other manufacturing information. The sensors in the asset components may have implicit relationships with each other, which are not interpretable without domain knowledge. Many problems exist which involve computation of relationships between sensors or subsystems in the asset components. Typically, the number of sensors in a real world asset may range anywhere from tens to thousands of sensors - and in this case, finding relationships between them becomes a highly computationally intensive task. In this paper, we study one such problem of anomaly detection in industrial data based on the functioning of the sensors and their interrelationships in both normal and abnormal conditions. We further demonstrate the issue of run-time and performance complexity in this problem, and present a speed-up strategy using Serverless Computing for parallelization, and demonstrate the usefulness of this method by comparing the speed-up achieved.","","978-1-6654-3902-2","10.1109/BigData52589.2021.9671711","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9671711","Internet of Things;Serverless;Anomaly detection;Natural Language Processing","Costs;Computational modeling;Serverless computing;Big Data;Maintenance engineering;Data models;Natural language processing","","1","","37","IEEE","13 Jan 2022","","","IEEE","IEEE Conferences"
"SlackAuto: Slack-Aware Vertical Autoscaling of CPU Resources for Serverless Computing","H. -J. Jang; H. -W. Jin","Department of Computer Science and Engineering, Konkuk University, Seoul, South Korea; Department of Computer Science and Engineering, Konkuk University, Seoul, South Korea",2024 IEEE 17th International Conference on Cloud Computing (CLOUD),"28 Aug 2024","2024","","","355","364","In modern serverless platforms, vertical auto scaling becomes increasingly important as a single service instance handles multiple requests concurrently. Vertical autoscaling typically relies on a prediction mechanism to proactively adjust resource allocation based on estimated future resource demands. However, inaccurate predictions result in a waste of resources or poor performance. To address this issue, we introduce slack-awareness into existing prediction/autoscaling mechanisms. Our proposed mechanism, named SlackAuto, models resource slack as a queue and controls its length to improve both CPU efficiency and application-level performance. We utilize the drift-plus-penalty algorithm of the Lyapunov optimization technique, which does not add significant run-time overheads and provides an efficient way to integrate slack-awareness with existing mechanisms. Our implementation operates in both periodic and event-driven modes, enabling an immediate response to changes in demand. Performance measurement results show that SlackAuto outper-forms existing vertical autoscaling algorithms in terms of resource efficiency and response latency.","2159-6190","979-8-3503-6853-6","10.1109/CLOUD62652.2024.00047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10643948","Slack;Vertical Autoscaling;Serverless Computing;Lyapunov Optimization","Measurement;Serverless computing;Prediction algorithms;Optimization","","","","27","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"Tracing Function Dependencies across Clouds","W. -T. Lin; C. Krintz; R. Wolski","Dept. of Computer Science, UC Santa Barbara; Dept. of Computer Science, UC Santa Barbara; Dept. of Computer Science, UC Santa Barbara",2018 IEEE 11th International Conference on Cloud Computing (CLOUD),"11 Sep 2018","2018","","","253","260","In this paper, we present Lowgo, a crosscloud tracing tool for capturing causal relationships in serverless applications. To do so, Lowgo records dependencies between functions, through cloud services, and across regions to facilitate debugging and reasoning about highly concurrent, multi-cloud applications. We empirically evaluate Lowgo using microbenchmarks and multi-function and multi-cloud applications. We find that Lowgo is able to capture causal dependencies with overhead that ranges from 2-12%, which is less than half that of the best-performing, cloud-specific approach.","2159-6190","978-1-5386-7235-8","10.1109/CLOUD.2018.00039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8457807","cloud computing;serverless computing;function as a service;faas;AWS Lambda;Azure Functions","Cloud computing;Tools;Pipelines;Servers;Google;Debugging;Computational modeling","","11","","25","IEEE","11 Sep 2018","","","IEEE","IEEE Conferences"
"Are Unikernels Ready for Serverless on the Edge?","F. Moebius; T. Pfandzelter; D. Bermbach","Scalable Software Systems Research Group, Technische Universität Berlin & Einstein Center Digital Future; Scalable Software Systems Research Group, Technische Universität Berlin & Einstein Center Digital Future; Scalable Software Systems Research Group, Technische Universität Berlin & Einstein Center Digital Future",2024 IEEE International Conference on Cloud Engineering (IC2E),"14 Nov 2024","2024","","","133","143","Function-as-a-Service (FaaS) is a promising edge computing execution model but requires secure sandboxing mechanisms to isolate workloads from multiple tenants on constrained infrastructure. Although Docker containers are lightweight and popular in open-source FaaS platforms, they are generally considered insufficient for executing untrusted code and providing sandbox isolation. Commercial cloud FaaS platforms thus rely on Linux microVMs or hardened container runtimes, which are secure but come with a higher resource footprint.Unikernels combine application code and limited operating system primitives into a single purpose appliance, reducing the footprint of an application and its sandbox while providing full Linux compatibility. In this paper, we study the suitability of unikernels as an edge FaaS execution environment using the Nanos and OSv unikernel tool chains. We compare performance along several metrics such as cold start overhead and idle footprint against sandboxes such as Firecracker Linux microVMs, Docker containers, and secure gVisor containers. We find that unikernels exhibit desirable cold start performance, yet lag behind Linux microVMs in stability. Nevertheless, we show that unikernels are a promising candidate for further research on Linux-compatible FaaS isolation.","2694-0825","979-8-3315-2869-0","10.1109/IC2E61754.2024.00022","Bundesministerium für Bildung und Forschung; Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10749739","serverless;edge computing;unikernels","Measurement;Codes;Runtime;Linux;Operating systems;Memory management;Containers;Programming;Stability analysis;Reliability","","3","","65","IEEE","14 Nov 2024","","","IEEE","IEEE Conferences"
"A Reinforcement Learning Approach to Reduce Serverless Function Cold Start Frequency","S. Agarwal; M. A. Rodriguez; R. Buyya","Cloud Computing and Distributed Systems(CLOUDS) Laboratory School of Computing and Information Systems, The University of Melbourne, Australia; Cloud Computing and Distributed Systems(CLOUDS) Laboratory School of Computing and Information Systems, The University of Melbourne, Australia; Cloud Computing and Distributed Systems(CLOUDS) Laboratory School of Computing and Information Systems, The University of Melbourne, Australia","2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","2 Aug 2021","2021","","","797","803","Serverless computing is an event-driven cloud computing architecture for processing requests on-demand, using light weight function containers and a micro-services model. A variety of applications like Internet of Things (IoT) services, edge computing, and stream processing have been introduced to the serverless paradigm. These applications are characterized by their stringent response time requirements, therefore expecting a quick and fault tolerant feedback from the application. The serverless, or Function-as-a-Service (FaaS), paradigm suffers from function ‘cold start’ challenges, where the serverless platform takes time to set up the dependencies, prepare the runtime environment and code for execution before serving the incoming workload. Most of the current works address the problem of cold start by (1) reducing the start-up or preparation time of function containers, or (2) reducing the frequency of function cold starts on the platform. Recent industrial research has identified that factors such as runtime environment, CPU and memory settings, invocation concurrency, and networking requirements, affect the cold start of a function. Therefore, we propose a Reinforcement Learning (Q-Learning) agent setting, to analyze the identified factors such as function CPU utilization, to ascertain the function-invocation patterns and reduce the function cold start frequency by preparing the function instances in advance. The proposed Q-Learning agent interacts with the Kubeless serverless platform by discretizing the environment states, actions and rewards with the use of per-instance CPU utilization, available function instances and success or failure rate of response, respectively. The workload is replicated using the Apache JMeter non-GUI toolkit and our agent is evaluated against the baseline default auto-scale feature of Kubeless. The agent demonstrates the capability of learning the invocation pattern, make informed decisions by preparing the optimal number of function instances over the period of learning, under controlled environment settings.","","978-1-7281-9586-5","10.1109/CCGrid51090.2021.00097","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9499423","Serverless Computing;Faas;Reinforcement Learning;Q-Learning;Cold Start;Kubeless","Runtime environment;Cloud computing;Time-frequency analysis;Fault tolerance;Memory management;Fault tolerant systems;Reinforcement learning","","42","","17","IEEE","2 Aug 2021","","","IEEE","IEEE Conferences"
"Object as a Service (OaaS): Enabling Object Abstraction in Serverless Clouds","P. Lertpongrujikorn; M. A. Salehi","High Performance Cloud Computing (HPCC) Lab, School of Computing and Informatics, University of Louisiana at Lafayette; High Performance Cloud Computing (HPCC) Lab, School of Computing and Informatics, University of Louisiana at Lafayette",2023 IEEE 16th International Conference on Cloud Computing (CLOUD),"25 Sep 2023","2023","","","238","248","Function as a Service (FaaS) paradigm is becoming widespread and is envisioned as the next generation of cloud computing systems that mitigate the burden for programmers and cloud solution architects. However, the FaaS abstraction only makes the cloud resource management aspects transparent but does not deal with the application data aspects. As such, developers have to intervene and undergo the burden of managing the application data, often via separate cloud services (e.g., AWS S3). Similarly, the FaaS abstraction does not natively support function workflow, hence, the developers often have to work with workflow orchestration services (e.g., AWS Step Functions) to build workflows. Moreover, they have to explicitly navigate the data throughout the workflow. To overcome these inherent problems of FaaS, our hypothesis is to design a higher-level cloud programming abstraction that can hide the complexities and mitigate the burden of developing cloud-native application development. Accordingly, in this research, we borrow the notion of object from object-oriented programming and propose a new abstraction level atop the function abstraction, known as Object as a Service (OaaS). OaaS encapsulates the application data and function into the object abstraction and relieves the developers from resource and data management burdens. It also unlocks opportunities for built-in optimization features, such as software reusability, data locality, and caching. OaaS natively supports dataflow programming such that developers define a workflow of functions transparently without getting involved in data navigation, synchronization, and parallelism aspects. We implemented a prototype of the OaaS platform and evaluated it under real-world settings against state-of-the-art platforms regarding the imposed overhead, scalability, and ease of use. The results demonstrate that OaaS streamlines cloud programming and offers scalability with an insignificant overhead to the underlying cloud system.","2159-6190","979-8-3503-0481-7","10.1109/CLOUD60044.2023.00035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10254994","FaaS;Serverless paradigm;Cloud computing;Cloud-native programming;Abstraction","Cloud computing;Navigation;Processor scheduling;Scalability;Prototypes;Synchronization;Resource management","","6","","46","IEEE","25 Sep 2023","","","IEEE","IEEE Conferences"
"Fusionize: Improving Serverless Application Performance through Feedback-Driven Function Fusion","T. Schirmer; J. Scheuner; T. Pfandzelter; D. Bermbach","TU Berlin & ECDF, Mobile Cloud Computing Research Group; Chalmers, University of Gothenburg; TU Berlin & ECDF, Mobile Cloud Computing Research Group; TU Berlin & ECDF, Mobile Cloud Computing Research Group",2022 IEEE International Conference on Cloud Engineering (IC2E),"16 Nov 2022","2022","","","85","95","Serverless computing increases developer productivity by removing operational concerns such as managing hardware or software runtimes. Developers, however, still need to partition their application into functions, which can be error-prone and adds complexity: Using a small function size where only the smallest logical unit of an application is inside a function maximizes flexibility and reusability. Yet, having small functions leads to invocation overheads, additional cold starts, and may increase cost due to double billing during synchronous invocations. In this paper we present Fusionize, a framework that removes these concerns from developers by automatically fusing the application code into a multi-function orchestration with varying function size. Developers only need to write the application code following a lightweight programming model and do not need to worry how the application is turned into functions. Our framework automatically fuses different parts of the application into functions and manages their interactions. Leveraging monitoring data, the framework optimizes the distribution of application parts to functions to optimize deployment goals such as end-to-end latency and cost. Using two example applications, we show that Fusionizecan automatically and iteratively improve the deployment artifacts of the application.","","978-1-6654-9115-0","10.1109/IC2E55432.2022.00017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9946241","serverless computing;FaaS;function fusion;cloud orchestration","Costs;Runtime;Codes;Fuses;Prototypes;Serverless computing;Programming","","8","","29","IEEE","16 Nov 2022","","","IEEE","IEEE Conferences"
"Spock: Exploiting Serverless Functions for SLO and Cost Aware Resource Procurement in Public Cloud","J. R. Gunasekaran; P. Thinakaran; M. T. Kandemir; B. Urgaonkar; G. Kesidis; C. Das","Computer Science and Engineering, Pennsylvania State University, University Park, PA; Computer Science and Engineering, Pennsylvania State University, University Park, PA; Computer Science and Engineering, Pennsylvania State University, University Park, PA; Computer Science and Engineering, Pennsylvania State University, University Park, PA; Computer Science and Engineering, Pennsylvania State University, University Park, PA; Computer Science and Engineering, Pennsylvania State University, University Park, PA",2019 IEEE 12th International Conference on Cloud Computing (CLOUD),"29 Aug 2019","2019","","","199","208","We are witnessing the emergence of elastic web services which are hosted in public cloud infrastructures. For reasons of cost-effectiveness, it is crucial for the elasticity of these web services to match the dynamically-evolving user demand. Traditional approaches employ clusters of virtual machines (VMs) to dynamically scale resources based on application demand. However, they still face challenges such as higher cost due to over-provisioning or incur service level objective (SLO) violations due to under-provisioning. Motivated by this observation, we propose Spock, a new scalable and elastic control system that exploits both VMs and serverless functions to reduce cost and ensure SLO for elastic web services. We show that under two different scaling policies, Spock reduces SLO violations of queries by up to 74% when compared to VM-based resource procurement schemes. Further, Spock yields significant cost savings, by up to 33% compared to traditional approaches which use only VMs.","2159-6190","978-1-7281-2705-7","10.1109/CLOUD.2019.00043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8814535","FaaS;serverless;autoscaling;cost-aware;SLO;lambda","","","49","","64","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"An Evaluation of Serverless Computing on X86 and ARM platforms: Performance and Design Implications","D. Xie; Y. Hu; L. Qin","Center for Information Research, Academy of Military Sciences, Beijing, China; Center for Information Research, Academy of Military Sciences, Beijing, China; Center for Information Research, Academy of Military Sciences, Beijing, China",2021 IEEE 14th International Conference on Cloud Computing (CLOUD),"8 Nov 2021","2021","","","313","321","Serverless computing is an emerging computing paradigm in cloud computing. Serverless users benefit from the features of serverless computing (e.g., auto scaling and pay-as-you-go model), and widely adopt it. Besides, ARM servers are becoming increasingly popular nowadays. It is of great importance to explore the performance of serverless computing on ARM platforms, as well as X86 platforms. This paper provides in-depth study of serverless computing on X86 and ARM platforms. We evaluate typical characteristics (e.g., startup latency, auto scaling, and performance isolation) of popular open source serverless platforms: Knative and Open-FaaS. We gain some valuable insights in different platforms. We believe our findings can guide serverless providers to design more efficient and robust serverless systems, thereby improving user experience.","2159-6190","978-1-6654-0060-2","10.1109/CLOUD53861.2021.00045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9582249","Serverless Computing;X86 and ARM;Evaluation;Performance","Program processors;Conferences;Computational modeling;User experience;Servers","","8","","31","IEEE","8 Nov 2021","","","IEEE","IEEE Conferences"
"GeoFF: Federated Serverless Workflows with Data Pre-Fetching","V. Carl; T. Schirmer; T. Pfandzelter; D. Bermbach","Scalable Software Systems Research Group, Technische Universität Berlin & Einstein Center Digital Future; Scalable Software Systems Research Group, Technische Universität Berlin & Einstein Center Digital Future; Scalable Software Systems Research Group, Technische Universität Berlin & Einstein Center Digital Future; Scalable Software Systems Research Group, Technische Universität Berlin & Einstein Center Digital Future",2024 IEEE International Conference on Cloud Engineering (IC2E),"14 Nov 2024","2024","","","144","151","Function-as-a-Service (FaaS) is a popular cloud computing model in which applications are implemented as workflows of multiple independent functions. While cloud providers usually offer composition services for such workflows, they do not support cross-platform workflows forcing developers to hardcode the composition logic. Furthermore, FaaS workflows tend to be slow due to cascading cold starts, inter-function latency, and data download latency on the critical path. In this paper, we propose GEOFF, a serverless choreography middleware that executes FaaS workflows across different public and private FaaS platforms, including ad-hoc workflow recomposition. Furthermore, GEOFF supports function pre-warming and data pre-fetching. This minimizes end-to-end workflow latency by taking cold starts and data download latency off the critical path. In experiments with our proof-of-concept prototype and a realistic application, we were able to reduce end-to-end latency by more than 50%.","2694-0825","979-8-3315-2869-0","10.1109/IC2E61754.2024.00023","Deutsche Forschungsgemeinschaft; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10749787","serverless;FaaS;edge computing;cloud choreography","Cloud computing;Computational modeling;Prototypes;Data models;Logic;Middleware","","","","37","IEEE","14 Nov 2024","","","IEEE","IEEE Conferences"
"Serverless Elastic Exploration of Unbalanced Algorithms","G. París; P. García-López; M. Sánchez-Artigas","Universitat Rovira i Virgili, Tarragona, Spain; IBM Watson Research, Yorktown Heights, NY, USA; Universitat Rovira i Virgili, Tarragona, Spain",2020 IEEE 13th International Conference on Cloud Computing (CLOUD),"18 Dec 2020","2020","","","149","157","In recent years, serverless computing and, in particular the Function-as-a-Service (Faas) execution model, has proven to be efficient for running parallel computing tasks. However, little attention has been paid to highly-parallel applications with unbalanced and irregular workloads. The main challenge of executing this type of algorithms in the cloud is the difficulty to account for the computing requirements beforehand. This places a burden on scientific users who very often make bad decisions by either overprovisioning resources or inadvertently limiting the parallelism of these algorithms due to resource contention. Our hypothesis is that the elasticity and ease of management of serverless computing can help users avoid such decisions, which may lead to undesirable cost-performance consequences for unbalanced problem spaces. In this work, we show that with a simple serverless executor pool abstraction one can achieve a better cost-performance trade-off than a Spark cluster of static size and large EC2 VMs. To support this conclusion, we evaluate two unbalanced algorithms: the Unbalanced Tree Search (UTS) and the Mandelbrot Set using the Mariani-Silver algorithm. For instance, our serverless implementation of UTS is able to outperform Spark by up to 55% with the same cost. This provides the first concrete evidence that highly-parallel, irregular workloads can be efficiently executed using purely stateless functions with almost zero burden on users - i.e., no need for users to understand non-obvious system-level parameters and optimizations.","2159-6190","978-1-7281-8780-8","10.1109/CLOUD49709.2020.00033","EU Horizon 2020 programme(grant numbers:825184); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284321","Serverless computing;FaaS;elasticity;UTS;Mariani-Silver","Concurrent computing;Clustering algorithms;FAA;Parallel processing;Elasticity;Sparks;Task analysis","","3","","21","IEEE","18 Dec 2020","","","IEEE","IEEE Conferences"
"SLAM: SLO-Aware Memory Optimization for Serverless Applications","G. Safaryan; A. Jindal; M. Chadha; M. Gerndt","Chair of Computer Architecture and Parallel Systems, Technische Universität München, Germany; Chair of Computer Architecture and Parallel Systems, Technische Universität München, Germany; Chair of Computer Architecture and Parallel Systems, Technische Universität München, Germany; Chair of Computer Architecture and Parallel Systems, Technische Universität München, Germany",2022 IEEE 15th International Conference on Cloud Computing (CLOUD),"24 Aug 2022","2022","","","30","39","Serverless computing paradigm has become more ingrained into the industry, as it offers a cheap alternative for application development and deployment. This new paradigm has also created new kinds of problems for the developer, who needs to tune memory configurations for balancing cost and performance. Many researchers have addressed the issue of minimizing cost and meeting Service Level Objective (SLO) requirements for a single FaaS function, but there has been a gap for solving the same problem for an application consisting of many FaaS functions, creating complex application workflows.In this work, we designed a tool called SLAM to address the issue. SLAM uses distributed tracing to detect the relationship among the FaaS functions within a serverless application. By modeling each of them, it estimates the execution time for the application at different memory configurations. Using these estimations, SLAM determines the optimal memory configuration for the given serverless application based on the specified SLO requirements and user-specified objectives (minimum cost or minimum execution time). We demonstrate the functionality of SLAM on AWS Lambda by testing on four applications. Our results show that the suggested memory configurations guarantee that more than 95% of requests are completed within the predefined SLOs.","2159-6190","978-1-6654-8137-3","10.1109/CLOUD55607.2022.00019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9860980","serverless;cost optimization;memory optimization;SLO","Industries;Simultaneous localization and mapping;Costs;Memory management;Serverless computing;Estimation;FAA","","16","","38","IEEE","24 Aug 2022","","","IEEE","IEEE Conferences"
"Optimizing Goodput of Real-time Serverless Functions using Dynamic Slicing with vGPUs","C. Prakash; A. Garg; U. Bellur; P. Kulkarni; U. Kurkure; H. Sivaraman; L. Vu","Indian Institute of Technology, Bombay; Indian Institute of Technology, Bombay; Indian Institute of Technology, Bombay; Indian Institute of Technology, Bombay; VMware; VMware; VMware",2021 IEEE International Conference on Cloud Engineering (IC2E),"22 Nov 2021","2021","","","60","70","As the popularity and relevance of the Function-as-a-Service (FaaS) model keeps growing, we believe newer avatars of the service will support computationally intensive SIMT functions that will execute on GPUs. With hardware-assisted virtualization of GPUs now possible, cloud offerings including GPUs usually bind a virtual GPU (vGPU) to a VM. While there is a choice of scheduling algorithms to multiplex vGPUs on to the physical GPU, the work-conserving best-effort scheduler helps to maintain a high level of utilization of the GPU. With this, we observe that the total share of the GPU per VM is non-deterministic and depends on how different VMs load the GPU via their vGPUs. As a result, any function-to-vGPU scheduler that does not explicitly account for this nondeterministic vGPU capacity will suffer from lower than optimal goodput - particularly when these functions are deadline bound as is the case with FaaS offerings today. In this work, we exploit a software based task slicing technique to dynamically determine task sizes for scheduling on vGPUs to maximize successful completion of functions within their deadlines. Our solution extends the conventional earliest-deadline first (EDF) scheduling algorithm by balancing scheduling opportunities (via kernel slicing) and maximizing the chances of functions finishing before their deadline. The work is motivated by the fact that static decisions that consider entire tasks as scheduling units or use a fixed, statically decided slice size as scheduling units cannot adapt to the non-deterministic vGPU capacity. A comparison of our solution with well-known deadline aware scheduling approach (earliest deadline first), yielded an improvement of up to 2.9x in goodput.","","978-1-6654-4970-0","10.1109/IC2E52221.2021.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9610338","GPU virtualization;FaaS;task scheduling","Multiplexing;Cloud computing;Scheduling algorithms;Graphics processing units;FAA;Dynamic scheduling;Software","","3","","45","IEEE","22 Nov 2021","","","IEEE","IEEE Conferences"
"XFaaS: Cross-platform Orchestration of FaaS Workflows on Hybrid Clouds","A. Khochare; T. Khare; V. Kulkarni; Y. Simmhan","Department of Computational and Data Sciences, Indian Institute of Science, Bangalore, INDIA; Department of Computational and Data Sciences, Indian Institute of Science, Bangalore, INDIA; Department of Computational and Data Sciences, Indian Institute of Science, Bangalore, INDIA; Department of Computational and Data Sciences, Indian Institute of Science, Bangalore, INDIA","2023 IEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","10 Jul 2023","2023","","","498","512","Functions as a Service (FaaS) have gained popularity for programming public clouds due to their simple abstraction, ease of deployment, effortless scaling and granular billing. Cloud providers also offer basic capabilities to compose these functions into workflows. FaaS and FaaS workflow models, however, are proprietary to each cloud provider. This prevents their portability across cloud providers, and requires effort to design workflows that run on different cloud providers or data centers. Such requirements are increasingly important to meet regulatory requirements, leverage cost arbitrage and avoid vendor lock-in. Further, the FaaS execution models are also different, and the overheads of FaaS workflows due to message indirection and cold-starts need custom optimizations for different platforms. In this paper, we propose XFaaS, a cross-platform deployment and orchestration engine for FaaS workflows to operate on multiple clouds. XFaaS allows “zero touch” deployment of functions and workflows across AWS and Azure clouds by automatically generating the necessary code wrappers, cloud queues, and coordinating with the native FaaS engine of the cloud providers. It also uses intelligent function fusion and placement logic to reduce the workflow execution latency in a hybrid cloud while mitigating costs, using performance and billing models specific to the providers based in detailed benchmarks. Our empirical results indicate that fusion offers up to ≈75 % benefits in latency and ≈57% reduction in cost, while placement strategies reduce the latency by ≈ 24%, compared to baselines in the best cases.","","979-8-3503-0119-9","10.1109/CCGrid57682.2023.00053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10171551","Serverless;FaaS;Workflows;Multi-Cloud","Cloud computing;Data centers;Costs;Codes;NoSQL databases;Programming;Packaging","","8","","45","IEEE","10 Jul 2023","","","IEEE","IEEE Conferences"
"Tiny Autoscalers for Tiny Workloads: Dynamic CPU Allocation for Serverless Functions","Y. Zhao; A. Uta","LIACS, Leiden University; LIACS, Leiden University","2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","19 Jul 2022","2022","","","170","179","In serverless computing, applications are executed under lightweight virtualization and isolation environments, such as containers or micro virtual machines. Typically, their memory allocation is set by the user before deployment. All other resources, such as CPU, are allocated by the provider statically and proportionally to memory allocations. This contributes to either under-utilization or throttling. The former significantly impacts the provider, while the latter impacts the client. To solve this problem and accommodate both clients and providers, a solution is dynamic CPU allocation achieved through autoscaling. Autoscaling has been investigated for long-running applications using history-based techniques and prediction. However, serverless applications are short-running workloads, where such techniques are not well suited. In this paper, we investigate tiny autoscalers and how dy-namic CPU allocation techniques perform for short-running serverless workloads. We experiment with Kubernetes as the underlying platform and implement using its vertical pod au-toscaler several dynamic CPU rightsizing techniques. We compare these techniques using state-of-the-art serverless workloads. Our experiments show that dynamic CPU allocation for short-running serverless functions is feasible and can be achieved with lightweight algorithms that offer good performance.","","978-1-6654-9956-9","10.1109/CCGrid54584.2022.00026","Dutch National Science Foundation(grant numbers:VI.202.195); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9825916","","Runtime;Heuristic algorithms;Serverless computing;Clustering algorithms;Containers;Dynamic scheduling;Virtual machining","","9","","56","IEEE","19 Jul 2022","","","IEEE","IEEE Conferences"
"DSServe - Data Science using Serverless","D. Patel; S. Lin; J. Kalagnanam","IBM TJ Watson Research Center, NY, USA; IBM TJ Watson Research Center, NY, USA; IBM TJ Watson Research Center, NY, USA",2022 IEEE International Conference on Big Data (Big Data),"26 Jan 2023","2022","","","2343","2345","AI Applications uses various data science tools such as Jupyter notebook to prescribe a series of steps, commonly referred as workflow, for building AI Solutions. The steps in workflow can be as simple as loading the data from remote storage, visualize the data for better understanding or conducting data quality study, or it can be as complex as generating features for modeling, best model discovery processes, etc. Clearly, different steps of the data science workflow has varying requirement of compute resources. Moreover, the execution of steps in workflow are Adhoc and Subjective. With wider availability of various Serverless technology, in this paper, we demonstrate a generalized framework that can be used to provide on demand scale out capability for the Data Science Workflow. In particular, we selected the most common AI operation, namely Automatic Model Selection, as an example to demonstrate benefits of serverless computing. We conducted a detailed experimental results using IBM Code Engine technology to validate the benefits of our proposed approach.","","978-1-6654-8045-1","10.1109/BigData55660.2022.10020441","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10020441","","Computational modeling;Loading;Serverless computing;Data visualization;Data science;Big Data;Data models","","","","26","IEEE","26 Jan 2023","","","IEEE","IEEE Conferences"
"RightFusion: Enabling QoS driven Function Fusion in Edge-Cloud FaaS","K. R. Sheshadri; J. Lakshmi","Cloud Systems Lab, Indian Institute of Science, Bangalore, INDIA; Cloud Systems Lab, Indian Institute of Science, Bangalore, INDIA",2024 IEEE International Conference on Cloud Engineering (IC2E),"14 Nov 2024","2024","","","160","167","Function-as-a-Service (FaaS), a form of serverless computing, allows applications to be composed as a workflow of stateless, event-driven, short-lived functions that are deployed automatically by the service provider. FaaS has its origins in the Cloud, but recently, it has seen considerable adoption in the Edge-Computing too. FaaS exhibits significant overheads between invocations of consecutive functions and function fusion is being used to resolve these. The fused function groups are assigned the highest resource of all the functions in the fusion group and are limited by the fixed resource sizes associated with the functions; this translates to high function execution costs. In this trade-off, the function size and execution time, which is associated with input size and user-performance expectation, are not considered for fusion decisions on Heterogeneous resources. In this paper, we present ‘RightFusion”, a novel fusion technique that performs function-fusion on an Edge-Cloud resource pool based on application input and QoS requirements. The platform right-sizes resource instances and performs cost-efficient function fusion while meeting the QoS requirements. It also exploits heterogeneous resource cost difference to choose resources for fused functions deployment. Evaluations based on a diverse range of applications show that RightFusion reduces resource usage by above $35 \%$ at Edge and Cloud while meeting the QoS requirements compared to the PeakFusion baseline.","2694-0825","979-8-3315-2869-0","10.1109/IC2E61754.2024.00025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10749776","","Costs;Conferences;Serverless computing;Quality of service","","","","32","IEEE","14 Nov 2024","","","IEEE","IEEE Conferences"
"Serverless Big Data Processing using Matrix Multiplication as Example","S. Werner; J. Kuhlenkamp; M. Klems; J. Müller; S. Tai","ISE, TU, Berlin, Germany; ISE, TU, Berlin, Germany; WeAdvise AG, Munich, Germany; TU, Berlin, Germany; ISE, TU, Berlin, Germany",2018 IEEE International Conference on Big Data (Big Data),"24 Jan 2019","2018","","","358","365","Serverless computing, or Function-as-a-Service (FaaS), is emerging as a popular alternative model to on-demand cloud computing. Function services are executed by a FaaS provider; a client no longer uses cloud infrastructure directly as in traditional cloud consumption. Is serverless computing a feasible and beneficial approach to big data processing, regarding performance, scalability, and cost effectiveness? In this paper, we explore this research question using matrix multiplication as example. We define requirements for the design of serverless big data applications, present a prototype for matrix multiplication using FaaS, and discuss and synthesize insights from results of extensive experimentation. We show that serverless big data processing can lower operational and infrastructure costs without compromising system qualities; serverless computing can even outperform cluster-based distributed compute frameworks regarding performance and scalability.","","978-1-5386-5035-6","10.1109/BigData.2018.8622362","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8622362","serverless;big data;cloud;matrix multiplication","Cloud computing;Scalability;FAA;Prototypes;Big Data applications;Task analysis","","30","","20","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"A Serverless Engine for High Energy Physics Distributed Analysis","J. Kuśnierz; V. E. Padulano; M. Malawski; K. Burkiewicz; E. T. Saavedra; P. Alonso-Jordá; M. Pitt; V. Avati","Institute of Computer Science, AGH, Kraków, Poland; EP-SFT, CERN, Geneva, Switzerland; Institute of Computer Science, AGH, Kraków, Poland; Institute of Computer Science, AGH, Kraków, Poland; Institute of Computer Science, AGH, Kraków, Poland; DSIC, UPV, Valencia, Spain; EP-CMG-OS, CERN, Geneva, Switzerland; EP-UHC, CERN, Geneva, Switzerland","2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","19 Jul 2022","2022","","","575","584","The Large Hadron Collider (LHC) at CERN has generated in the last decade an unprecedented volume of data for the High-Energy Physics (HEP) field. Scientific collaborations interested in analysing such data very often require computing power beyond a single machine. This issue has been tackled traditionally by running analyses in distributed environments using stateful, managed batch computing systems. While this approach has been effective so far, current estimates for future computing needs of the field present large scaling challenges. Such a managed approach may not be the only viable way to tackle them and an interesting alternative could be provided by serverless architectures, to enable an even larger scaling potential. This work describes a novel approach to running real HEP scientific applications through a distributed serverless computing engine. The engine is built upon ROOT, a well-established HEP data analysis software, and distributes its computations to a large pool of concurrent executions on Amazon Web Services Lambda Serverless Platform. Thanks to the developed tool, physicists are able to access datasets stored at CERN (also those that are under restricted access policies) and process it on remote infrastructures outside of their typical environment. The analysis of the serverless functions is monitored at runtime to gather performance metrics, both for data- and computation-intensive workloads.","","978-1-6654-9956-9","10.1109/CCGrid54584.2022.00067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9826036","Serverless;Distributed Computing;CERN;ROOT;HEP;MapReduce;AWS;Lambda","Codes;Runtime;Web services;Large Hadron Collider;Serverless computing;C++ languages;Computer architecture","","5","","40","IEEE","19 Jul 2022","","","IEEE","IEEE Conferences"
"Serverless Approach to Sensitivity Analysis of Computational Models","P. Kica; M. Otta; K. Czechowic; K. Zajac; P. Nowakowski; A. Narracott; I. Halliday; M. Malawski","Sano Centre for Computational Medicine, Kraków, Poland; Sano Centre for Computational Medicine, Kraków, Poland; Department of Infection, Immunity and Cardiovascular Disease, University of Sheffield, Sheffield, UK; Sano Centre for Computational Medicine, Kraków, Poland; Sano Centre for Computational Medicine, Kraków, Poland; Department of Infection, Immunity and Cardiovascular Disease, University of Sheffield, Sheffield, UK; Department of Infection, Immunity and Cardiovascular Disease, University of Sheffield, Sheffield, UK; Sano Centre for Computational Medicine, Kraków, Poland","2023 IEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","10 Jul 2023","2023","","","627","639","Digital twins are virtual representations of physical objects or systems used for the purpose of analysis, most often via computer simulations, in many engineering and scientific disciplines. Recently, this approach has been introduced to computational medicine, within the concept of Digital Twin in Healthcare (DTH). Such research requires verification and validation of its models, as well as the corresponding sensitivity analysis and uncertainty quantification (VVUQ). From the computing perspective, VVUQ is a computationally intensive process, as it requires numerous runs with variations of input parameters. Researchers often use high-performance computing (HPC) solutions to run VVUQ studies where the number of parameter combinations can easily reach tens of thousands. However, there is a viable alternative to HPC for a substantial subset of computational models - serverless computing. In this paper we hypothesize that using the serverless computing model can be a practical and efficient approach to selected cases of running VVUQ calculations. We show this on the example of the EasyVVUQ library, which we extend by providing support for many serverless services. The resulting library - CloudVVUQ - is evaluated using two real-world applications from the computational medicine domain adapted for serverless execution. Our experiments demonstrate the scalability of the proposed approach.","","979-8-3503-0119-9","10.1109/CCGrid57682.2023.00064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10171522","Serverless;Digital twins;Computational modeling;Sensitivity analysis;Distributed computing;Cloud computing;AWS;GCP;Lambda;High-Performance Computing","Analytical models;Uncertainty;Sensitivity analysis;Computational modeling;Scalability;Serverless computing;Libraries","","","","37","IEEE","10 Jul 2023","","","IEEE","IEEE Conferences"
"Exploring Serverless Computing for Neural Network Training","L. Feng; P. Kudva; D. Da Silva; J. Hu","Texas A&M University, College Station; IBM Research; Texas A&M University, College Station; Texas A&M University, College Station",2018 IEEE 11th International Conference on Cloud Computing (CLOUD),"11 Sep 2018","2018","","","334","341","Serverless or functions as a service runtimes have shown significant benefits to efficiency and cost for event-driven cloud applications. Although serverless runtimes are limited to applications requiring lightweight computation and memory, such as machine learning prediction and inference, they have shown improvements on these applications beyond other cloud runtimes. Training deep learning can be both compute and memory intensive. We investigate the use of serverless runtimes while leveraging data parallelism for large models, show the challenges and limitations due to the tightly coupled nature of such models, and propose modifications to the underlying runtime implementations that would mitigate them. For hyperparameter optimization of smaller deep learning models, we show that serverless runtimes can provide significant benefit.","2159-6190","978-1-5386-7235-8","10.1109/CLOUD.2018.00049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8457817","serverless computing;cloud computing;deep learning;cloud scaling;cloud cost and performance","Servers;Training;Merging;Runtime;Neural networks;Data transfer;Machine learning","","59","","28","IEEE","11 Sep 2018","","","IEEE","IEEE Conferences"
"Amoeba: QoS-Awareness and Reduced Resource Usage of Microservices with Serverless Computing","Z. Li; Q. Chen; S. Xue; T. Ma; Y. Yang; Z. Song; M. Guo","Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Alibaba Cloud, China; Alibaba Cloud, China; Alibaba Cloud, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China",2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS),"14 Jul 2020","2020","","","399","408","While microservices that have stringent Quality-of-Service constraints are deployed in the Clouds, the long-term rented infrastructures that host the microservices are under-utilized except peak hours due to the diurnal load pattern. It is resource efficient for Cloud vendors and cost efficient for service maintainers to deploy the microservices in the long-term infrastructure at high load and in the serverless computing platform at low load. However, prior work fails to take advantage of the opportunity, because the contention between microservices on the serverless platform seriously affects their response latencies.Our investigation shows that the load of a microservice, the shared resource contentions on the serverless platform, and its sensitivities to the contention together affect the response latency of the microservice on the platform. To this end, we propose Amoeba, a runtime system that dynamically switches the deployment of a microservice. Amoeba is comprised of a contention-aware deployment controller, a hybrid execution engine, and a multi-resource contention monitor. The deployment controller predicts the tail latency of a microservice based on its load and the contention on the serverless platform, and determines the appropriate deployment of the microservice. The hybrid execution engine enables the quick switch of the two deploy modes. The contention monitor periodically quantifies the contention on multiple types of shared resources. Experimental results show that Amoeba is able to significantly reduce up to 72.9% of CPU usage and up to 84.9% of memory usage compared with the traditional pure IaaS-based deployment, while ensuring the required latency target.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139803","Serverless computing;Microservices;QoS","Quality of service;Benchmark testing;Containers;Cloud computing;Servers;Sensitivity;Runtime","","27","","31","IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Lambdata: Optimizing Serverless Computing by Making Data Intents Explicit","Y. Tang; J. Yang","Department of Computer Science, Columbia University, New York, NY, USA; Department of Computer Science, Columbia University, New York, NY, USA",2020 IEEE 13th International Conference on Cloud Computing (CLOUD),"18 Dec 2020","2020","","","294","303","Serverless computing emerges as a new paradigm to build cloud applications, in which developers write small functions that react to cloud infrastructure events, and cloud providers maintain all resources and schedule the functions in containers. Serverless computing thus enables developers to focus on their core business logic and leave server management and scaling to cloud providers. Unfortunately, existing serverless computing systems suffer from a key limitation that deprives them of enjoying significant speedups. Specifically, they treat each cloud function as a black box and are blind to which data the function reads or writes, therefore missing potentially huge optimization opportunities, such as caching data and colocating functions. We present Lambdata, a novel serverless computing system that enables developers to declare a cloud function's data intents, including both data read and data written. Once data intents are made explicit, Lambdata performs a variety of optimizations to improve speed, including caching data locally and scheduling functions based on code and data locality. Our evaluation of Lambdata shows that it achieves an average speedup of 1.51x on the turnaround time of practical workloads and reduces monetary cost by 16.5%.","2159-6190","978-1-7281-8780-8","10.1109/CLOUD49709.2020.00049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284203","serverless computing;cloud function;cloud storage","Cloud computing;Schedules;Scheduling algorithms;Conferences;Containers;Servers;Optimization","","16","","32","IEEE","18 Dec 2020","","","IEEE","IEEE Conferences"
"Fair, Efficient Multi-Resource Scheduling for Stateless Serverless Functions with Anubis","A. Samanta; R. Stutsman",University of Utah; University of Utah,"2024 IEEE 24th International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","8 Oct 2024","2024","","","106","112","Although serverless platforms have been extremely popular recently, certain kinds of applications are not well-supported by these platforms. For example, they work well for workloads that transform or aggregate bulk data but not for applications where predictable response times are crucial. These workloads have fine-grained tasks and stringent service level agreements (SLAs). Current serverless platforms’ overheads and resource contention lead to variable performance, making using them for real-time online applications impossible. We demonstrate Anubis, a new platform built on top of OpenWhisk, that helps meet SLAs for response-time-sensitive serverless workloads, even when multiple concurrent workloads compete for various resource types. Anubis’s approach centers on multi-resource fair queuing. Even as stateless functions compete for CPU, storage, and network resources, Anubis ensures each gets its fair share according to its dominant resource needs. We show that when various serverless workloads are intermixed, Anubis’s approach can reduce SLA violations by 15–34%, improving max-min fairness by 2× compared to competing scheduling policies.","2993-2114","979-8-3503-9566-2","10.1109/CCGrid59990.2024.00021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10701320","Serverless computing;resource management;scheduling;performance analysis","Cloud computing;Processor scheduling;Clustering algorithms;Transforms;Minimax techniques;Data aggregation;Real-time systems;Resource management;Time factors;Service level agreements","","","","59","IEEE","8 Oct 2024","","","IEEE","IEEE Conferences"
"Real-time task scheduling in a FaaS cloud","M. Szalay; P. Mátray; L. Toka","MTA-BME Network Softwarization Research Group, Faculty of Electrical Engineering and Informatics, Budapest University of Technology and Economics; Ericsson Research; MTA-BME Network Softwarization Research Group, Faculty of Electrical Engineering and Informatics, Budapest University of Technology and Economics",2021 IEEE 14th International Conference on Cloud Computing (CLOUD),"8 Nov 2021","2021","","","497","507","Today, Function-as-a-Service is the most promising concept of serverless cloud computing. It makes possible for developers to focus on application development without any system management effort: FaaS ensures resource allocation, fast response time, schedulability, scalability, resiliency, and upgrad-ability. Applications of 5G, IoT, and Industry 4.0 raise the idea to open cloud-edge computing infrastructures for time-critical applications too, i.e., there is a strong desire to pose real-time requirements for computing systems like FaaS. However, multinode systems make real-time scheduling significantly complex since guaranteeing real-time task execution is challenging even on one computing node with multi-core processors. In this paper, we present an analytical model and a heuristic partitioning scheduling algorithm for a partitioned scheduling system suitable for real-time FaaS platforms of multi-node clusters. We present the architecture of the envisioned real-time FaaS platform, emphasize its benefits and the requirements for the underlying network and nodes, and survey the related work that could meet these demands.","2159-6190","978-1-6654-0060-2","10.1109/CLOUD53861.2021.00065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9582186","real-time scheduling;real-time cloud;real-time FaaS;partitioned scheduling;partitioned-EDF;heterogeneous multiprocessor;multi-node and multiprocessor system","Cloud computing;Job shop scheduling;Runtime;Scheduling algorithms;Scalability;FAA;Real-time systems","","5","","36","IEEE","8 Nov 2021","","","IEEE","IEEE Conferences"
"EMARS: Efficient Management and Allocation of Resources in Serverless","A. Saha; S. Jindal","School of Computing, University of Utah Salt, Lake City, USA; School of Computing, University of Utah Salt, Lake City, USA",2018 IEEE 11th International Conference on Cloud Computing (CLOUD),"11 Sep 2018","2018","","","827","830","We introduce EMARS, an efficient resource management system for serverless cloud computing frameworks with the goal to enhance resource (focus on memory) allocation among containers. We have built our prototype on top of an open-source serverless platform, OpenLambda. It is based upon application workloads and serverless functions' memory needs. As a background motivation we analyzed the latencies and memory requirements of functions running on AWS lambda. The memory limits also lead to variations in number of containers spawned on OpenLambda. We use memory limit settings to propose a model of predictive efficient memory management.","2159-6190","978-1-5386-7235-8","10.1109/CLOUD.2018.00113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8457882","Serverless;cloud computing;memory limit;response time","Containers;Memory management;Resource management;Cloud computing;Servers;Predictive models;Time factors","","23","","12","IEEE","11 Sep 2018","","","IEEE","IEEE Conferences"
"Performance Analysis of Apache OpenWhisk Across the Edge-Cloud Continuum","A. Alabbas; A. Kaushal; O. Almurshed; O. Rana; N. Auluck; C. Perera","Cardiff University, United Kingdom; Imam Abdulrahman Bin Faisal University, Saudi Arabia; Cardiff University, United Kingdom; Cardiff University, United Kingdom; Indian Institute of Technology, Ropar, India; Cardiff University, United Kingdom",2023 IEEE 16th International Conference on Cloud Computing (CLOUD),"25 Sep 2023","2023","","","401","407","Serverless computing offers opportunities for auto-scaling, a pay-for-use cost model, quicker deployment and faster updates to support computing services. Apache OpenWhisk is one such open-source, distributed serverless platform that can be used to execute user functions in a stateless manner. We conduct a performance analysis of OpenWhisk on an edge-cloud continuum, using a function chain of video analysis applications. We consider a combination of Raspberry Pi and cloud nodes to deploy OpenWhisk, modifying a number of parameters, such as maximum memory limit and runtime, to investigate application behaviours. The five main factors considered are: cold and warm activation, memory and input size, CPU architecture, runtime packages used, and concurrent invocations. The results have been evaluated using initialization, and execution time, minimum memory requirement, inference time and accuracy.","2159-6190","979-8-3503-0481-7","10.1109/CLOUD60044.2023.00054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10255019","edge-cloud computing;serverless;function as a service;OpenWhisk;performance evaluation","Cloud computing;Runtime;Costs;Economic indicators;Computational modeling;Memory management;Memory architecture","","8","","32","IEEE","25 Sep 2023","","","IEEE","IEEE Conferences"
"FaaSter Troubleshooting - Evaluating Distributed Tracing Approaches for Serverless Applications","M. C. Borges; S. Werner; A. Kilic","Information Systems Engineering, Technische Universität Berlin, Berlin, Germany; Information Systems Engineering, Technische Universität Berlin, Berlin, Germany; Technische Universität Berlin, Berlin, Germany",2021 IEEE International Conference on Cloud Engineering (IC2E),"22 Nov 2021","2021","","","83","90","Serverless applications can be particularly difficult to troubleshoot, as these applications are often composed of various managed and partly managed services. Faults are often unpredictable and can occur at multiple points, even in simple compositions. Each additional function or service in a serverless composition introduces a new possible fault source and a new layer to obfuscate faults. Currently, serverless platforms offer only limited support for identifying runtime faults. Developers looking to observe their serverless compositions often have to rely on scattered logs and ambiguous error messages to pinpoint root causes. In this paper, we investigate the use of distributed tracing for improving the observability of faults in serverless applications. To this end, we first introduce a model for characterizing fault observability, then provide a prototypical tracing implementation-specifically, a developer-driven and a platform-supported tracing approach. We compare both approaches with our model, measure associated trade-offs (execution latency, resource utilization), and contribute new insights for troubleshooting serverless compositions.","","978-1-6654-4970-0","10.1109/IC2E52221.2021.00022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9610265","Serverless Computing;FaaS Platforms;Distributed Tracing;Observability","Fault diagnosis;Costs;Runtime;Computational modeling;Conferences;Tools;Resource management","","9","","17","IEEE","22 Nov 2021","","","IEEE","IEEE Conferences"
"QoS-Aware and Cost-Efficient Dynamic Resource Allocation for Serverless ML Workflows","H. Wu; J. Deng; H. Fan; S. Ibrahim; S. Wu; H. Jin","National Engineering Research Center for Big Data Technology and System Services Computing Technology and System Lab, Cluster and Grid Computing Lab School of Computer Science and Technology, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System Services Computing Technology and System Lab, Cluster and Grid Computing Lab School of Computer Science and Technology, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System Services Computing Technology and System Lab, Cluster and Grid Computing Lab School of Computer Science and Technology, Huazhong University of Science and Technology, China; Inria, Univ. Rennes, CNRS, IRISA, France; National Engineering Research Center for Big Data Technology and System Services Computing Technology and System Lab, Cluster and Grid Computing Lab School of Computer Science and Technology, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System Services Computing Technology and System Lab, Cluster and Grid Computing Lab School of Computer Science and Technology, Huazhong University of Science and Technology, China",2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS),"18 Jul 2023","2023","","","886","896","Machine Learning (ML) workflows are increasingly deployed on serverless computing platforms to benefit from their elasticity and fine-grain pricing. Proper resource allocation is crucial to achieve fast and cost-efficient execution of serverless ML workflows (specially for hyperparameter tuning and model training). Unfortunately, existing resource allocation methods are static, treat functions equally, and rely on offline prediction, which limit their efficiency. In this paper, we introduce CE-scaling – a Cost-Efficient autoscaling framework for serverless ML work-flows. During the hyperparameter tuning, CE-scaling partitions resources across stages according to their exact usage to minimize resource waste. Moreover, it incorporates an online prediction method to dynamically adjust resources during model training. We implement and evaluate CE-scaling on AWS Lambda using various ML models. Evaluation results show that compared to state-of-the-art static resource allocation methods, CE-scaling can reduce the job completion time and the monetary cost by up to 63% and 41% for hyperparameter tuning, respectively; and by up to 58% and 38% for model training.","1530-2075","979-8-3503-3766-2","10.1109/IPDPS54959.2023.00093","Research and Development; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10177446","serverless computing;distributed machine learning;resource provisioning","Training;Costs;Computational modeling;Serverless computing;Quality of service;Pricing;Predictive models","","10","","32","IEEE","18 Jul 2023","","","IEEE","IEEE Conferences"
"FaaS and Furious: abstractions and differential caching for efficient data pre-processing","J. Tagliabue; R. Curtin; C. Greco","Bauplan Labs, New York, US; Bauplan Labs, Atlanta, US; Bauplan Labs, New York, US",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","3562","3567","Data pre-processing pipelines are the bread and butter of any successful AI project. We introduce a novel programming model for pipelines in a data lakehouse, allowing users to interact declaratively with assets in object storage. Motivated by real-world industry usage patterns, we exploit these new abstractions with a columnar and differential cache to maximize iteration speed for data scientists, who spent most of their time in pre-processing – adding or removing features, restricting or relaxing time windows, wrangling current or older datasets. We show how the new cache works transparently across programming languages, schemas and time windows, and provide preliminary evidence on its efficiency on standard data workloads.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825377","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825377","faas;data processing;data pipelines;cache;lakehouse","Industries;Computer languages;Dairy products;Pipelines;Programming;Big Data;Data models;Artificial intelligence;Standards","","","","25","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Deadline-aware Dynamic Resource Management in Serverless Computing Environments","A. Mampage; S. Karunasekera; R. Buyya","School of Computing and Information Systems, Cloud Computing and Distributed Systems (CLOUDS) Laboratory, The University of Melbourne, Australia; School of Computing and Information Systems, Cloud Computing and Distributed Systems (CLOUDS) Laboratory, The University of Melbourne, Australia; School of Computing and Information Systems, Cloud Computing and Distributed Systems (CLOUDS) Laboratory, The University of Melbourne, Australia","2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","2 Aug 2021","2021","","","483","492","Serverless computing enables rapid application development and deployment by composing loosely coupled microservices at a scale. This emerging paradigm greatly unburdens the users of cloud environments, from the need to provision and manage the underlying cloud resources. With this shift in responsibility, the cloud provider faces the challenge of providing acceptable performance to the user without compromising on reliability, while having minimal knowledge of the application requirements. Sub-optimal resource allocations, specifically the CPU resources, could result in the violation of performance requirements of applications. Further, the fine-grained serverless billing model only charges for resource usage in terms of function execution time. At the same time, the provider has to maintain the underlying infrastructure in always-on mode to facilitate asynchronous function calls. Thus, achieving optimum utilization of cloud resources without compromising on application requirements is of high importance to the provider. Most of the current works only focus on minimizing function execution times caused by delays in infrastructure set up and reducing resource costs for the end-user. However, in this paper, we focus on both the provider and user’s perspective and propose a function placement policy and a dynamic resource management policy for applications deployed in serverless computing environments. The policies minimize the resource consumption cost for the service provider while meeting the user’s application requirement, i.e., deadline. The proposed solutions are sensitive to deadline and efficiently increase the resource utilization for the provider, while dynamically managing resources to improve function response times. We implement and evaluate our approach through simulation using ContainerCloudSim toolkit. The proposed function placement policy when compared with baseline scheduling techniques can reduce resource consumption by up to three times. The dynamic resource allocation policy when evaluated with a fixed resource allocation policy and a proportional CPU-shares policy shows improvements of up to 25% in meeting the required function deadlines.","","978-1-7281-9586-5","10.1109/CCGrid51090.2021.00058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9499407","serverless computing;function placement;dy-namic resource management;resource efficiency","Cloud computing;Processor scheduling;Meetings;Dynamic scheduling;Delays;Resource management;Time factors","","35","","27","IEEE","2 Aug 2021","","","IEEE","IEEE Conferences"
"HeROfake: Heterogeneous Resources Orchestration in a Serverless Cloud – An Application to Deepfake Detection","V. Lannurien; L. D'Orazio; O. Barais; E. Bernard; O. Weppe; L. Beaulieu; A. Kacete; S. Paquelet; J. Boukhobza",b<>com Institute of Research and Technology; b<>com Institute of Research and Technology; b<>com Institute of Research and Technology; b<>com Institute of Research and Technology; b<>com Institute of Research and Technology; b<>com Institute of Research and Technology; b<>com Institute of Research and Technology; b<>com Institute of Research and Technology; b<>com Institute of Research and Technology,"2023 IEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","10 Jul 2023","2023","","","154","165","Serverless is a trending service model for cloud computing. It shifts a lot of the complexity from customers to service providers. However, current serverless platforms mostly consider the provider's infrastructure as homogeneous, as well as the users' requests. This limits possibilities for the provider to leverage heterogeneity in their infrastructure to improve function response time and reduce energy consumption. We propose a heterogeneity-aware serverless orchestrator for private clouds that consists of two components: the autoscaler allocates heterogeneous hardware resources (CPUs, GPUs, FPGAs) for function replicas, while the scheduler maps function executions to these replicas. Our objective is to guarantee function response time, while enabling the provider to reduce resource usage and energy consumption. This work considers a case study for a deepfake detection application relying on CNN inference. We devised a simulation environment that implements our model and a baseline Knative orchestrator, and evaluated both policies with regard to consolidation of tasks, energy consumption and SLA penalties. Experimental results show that our platform yields substantial gains for all those metrics, with an average of 35% less energy consumed for function executions while consolidating tasks on less than 40% of the infrastructure's nodes, and more than 60% less SLA violations.","","979-8-3503-0119-9","10.1109/CCGrid57682.2023.00024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10171518","deepfake;serverless;allocation;scheduling;SLA;energy consumption;heterogeneous resources;workload characterization;GPU;FPGA","Measurement;Cloud computing;Energy consumption;Deepfakes;Computational modeling;Hardware;Complexity theory","","2","","47","IEEE","10 Jul 2023","","","IEEE","IEEE Conferences"
"Serverless Data Analytics with Flint","Y. Kim; J. Lin","David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Ontario, Canada; David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Ontario, Canada",2018 IEEE 11th International Conference on Cloud Computing (CLOUD),"11 Sep 2018","2018","","","451","455","Serverless architectures organized around loosely-coupled function invocations represent an emerging design for many applications. Recent work mostly focuses on user-facing products and event-driven processing pipelines. In this paper, we explore a completely different part of the application space and examine the feasibility of analytical processing on big data using a serverless architecture. We present Flint, a prototype Spark execution engine that takes advantage of AWS Lambda to provide a pure pay-as-you-go cost model. With Flint, a developer uses PySpark exactly as before, but without needing an actual Spark cluster. We describe the design, implementation, and performance of Flint, along with the challenges associated with serverless analytics.","2159-6190","978-1-5386-7235-8","10.1109/CLOUD.2018.00063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8457831","serverless computing, cloud computing, data analytics, data science","Sparks;Task analysis;Cloud computing;Engines;Standards;Metadata;Data analysis","","44","","8","IEEE","11 Sep 2018","","","IEEE","IEEE Conferences"
"Improving Robustness of Heterogeneous Serverless Computing Systems via Probabilistic Task Pruning","C. Denninnart; J. Gentry; M. Amini Salehi","High Performance Cloud Computing (HPCC) Laboratory, University of Louisiana at Lafayette, USA; High Performance Cloud Computing (HPCC) Laboratory, University of Louisiana at Lafayette, USA; High Performance Cloud Computing (HPCC) Laboratory, University of Louisiana at Lafayette, USA",2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),"29 Jul 2019","2019","","","6","15","Cloud-based serverless computing is an increasingly popular computing paradigm. In this paradigm, different services have diverse computing requirements that justify deploying an inconsistently Heterogeneous Computing (HC) system to efficiently process them. In an inconsistently HC system, each task needed for a given service, potentially exhibits different execution times on each type of machine. An ideal resource allocation system must be aware of such uncertainties in execution times and be robust against them, so that Quality of Service (QoS) requirements of users are met. This research aims to maximize the robustness of an HC system utilized to offer a serverless computing system, particularly when the system is oversubscribed. Our strategy to maximize robustness is to develop a task pruning mechanism that can be added to existing task-mapping heuristics without altering them. Pruning tasks with a low probability of meeting their deadlines improves the likelihood of other tasks meeting their deadlines, thereby increasing system robustness and overall QoS. To evaluate the impact of the pruning mechanism, we examine it on various configurations of heterogeneous and homogeneous computing systems. Evaluation results indicate a considerable improvement (up to 35%) in the system robustness.","","978-1-7281-3510-6","10.1109/IPDPSW.2019.00012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8778313","Heterogeneous Computing (HC);Scheduling;Mapping Heuristic;Serverless;Pruning;Robustness","Task analysis;Robustness;Resource management;Uncertainty;Quality of service;Probabilistic logic;Streaming media","","13","","37","IEEE","29 Jul 2019","","","IEEE","IEEE Conferences"
"Scalability Analysis of Blockchain on a Serverless Cloud","A. Kaplunovich; K. P. Joshi; Y. Yesha","Computer Science Department, University of Maryland, Baltimore, USA; Information Systems Department, University of Maryland, Baltimore, USA; Computer Science Department, University of Maryland, Baltimore, USA",2019 IEEE International Conference on Big Data (Big Data),"24 Feb 2020","2019","","","4214","4222","While adopting Blockchain technologies to automate their enterprise functionality, organizations are recognizing the challenges of scalability and manual configuration that the state of art present. Scalability of Hyperledger Fabric is an open challenge recognized by the research community. We have automated many of the configuration steps of installing Hyperledger Fabric Blockchain on AWS infrastructure and have benchmarked the scalability of that system. We have used the UCR (University of California Riverside) Time Series Archive with 128 timeseries datasets containing over 191,177 rows of data totaling 76,453,742 numbers. Using an automated Serverless approach, we have loaded this dataset, by chunks, into different AWS instances, triggering the load by SQS messaging. In this paper, we present the results of this benchmarking study and describe the approach we took to automate the Hyperledger Fabric processes using serverless Lambda functions and SQS triggering. We will also discuss what is needed to make the Blockchain technology more robust and scalable.","","978-1-7281-0858-2","10.1109/BigData47090.2019.9005529","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9005529","blockchain;serverless;AWS Cloud;Lambda;automation;messaging;benchmark","Fabrics;Servers;Scalability;Cloud computing;Time series analysis;Benchmark testing","","9","","15","IEEE","24 Feb 2020","","","IEEE","IEEE Conferences"
"HeROcache: Storage-Aware Scheduling in Heterogeneous Serverless Edge – The Case of IDS","V. Lannurien; C. Slimani; L. d’Orazio; O. Barais; S. Paquelet; J. Boukhobza","b<>com Institute of Research and Technology; Inria, CNRS, IRISA, Univ. Rennes; b<>com Institute of Research and Technology; b<>com Institute of Research and Technology; b<>com Institute of Research and Technology; b<>com Institute of Research and Technology","2024 IEEE 24th International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","8 Oct 2024","2024","","","587","597","Intrusion Detection Systems (IDS) are time-sensitive applications that aim to classify potentially malicious network traffic. IDSs are part of a class of applications that rely on short-lived functions that can be run reactively and, as such, could be deployed on edge resources, to offload processing from energy-constrained battery-backed devices. The serverless service model could fit the needs of such applications, given that the platform allows adequate levels of Quality of Service (QoS) for a variety of users, since the criticality of IDS applications depends on several parameters. Deploying serverless functions on unreserved edge resources requires to pay particular attention to (1) initialization delays that could be significant on low resources platforms, (2) inter-function communication between edge nodes, and (3) heterogeneous devices. In this paper, we propose both a storage-aware allocation and scheduling policy that seek to minimize task placement costs for service providers on edge devices while optimizing QoS for IDS users. To do so, we propose a caching and consolidation strategy that minimizes cold starts and inter-function communication delays while satisfying QoS by leveraging heterogeneous edge resources. We evaluated our platform in a simulation environment using characterization data from real-world IDS tasks and execution platforms and compared it with a vanilla Knative orchestrator and a storage-agnostic policy. Our strategy achieves 18% fewer QoS penalties while consolidating applications across 80% fewer edge nodes.","2993-2114","979-8-3503-9566-2","10.1109/CCGrid59990.2024.00071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10701415","serverless;orchestration;scheduling;edge;cloud;IDS;cache;consolidation;heterogeneous computing","Cloud computing;Processor scheduling;Image edge detection;Metaheuristics;Intrusion detection;Quality of service;Telecommunication traffic;Machine learning;Delays;Resource management","","2","","25","IEEE","8 Oct 2024","","","IEEE","IEEE Conferences"
"FederatedTree: A Secure Serverless Algorithm for Federated Learning to Reduce Data Leakage","M. Gharibi; S. Bhagavan; P. Rao","IBM Data and AI, San Jose, CA, USA; University of Missouri-Kansas City, Kansas City, MO, USA; University of Missouri-Columbia, Columbia, MO, USA",2021 IEEE International Conference on Big Data (Big Data),"13 Jan 2022","2021","","","4078","4083","In Federated Learning there have been many op-timization methods that allow flexible local updating such as FedAvg that has become the de facto mechanism for averaging local stochastic gradient descent without sharing the data. Classic FL methods such as FedAvg struggle with trust and data leakage issues. In FedAvg and similar techniques, clients assume the aggregator server is a trusted but curious server. However, even if the server is trusted, the models still leak a lot of data through the weights. Several techniques have been proposed to reduce data leakage. One mechanism involves sharing pieces of the data with the server, but it violates the key privacy assumption of federated learning. Other solutions such as Federated Learning with Differential Privacy aim to reduce data leakage by adding noise to the weights/gradients. However, there is a trade-off between accuracy and the amount of noise added.In this paper, we propose a practical Federated Learning algorithm of deep neural networks on iterative model averaging we called FederatedTree. While FedAvg with differential privacy adds noise to the weights to provide a level of privacy, our algorithm applies a secure sequential averaging without adding noise to the models. FederatedTree solves the trust issue between client-to-client, client-to-server (if exists) and reduces the amount of data leakage without adding noise that lowers the model accuracy. The results show that the FederatedTree algorithm provides a high privacy rate with higher accuracy on popular datasets: MNIST, Fashion MNIST, CIFAR-10. Furthermore, FederatedTree utilizes a binary tree structure to reduce the sequential averaging time and remove the overhead of the excessive communication between the server and the clients.","","978-1-6654-3902-2","10.1109/BigData52589.2021.9672039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9672039","","Training;Privacy;Differential privacy;Distributed databases;Stochastic processes;Binary trees;Big Data","","5","","32","IEEE","13 Jan 2022","","","IEEE","IEEE Conferences"
"SimuScale: Optimizing Parameters for Autoscaling of Serverless Edge Functions Through Co-Simulation","P. Raith; S. Nastic; S. Dustdar","Distributed Systems Group, TU Wien, Vienna, Austria; Distributed Systems Group, TU Wien, Vienna, Austria; Distributed Systems Group, TU Wien, Vienna, Austria",2024 IEEE 17th International Conference on Cloud Computing (CLOUD),"28 Aug 2024","2024","","","305","315","Serverless Edge Computing is growing in popularity, and while commercial providers are starting to offer edge-oriented products, much research is still being done on orches-trating functions (e.g., autoscaling). These approaches range from threshold- to AI-based strategies and support various Service Level Objectives (SLOs), such as Round-Trip-Time (RTT) and re-source usage. However, the Quality of Service (Qo$S$) continuously deteriorates due to the dynamic edge-cloud continuum and static parameterization of orchestration strategy parameters. Platforms must adapt the orchestration parameters during runtime to counteract this drift that causes SLO violations. To this end, we introduce the Orchestration Parameter Optimization Prob-lem (OPOP), which aims to find parameters for orchestration strategies to minimize SLO violations. We propose a novel self-adaptive Simulation-based Scaling (SimuScale) approach that uses co-simulation to solve OPOP for autoscalers during runtime. SimuScale uses live monitoring data to feed the simulation and perform parameter optimization. Our Proof of Concept is inte-grated with Kubernetes and evaluated on a real-world edge-cloud testbed. While this work focuses on a threshold-based autoscaler, it can be extended to optimize other orchestration components (e.g., schedulers). Our experimental results show that SimuScale finds parameters that decrease RTT SLO violations between 15% and 40%. SimuScale also can reduce resource usage by 29.87% while maintaining the target 95th RTT percentile. Moreover, it can reduce variance caused by different request patterns, making orchestration strategies more resilient in realistic scenarios.","2159-6190","979-8-3503-6853-6","10.1109/CLOUD62652.2024.00042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10643923","Serverless Edge Computing;Co-Simulation;Self-Adaptive System;Autoscaler;Edge-Cloud Continuum","Cloud computing;Runtime;Quality of service;Feeds;Optimization;Monitoring;Edge computing","","","","39","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"Cloud Services Enable Efficient AI-Guided Simulation Workflows across Heterogeneous Resources","L. Ward; J. G. Pauloski; V. Hayot-Sasson; R. Chard; Y. Babuji; G. Sivaraman; S. Choudhury; K. Chard; R. Thakur; I. Foster","Argonne National Laboratory, Lemont, IL; University of Chicago, Chicago, IL; University of Chicago, Chicago, IL; Argonne National Laboratory, Lemont, IL; University of Chicago, Chicago, IL; Argonne National Laboratory, Lemont, IL; Pacific Northwest National Laboratory, Richland, WA; University of Chicago, Chicago, IL; Argonne National Laboratory, Lemont, IL; Argonne National Laboratory, Lemont, IL",2023 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),"4 Aug 2023","2023","","","32","41","Applications that fuse machine learning and simulation can benefit from the use of multiple computing resources, with, for example, simulation codes running on highly parallel supercomputers and AI training and inference tasks on specialized accelerators. Here, we present our experiences deploying two AI-guided simulation workflows across such heterogeneous systems. A unique aspect of our approach is our use of cloud-hosted management services to manage challenging aspects of cross-resource authentication and authorization, function-as-a-service (FaaS) function invocation, and data transfer. We show that these methods can achieve performance parity with systems that rely on direct connection between resources. We achieve parity by integrating the FaaS system and data transfer capabilities with a system that passes data by reference among managers and workers, and a user-configurable steering algorithm to hide data transfer latencies. We anticipate that this ease of use can enable routine use of heterogeneous resources in computational science.","","979-8-3503-1199-0","10.1109/IPDPSW59300.2023.00018","Office of Science; National Nuclear Security Administration; Office of Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10196576","Heterogeneous Computing;Function-as-a-Service;Machine Learning;Distributed Systems;Computational Steering","Training;Distributed processing;Codes;Scientific computing;Fuses;Machine learning;Data transfer","","4","","63","IEEE","4 Aug 2023","","","IEEE","IEEE Conferences"
"Towards Latency Sensitive Cloud Native Applications: A Performance Study on AWS","I. Pelle; J. Czentye; J. Dóka; B. Sonkoly",MTA-BME Network Softwarization Research Group; Budapest University of Technology and Economics; Budapest University of Technology and Economics; Budapest University of Technology and Economics,2019 IEEE 12th International Conference on Cloud Computing (CLOUD),"29 Aug 2019","2019","","","272","280","Microservices, serverless architectures, cloud native programming are novel paradigms and techniques which could significantly reduce the burden on both developers and operators of future services. Several types of applications fit in well with the new concepts easing the life of different stakeholders while enabling cloud-grade service deployments. However, latency sensitive applications with strict delay constraints between different components pose additional challenges on the platforms. In order to gain benefit from recent cloud technologies for latency sensitive applications as well, a comprehensive performance analysis of available platforms and relevant components is a crucial first step. In this paper, we address one of the most widely used and versatile cloud platforms, namely Amazon Web Services (AWS), and reveal the delay characteristics of key components and services which impact the overall performance of latency sensitive applications. Our contribution is threefold. First, we define a detailed measurement methodology for CaaS/FaaS (Container/Function as a Service) platforms, specifically for AWS. Second, we provide a comprehensive analysis of AWS components focusing on delay characteristics. Third, we attempt to adjust a drone control application to the platform and investigate the performance on today's system.","2159-6190","978-1-7281-2705-7","10.1109/CLOUD.2019.00054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8814512","AWS;Cloud computing;Latency sensitive applications;Performance measurement;Function as a Service","","","48","","10","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"Coding the Computing Continuum: Fluid Function Execution in Heterogeneous Computing Environments","R. Kumar; M. Baughman; R. Chard; Z. Li; Y. Babuji; I. Foster; K. Chard","Department of Computer Science, University of Chicago, Chicago, IL, USA; Department of Computer Science, University of Chicago, Chicago, IL, USA; Data Science and Learning Division, Argonne National Laboratory, Lemont, IL, USA; Department of Computer Science, University of Chicago, Chicago, IL, USA; Department of Computer Science, University of Chicago, Chicago, IL, USA; Department of Computer Science, University of Chicago, Chicago, IL, USA; Department of Computer Science, University of Chicago, Chicago, IL, USA",2021 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),"24 Jun 2021","2021","","","66","75","Advances in network technologies have greatly decreased barriers to accessing physically distributed computers. This newfound accessibility coincides with increasing hardware specialization, creating exciting new opportunities to dispatch workloads to the best resource for a specific purpose, rather than those that are closest or most easily accessible. We present Delta, a service designed to intelligently schedule function-based workloads across a distributed set of heterogeneous computing resources. Delta implements an extensible architecture in which different predictors and scheduling algorithms can be integrated to provide dynamically evolving estimates of function execution times on different resources-estimates that can be used to determine the most appropriate location for execution. We describe predictors for function runtime, data transfer time, and cold-start resource provisioning and configuration delay; dynamic learning methods that update predictor models over time; and scheduling strategies that take into account both function and endpoint information. We show that these methods can halve workload makespan when compared with a strategy that selects the fastest resource, and decrease makespan by a factor of five when compared to a round robin strategy, when deployed on a heterogeneous testbed with resources ranging from a Raspberry Pi to a GPU node in an academic cloud.","","978-1-6654-3577-2","10.1109/IPDPSW52791.2021.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460607","Computing continuum;function as a service;serverless;heterogeneous computing;scheduling","Learning systems;Schedules;Runtime;Fluids;Graphics processing units;Predictive models;Dynamic scheduling","","19","","45","IEEE","24 Jun 2021","","","IEEE","IEEE Conferences"
"On-Premises Serverless Computing for Event-Driven Data Processing Applications","A. Pérez; S. Risco; D. M. Naranjo; M. Caballer; G. Moltó","Instituto de Instrumentación para Imagen Molecular (I3M), Centro mixto CSIC - Universitat Politècnica de València, Valencia, España; Instituto de Instrumentación para Imagen Molecular (I3M), Centro mixto CSIC - Universitat Politècnica de València, Valencia, España; Instituto de Instrumentación para Imagen Molecular (I3M), Centro mixto CSIC - Universitat Politècnica de València, Valencia, España; Instituto de Instrumentación para Imagen Molecular (I3M), Centro mixto CSIC - Universitat Politècnica de València, Valencia, España; Instituto de Instrumentación para Imagen Molecular (I3M), Centro mixto CSIC - Universitat Politècnica de València, Valencia, España",2019 IEEE 12th International Conference on Cloud Computing (CLOUD),"29 Aug 2019","2019","","","414","421","The advent of open-source serverless computing frameworks has introduced the ability to bring the Functions-as-a-Service (FaaS) paradigm for applications to be executed on-premises. In particular, data-driven scientific applications can benefit from these frameworks with the ability to trigger scalable computation in response to incoming workloads of files to be processed. This paper introduces an open-source framework to achieve on-premises serverless computing for event-driven data processing applications that features: i) the automated provisioning of an elastic Kubernetes cluster that can grow and shrink, in terms of the number of nodes, on multi-Clouds; ii) the automated deployment of a FaaS framework together with a data storage back-end that triggers events upon file uploads; iii) a service that provides a REST API to orchestrate the creation of such functions and iv) a graphical user interface that provides a unified entry point to interact with the aforementioned services. Together, this provides a framework to deploy a computing platform to create highly-parallel event-driven file-processing serverless applications that execute on customized runtime environments provided by Docker containers that run on an elastic Kubernetes cluster. The usefulness of this framework is exemplified by means of the execution of a data-driven workflow for optimised object detection on video. The workflow is tested under three different workloads which process ten, a hundred and a thousand functions. The results show that the presented architecture is able to process such workloads taking advantage of its elasticity to make a sensible usage of the resources.","2159-6190","978-1-7281-2705-7","10.1109/CLOUD.2019.00073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8814513","Cloud Computing;Scientific Computing;Distributed Infrastructures;Containers;Docker","","","31","","28","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"VECA: Reliable and Confidential Resource Clustering for Volunteer Edge-Cloud Computing","H. S. Yeddulapalli; M. L. Alarcon; U. Roy; R. L. Neupane; D. Gafurov; M. Mounesan; S. Debroy; P. Calyam","University of Missouri-Columbia, USA; University of Missouri-Columbia, USA; University of Missouri-Columbia, USA; University of Missouri-Columbia, USA; University of Missouri-Columbia, USA; City University of New York, USA; City University of New York, USA; University of Missouri-Columbia, USA",2024 IEEE International Conference on Cloud Engineering (IC2E),"14 Nov 2024","2024","","","152","159","Volunteer Edge-Cloud (VEC) computing has a significant potential to support scientific workflows in user communities contributing volunteer edge nodes. However, managing heterogeneous and intermittent resources to support machine/deep learning (ML/DL) based workflows poses challenges in resource governance for reliability, and confidentiality for model/data privacy protection. There is a need for approaches to handle the volatility of volunteer edge node availability, and also to scale the confidential data-intensive workflow execution across a large number of VEC nodes. In this paper, we present VECA, a reliable and confidential VEC resource clustering solution featuring three-fold methods tailored for executing ML/DL-based scientific workflows on VEC resources. Firstly, a capacity-based clustering approach enhances system reliability and minimizes VEC node search latency. Secondly, a novel two-phase, globally distributed scheduling scheme optimizes job allocation based on node attributes and using time-series-based Recurrent Neural Networks. Lastly, the integration of confidential computing ensures privacy preservation of the scientific workflows, where model and data information are not shared with VEC resources providers. We evaluate VECA in a Function-as-a-Service (FaaS) cloud testbed that features OpenFaaS and MicroK8S to support two ML/DL-based scientific workflows viz., G2P-Deep (bioinformatics) and PAS-ML (health informatics). Results from tested experiments demonstrate that our proposed VECA approach outperforms state-of-the-art methods; especially VECA exhibits a two-fold reduction in VEC node search latency and over $20 \%$ improvement in productivity rates following execution failures compared to the next best method.","2694-0825","979-8-3315-2869-0","10.1109/IC2E61754.2024.00024","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10749734","Volunteer Edge-Cloud Computing;k-means Clustering;Recurrent Neural Networks;Confidential Computing;Function-as-a-Service;Volatility","Productivity;Privacy;Data privacy;Recurrent neural networks;Processor scheduling;Computational modeling;Reliability engineering;Resource management;Bioinformatics;Protection","","2","","30","IEEE","14 Nov 2024","","","IEEE","IEEE Conferences"
"Pay-as-you-Train: Efficient ways of Serverless Training","D. Chahal; M. Mishra; S. C. Palepu; R. K. Singh; R. Singhal","TCS Research, Mumbai, India; TCS Research, Mumbai, India; TCS Research, Mumbai, India; TCS Research, Mumbai, India; TCS Research, Mumbai, India",2022 IEEE International Conference on Cloud Engineering (IC2E),"16 Nov 2022","2022","","","116","125","Serverless (FaaS) architecture is emerging as a paradigm of choice for many application types, including event triggered, query processing, and machine learning (ML). The use of serverless platforms for ML inference is well known, but its applicability for model training is still under exploration. This paper presents an efficient “pay-as-you-train” methodology for training large deep learning models using serverless cloud services for compute and data management. Serverless compute (such as AWS Lambda) and serverless data management systems (such as AWS key-value store DynamoDB) impose restrictions on the computing time and size of the allowed data objects respectively. We present a novel approach for training deep learning models, which overcomes the limitations imposed by the underlying serverless platforms. We also present an analytical model to study the performance and cost involved in training using different data management services (such as AWS object storage S3, in-memory Memcached, and DynamoDB) as a communication channel with serverless platforms. Additionally, we compare the performance and cost of these services available on cloud. Our optimization techniques improve the performance and hence the cost of training by a factor of 1.2x to 5.5x with these services.","","978-1-6654-9115-0","10.1109/IC2E55432.2022.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9946347","Serverless;machine learning;database;performance","Training;Deep learning;Costs;Computational modeling;Query processing;Distributed databases;Computer architecture","","2","","42","IEEE","16 Nov 2022","","","IEEE","IEEE Conferences"
"Towards a Model-Based Serverless Platform for the Cloud-Edge-IoT Continuum","N. Ferry; R. Dautov; H. Song","I3S/INRIA Kairos, Université Côte d'Azur, Sophia Antipolis, France; SINTEF Digital, Oslo, Norway; SINTEF Digital, Oslo, Norway","2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","19 Jul 2022","2022","","","851","858","One of the most prominent implementations of the serverless programming model is Function-as-a-Service (FaaS). Using FaaS, application developers provide source code of serverless functions, typically describing only parts of a larger application, and define triggers for executing these functions on infrastructure components managed by the FaaS provider. There are still challenges that hinder the wider adoption of the FaaS model across the whole Cloud-Edge-IoT continuum. These include the high heterogeneity of the Edge and IoT infrastructure, vendor lock-in, the need to deploy and adapt serverless functions as well as their supporting services and software stacks into their cyber-physical execution environment. As a first step towards addressing these challenges, we introduce the SERVERLEss4I0T platform for the design, deployment, and maintenance of applications over the Cloud-Edge-IoT continuum. In particular, our platform enables the specification and deployment of serverless functions on Cloud and Edge resources, as well as the deployment of their supporting services and software stacks over the whole Cloud-Edge-IoT continuum.","","978-1-6654-9956-9","10.1109/CCGrid54584.2022.00101","European Commission(grant numbers:101020416); Norwegian Research Council(grant numbers:309700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9826113","Internet of Things;Edge Computing;Cloud Computing;Model-Driven Engineering;Deployment","Cloud computing;Adaptation models;Codes;Computational modeling;FAA;Programming;Maintenance engineering","","6","","16","IEEE","19 Jul 2022","","","IEEE","IEEE Conferences"
"Challenges for Scheduling Scientific Workflows on Cloud Functions","J. Kijak; P. Martyna; M. Pawlik; B. Balis; M. Malawski","Department of Computer Science, AGH University of Science and Technology, Krakow, Poland; Department of Computer Science, AGH University of Science and Technology, Krakow, Poland; Department of Computer Science, AGH University of Science and Technology, Krakow, Poland; Department of Computer Science, AGH University of Science and Technology, Krakow, Poland; Department of Computer Science, AGH University of Science and Technology, Krakow, Poland",2018 IEEE 11th International Conference on Cloud Computing (CLOUD),"11 Sep 2018","2018","","","460","467","Serverless computing, also known as Function-as-a-Service (FaaS) or Cloud Functions, is a new method of running distributed applications by executing functions on the infrastructure of cloud providers. Although it frees the developers from managing servers, there are still decisions to be made regarding selection of function configurations based on the desired performance and cost. The billing model of this approach considers time of execution, measured in 100ms units, as well as the size of the memory allocated per function. In this paper, we look into the problem of scheduling scientific workflows, which are applications consisting of multiple tasks connected into a dependency graph. We discuss challenges related to workflow scheduling and propose the Serverless Deadline-Budget Workflow Scheduling (SDBWS) algorithm adapted to serverless platforms. We present preliminary experiments with a small-scale Montage workflow run on the AWS Lambda infrastructure.","2159-6190","978-1-5386-7235-8","10.1109/CLOUD.2018.00065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8457833","Faas;serverless computing;cloud functions;scientific workflow;task scheduling","Task analysis;Cloud computing;FAA;Processor scheduling;Computational modeling;Adaptation models;Engines","","28","","13","IEEE","11 Sep 2018","","","IEEE","IEEE Conferences"
"Performance Evaluation of Data-Centric Workloads in Serverless Environments","A. M. Nestorov; J. Polo; C. Misale; D. Carrera; A. S. Youssef",Barcelona Supercomputing Center Universitat Politècnica de Catalunya; Barcelona Supercomputing Center; IBM T.J. Watson Research Center; Barcelona Supercomputing Center; IBM T.J. Watson Research Center,2021 IEEE 14th International Conference on Cloud Computing (CLOUD),"8 Nov 2021","2021","","","491","496","Serverless computing is a cloud-based execution paradigm that allows provisioning resources on-demand, freeing developers from infrastructure management and operational concerns. It typically involves deploying workloads as stateless functions that take no resources when not in use, and is meant to scale transparently. To make serverless effective, providers impose limits on a per-function level, such as maximum duration, fixed amount of memory, and no persistent local storage. These constraints make it challenging for data-intensive workloads to take advantage of serverless because they lead to sharing significant amounts of data through remote storage. In this paper, we build a performance model for serverless workloads that considers how data is shared between functions, including the amount of data and the underlying technology that is being used. The model's accuracy is assessed by running a real workload in a cluster using Knative, a state-of-the-art serverless environment, showing a relative error of 5.52%. With the proposed model, we evaluate the performance of data-intensive workloads in serverless, analyzing parallelism, scalability, resource requirements, and scheduling policies. We also explore possible solutions for the data-sharing problem, like using local memory and storage. Our results show that the performance of data-intensive workloads in serverless can be up to 4.32= faster depending on how these are deployed.","2159-6190","978-1-6654-0060-2","10.1109/CLOUD53861.2021.00064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9582265","Serverless;Distributed Computing;Kubernetes;Knative;Tekton;Performance Model;Storage","Performance evaluation;Processor scheduling;Computational modeling;Scalability;Pipelines;Memory management;Parallel processing","","2","","16","IEEE","8 Nov 2021","","","IEEE","IEEE Conferences"
"Cost-Effective Malware Detection as a Service Over Serverless Cloud Using Deep Reinforcement Learning","Y. Birman; S. Hindi; G. Katz; A. Shabtai","Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev; Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev; Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev; Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev","2020 20th IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing (CCGRID)","14 Jul 2020","2020","","","420","429","The current trends of cloud computing in general, and serverless computing in particular, affect multiple aspects of organizational activity. Organizations of all sizes are transitioning parts of their operations off-premise in order to reduce costs and scale their operations more efficiently. The field of network security is no exception, with many organizations taking advantage of the distributed and scalable cloud environment. Since the charging model for serverless computing is ""pay as you go"" (i.e., payment per action), a reduction in the number of required computations translates into significant cost savings. This understanding is also relevant to the field of malware detection, where organizations often deploy multiple types of detectors to increase detection accuracy. In this study, we utilize deep reinforcement learning to reduce computational costs in the cloud by selectively querying only a subset of available detectors. We demonstrate that our approach is not only effective both for on-premise and cloud-based computing architectures, but that applying it to serverless computing can reduce costs by an order of magnitude while maintaining near-optimal performance.","","978-1-7281-6095-5","10.1109/CCGrid49817.2020.00-51","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139646","","Cloud computing;Malware;Organizations;Detectors;Security;Computer architecture;Machine learning","","5","","30","IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"AI-based Resource Allocation: Reinforcement Learning for Adaptive Auto-scaling in Serverless Environments","L. Schuler; S. Jamil; N. Kühl",Karlsruhe Institute of Technology; IBM Research & Development GmbH; IBM Deutschland GmbH Karlsruhe Institute of Technology,"2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","2 Aug 2021","2021","","","804","811","Serverless computing has emerged as a compelling new paradigm of cloud computing models in recent years. It promises the user services at large scale and low cost while eliminating the need for infrastructure management. On cloud provider side, flexible resource management is required to meet fluctuating demand. It can be enabled through automated provisioning and deprovisioning of resources. A common approach among both commercial and open source serverless computing platforms is workload-based auto-scaling, where a designated algorithm scales instances according to the number of incoming requests. In the recently evolving serverless framework Knative a request-based policy is proposed, where the algorithm scales resources by a configured maximum number of requests that can be processed in parallel per instance, the so-called concurrency. As we show in a baseline experiment, this predefined concurrency level can strongly influence the performance of a serverless application. However, identifying the concurrency configuration that yields the highest possible quality of service is a challenging task due to various factors, e.g. varying workload and complex infrastructure characteristics, influencing throughput and latency. While there has been considerable research into intelligent techniques for optimizing auto-scaling for virtual machine provisioning, this topic has not yet been discussed in the area of serverless computing. For this reason, we investigate the applicability of a reinforcement learning approach to request-based auto-scaling in a serverless framework. Our results show that within a limited number of iterations our proposed model learns an effective scaling policy per workload, improving the performance compared to the default auto-scaling configuration.","","978-1-7281-9586-5","10.1109/CCGrid51090.2021.00098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9499535","serverless;auto-scaling;reinforcement learning;Knative","Concurrent computing;Cloud computing;Analytical models;Adaptation models;Computational modeling;Reinforcement learning;Tools","","46","","26","IEEE","2 Aug 2021","","","IEEE","IEEE Conferences"
"Astra: Autonomous Serverless Analytics with Cost-Efficiency and QoS-Awareness","J. Jarachanthan; L. Chen; F. Xu; B. Li",University of Louisiana at Lafayette; University of Louisiana at Lafayette; East China Normal University; Hong Kong University of Science and Technology,2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS),"28 Jun 2021","2021","","","756","765","With the ability to simplify the code deployment with one-click upload and lightweight execution, serverless computing has emerged as a promising paradigm with increasing popularity. However, there remain open challenges when adapting data-intensive analytics applications to the serverless context, in which users of serverless analytics encounter with the difficulty in coordinating computation across different stages and provisioning resources in a large configuration space. This paper presents our design and implementation of Astra, which configures and orchestrates serverless analytics jobs in an autonomous manner, while taking into account flexibly-specified user requirements. Astra relies on the modeling of performance and cost which characterizes the intricate interplay among multi-dimensional factors (e.g., function memory size, degree of parallelism at each stage). We formulate an optimization problem based on user-specific requirements towards performance enhancement or cost reduction, and develop a set of algorithms based on graph theory to obtain optimal job execution. We deploy Astra in the AWS Lambda platform and conduct real-world experiments over three representative benchmarks with different scales. Results demonstrate that Astra can achieve the optimal execution decision for serverless analytics, by improving the performance of 21% to 60% under a given budget constraint, and resulting in a cost reduction of 20% to 80% without violating performance requirement, when compared with three baseline configuration algorithms.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00085","Louisiana Board of Regents; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460548","Cloud computing;serverless computing;resource provisioning;modeling;optimization","Distributed processing;Computational modeling;Quality of service;Parallel processing;Benchmark testing;Graph theory;Optimization","","11","","32","IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Persistent Memory-Aware Scheduling for Serverless Workloads","A. Samanta; F. Ahmed; L. Cao; R. Stutsman; P. Sharma","University of Utah, USA; Hewlett Packard Labs, USA; Hewlett Packard Labs, USA; University of Utah, USA; Hewlett Packard Labs, USA",2023 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),"4 Aug 2023","2023","","","615","621","In the last few years, persistent memory (PM) has become widely commercially available. PM’s direct load/store interface allows fine-grained storage access that fully bypasses all software I/O overheads. However, existing PM modules exhibit performance anomalies, particularly when they are concurrently accessed by many threads. PM is poised to change how all applications access storage, and in this paper we look at how PM might be best managed and used in serverless applications. We make the case that serverless presents a special opportunity for PM because serverless platforms can use the structure of serverless applications to gain more insight into their I/O behavior and because serverless platforms include specialized schedulers that can exploit the combined knowledge of application and PM behavior for managing concurrent workloads.","","979-8-3503-1199-0","10.1109/IPDPSW59300.2023.00105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10196655","Serverless computing;persistent memory;scheduling;Intel Optane DCPMM","Distributed processing;Processor scheduling;Instruction sets;Conferences;Software;Behavioral sciences","","2","","44","IEEE","4 Aug 2023","","","IEEE","IEEE Conferences"
"LIBRA: An Economical Hybrid Approach for Cloud Applications with Strict SLAs","A. Raza; Z. Zhang; N. Akhtar; V. Isahagian; I. Matta",Boston University; Boston University; Akamai Technologies Inc.; IBM Research; Boston University,2021 IEEE International Conference on Cloud Engineering (IC2E),"22 Nov 2021","2021","","","136","146","Function-as-a-Service (FaaS) has recently emerged to reduce the deployment cost of running cloud applications compared to Infrastructure-as-a-Service (IaaS). FaaS follows a serverless “pay-as-you-go” computing model; it comes at a higher cost per unit of execution time but typically application functions experience lower provisioning time (startup delay). IaaS requires the provisioning of Virtual Machines, which typically suffer from longer cold-start delays that cause higher queuing delays and higher request drop rates. We present LIBRA, a balanced (hybrid) approach that leverages both VM-based and serverless resources to efficiently manage cloud resources for the applications. LIBRA closely monitors the application demand and provisions appropriate VM and serverless resources such that the running cost is minimized and Service-Level Agreements are met. Unlike state of the art, LIBRA not only hides VM cold-start delays, and hence reduces response time, by leveraging serverless, but also directs a low-rate bursty portion of the demand to serverless where it would be less costly than spinning up new VMs. We evaluate LIBRA on real traces in a simulated environment as well as on the AWS commercial cloud. Our results show that LIBRA outperforms other resource-provisioning policies, including a recent hybrid approach - LIBRA achieves more than 85% reduction in SLA violations and up to 53% cost savings.","","978-1-6654-4970-0","10.1109/IC2E52221.2021.00028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9610213","EC2;Lambda;IaaS;FaaS","Cloud computing;Costs;Conferences;Computational modeling;FAA;Virtual machining;Delays","","13","","59","IEEE","22 Nov 2021","","","IEEE","IEEE Conferences"
"Performance Optimization for Edge-Cloud Serverless Platforms via Dynamic Task Placement","A. Das; S. Imai; S. Patterson; M. P. Wittie","Dept. of Computer Science, Rensselaer Polytechnic Institute; Dept. of Computer Science, Rensselaer Polytechnic Institute; Dept. of Computer Science, Rensselaer Polytechnic Institute; Gianforte School of Computing, Montana State University","2020 20th IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing (CCGRID)","14 Jul 2020","2020","","","41","50","We present a framework for performance optimization in serverless edge-cloud platforms using dynamic task placement. We focus on applications for smart edge devices, for example, smart cameras or speakers, that need to perform processing tasks on input data in real to near-real time. Our framework allows the user to specify cost and latency requirements for each application task, and for each input, it determines whether to execute the task on the edge device or in the cloud. Further, for cloud executions, the framework identifies the container resource configuration needed to satisfy the performance goals. We have evaluated our framework in simulation using measurements collected from serverless applications in AWS Lambda and AWS Greengrass. In addition, we have implemented a prototype of our framework that runs in these same platforms. In experiments with our prototype, our models can predict average end-to-end latency with less than 6% error, and we obtain almost three orders of magnitude reduction in end-to-end latency compared to edge-only execution.","","978-1-7281-6095-5","10.1109/CCGrid49817.2020.00-89","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139674","","Cloud computing;Containers;Pipelines;Task analysis;Performance evaluation;Image edge detection;Data processing","","44","","32","IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"WLEC: A Not So Cold Architecture to Mitigate Cold Start Problem in Serverless Computing","K. Solaiman; M. A. Adnan","Bangladesh University of Engineering and Technology (BUET), Dhaka, Bangladesh; Bangladesh University of Engineering and Technology (BUET), Dhaka, Bangladesh",2020 IEEE International Conference on Cloud Engineering (IC2E),"19 May 2020","2020","","","144","153","As serverless computing gains popularity among developers for its low costing and elasticity, it has emerged as a promising research field in computer science. Despite its popularity, the cold start remains an issue that needs more attention. In this paper, we address the cold start problem of the serverless platform. We propose WLEC, a container management architecture to minimize the cold start time. WLEC uses a modified S2LRU structure, called S2LRU ++ with an additional third queue. We implement WLEC in OpenLambda and evaluate it in both AWS and Local VM environment with six different metrics in addition to one real-time image resizing application. Among improvements in all metrics, 50% less memory consumption compared to the all-warm method and 31% average cold start duration reduction compared to the no-warm method are the most notable ones.","","978-1-7281-1099-8","10.1109/IC2E48712.2020.00022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9096489","container;lambda functions;cold start;warm up queue","Containers;Computational modeling;Virtualization;Cloud computing;Runtime;Servers;Computer architecture","","28","","36","IEEE","19 May 2020","","","IEEE","IEEE Conferences"
"Benchmarking Serverless Workloads on Kubernetes","H. Govind; H. González–Vélez","Cloud Competency Centre, National College of Ireland, Dublin, Ireland; Cloud Competency Centre, National College of Ireland, Dublin, Ireland","2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","2 Aug 2021","2021","","","704","712","As a disruptive paradigm in the cloud landscape, Serverless Computing is attracting attention because of its unique value propositions to reduce operating costs and outsource infrastructure management. Nevertheless, enterprise Functionas-a-Service (FaaS) platforms may pose significant risks such as vendor lock-in, lack of security control due to multi-tenancy, complicated pricing models, and legal and regulatory compliance— particularly in mobile computing scenarios. This work proposes a production-grade fault-tolerant serverless architecture based on a highly-available Kubernetes topology using an open-source framework, deployed on OpenStack instances, and benchmarked with a realistic scaled-down Azure workload traces dataset. By measuring success rate, throughput, latency, and auto scalability, we have managed to assess not only resilience but also sustained performance under a logistic model for three distinct representative workloads. Our test executions show, with 95%–confidence, that between 70 and 90 concurrent users can access the system while experiencing acceptable performance. Beyond the breaking point identified (i.e. 91 transactions per second), the Kubernetes cluster has to be scaled-up or scaled out to meet the QoS and availability requirements.","","978-1-7281-9586-5","10.1109/CCGrid51090.2021.00085","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9499690","Serverless;OpenFaas;High Availability;Workload modelling;Service Level Agreement;SLA;Mobile Computing;Azure;Containers","Cloud computing;Computational modeling;Scalability;Quality of service;Pricing;Benchmark testing;Throughput","","8","","21","IEEE","2 Aug 2021","","","IEEE","IEEE Conferences"
"Type, pad, and place: Avoiding data leaks in Cloud-IoT FaaS orchestrations","A. Bocci; S. Forti; G. -L. Ferrari; A. Brogi","Department of Computer Science, University of Pisa, Pisa, Italy; Department of Computer Science, University of Pisa, Pisa, Italy; Department of Computer Science, University of Pisa, Pisa, Italy; Department of Computer Science, University of Pisa, Pisa, Italy","2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","19 Jul 2022","2022","","","798","805","Placing applications composed as orchestrated serverless functions onto Cloud-loT infrastructures is a chal-lenging problem as it must consider hardware, software, network Quality of Service, and service interactions constraints. In this paper, we propose a novel declarative methodology that handles all of the above, also relying on information-flow analyses and padding techniques to prevent information leaks through side channels. A motivating use case from augmented reality is used to showcase the open-source prototype implementing our proposal.","","978-1-6654-9956-9","10.1109/CCGrid54584.2022.00094","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9826027","FaaS orchestrations;information-flow security;Cloud-loT continuum;declarative programming","Cloud computing;Prototypes;FAA;Quality of service;Hardware;Proposals;Open source software","","2","","24","IEEE","19 Jul 2022","","","IEEE","IEEE Conferences"
"OverSketched Newton: Fast Convex Optimization for Serverless Systems","V. Gupta; S. Kadhe; T. Courtade; M. W. Mahoney; K. Ramchandran","Department of EECS, UC Berkeley; Department of EECS, UC Berkeley; Department of EECS, UC Berkeley; ICSI and Statistics Department, UC Berkeley; Department of EECS, UC Berkeley",2020 IEEE International Conference on Big Data (Big Data),"19 Mar 2021","2020","","","288","297","Motivated by recent developments in serverless systems for large-scale computation as well as improvements in scalable randomized matrix algorithms, we develop OverSketched Newton, a randomized Hessian-based optimization algorithm to solve large-scale convex optimization problems in serverless systems. OverSketched Newton leverages matrix sketching ideas from Randomized Numerical Linear Algebra to compute the Hessian approximately. These sketching methods lead to inbuilt resiliency against stragglers that are a characteristic of serverless architectures. Depending on whether or not the problem is strongly convex, we propose different iteration updates using the approximate Hessian. For both cases, we establish convergence guarantees for OverSketched Newton, and we empirically validate our results by solving large-scale supervised learning problems on real-world datasets. Experiments demonstrate a reduction of ~50% in total running time on AWS Lambda, compared to state-of-the-art distributed optimization schemes.","","978-1-7281-6251-5","10.1109/BigData50022.2020.9378289","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9378289","serverless computing;second-order optimization;matrix sketching;coded computing","Supervised learning;Linear algebra;Big Data;Approximation algorithms;Convex functions;Optimization;Resilience","","15","","49","IEEE","19 Mar 2021","","","IEEE","IEEE Conferences"
"Performance Characterization and Modeling of Serverless and HPC Streaming Applications","A. Luckow; S. Jha","BMW Group, Munich, Germany; RADICAL ECE, Rutgers University, Piscataway, NJ, USA",2019 IEEE International Conference on Big Data (Big Data),"24 Feb 2020","2019","","","5688","5696","Industrial and scientific streaming applications require support for different types of processing and the management of heterogeneous infrastructure over a dynamic range of scales: from the edge to the cloud and HPC, and intermediate resources. Serverless is an emerging service that combines high-level middleware services, such as distributed execution engines for managing tasks, with low-level infrastructure. It offers the potential of usability and scalability but adds to the complexity of managing heterogeneous and dynamic resources. In response, we extend Pilot-Streaming to support serverless platforms. Pilot-Streaming provides a unified abstraction for resource management for HPC, cloud, and serverless, and allocates resource containers independent of the application workload removing the need to write resource-specific code. Understanding the performance and scaling characteristics of streaming applications and infrastructure presents another challenge. StreamInsight provides insight into the performance of streaming applications and infrastructure, their selection, configuration, and scaling behavior. Underlying StreamInsight is the universal scalability law, which permits the accurate quantification of scalability properties of streaming applications. Using experiments on HPC and AWS Lambda, we demonstrate that StreamInsight provides an accurate model for a variety of application characteristics, e. g., machine learning model sizes and resource configurations.","","978-1-7281-0858-2","10.1109/BigData47090.2019.9006530","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9006530","Serverless;Streaming;Performance;HPC","Task analysis;Resource management;Engines;Data models;Containers;Parallel processing;Big Data","","5","","39","IEEE","24 Feb 2020","","","IEEE","IEEE Conferences"
"Duo: Improving Data Sharing of Stateful Serverless Applications by Efficiently Caching Multi-Read Data","Z. Huang; H. Fan; C. Cheng; S. Wu; H. Jin","National Engineering Research Center for Big Data Technology and System Services Computing Technology and System Lab, Cluster and Grid Computing Lab School of Computer Science and Technology, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System Services Computing Technology and System Lab, Cluster and Grid Computing Lab School of Computer Science and Technology, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System Services Computing Technology and System Lab, Cluster and Grid Computing Lab School of Computer Science and Technology, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System Services Computing Technology and System Lab, Cluster and Grid Computing Lab School of Computer Science and Technology, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System Services Computing Technology and System Lab, Cluster and Grid Computing Lab School of Computer Science and Technology, Huazhong University of Science and Technology, China",2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS),"18 Jul 2023","2023","","","875","885","A growing number of applications are moving to serverless architectures for high elasticity and fine-grained billing. For stateful applications, however, the use of serverless architectures is likely to lead to significant performance degradation, as frequent data sharing between different execution stages involves time-consuming remote storage access. Current platforms leverage memory cache to speed up remote access. However, conventional caching strategies show limited performance improvement. We experimentally find that the reason is that current strategies overlook the stage-dependent access patterns of stateful serverless applications, i.e., data that are read multiple times across stages (denoted as multi-read data) are wrongly evicted by data that are read only once (denoted as read-once data), causing a high cache miss ratio.Accordingly, we propose a new caching strategy, Duo, whose design principle is to cache multi-read data as long as possible. Specifically, Duo contains a large cache list and a small cache list, which act as Leader list and Wingman list, respectively. Leader list ignores the data that is read for the first time to prevent itself from being polluted by massive read-once data at each stage. Wingman list inspects the data that are ignored or evicted by Leader list, and pre-fetches the data that will probably be read again based on the observation that multi-read data usually appear periodically in groups. Compared to the state-of-the-art works, Duo improves hit ratio by 1.1×-2.1× and reduces the data sharing overhead by 25%-62%.","1530-2075","979-8-3503-3766-2","10.1109/IPDPS54959.2023.00092","National Key Research and Development Program of China; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10177481","serverless computing;data sharing;remote storage;cache","Degradation;Distributed processing;Distributed databases;Elasticity","","2","","37","IEEE","18 Jul 2023","","","IEEE","IEEE Conferences"
"Towards a Multi-objective Scheduling Policy for Serverless-based Edge-Cloud Continuum","L. Angelelli; A. A. da Silva; Y. Georgiou; M. Mercier; G. Mounié; D. Trystram","CNRS, Inria, Grenoble INP, LIG, Univ. Grenoble Alpes, Grenoble, France; CNRS, Inria, Grenoble INP, LIG, Univ. Grenoble Alpes, Grenoble, France; CNRS, Inria, Grenoble INP, LIG, Univ. Grenoble Alpes, Grenoble, France; Ryax Technologies, Lyon, France; CNRS, Inria, Grenoble INP, LIG, Univ. Grenoble Alpes, Grenoble, France; CNRS, Inria, Grenoble INP, LIG, Univ. Grenoble Alpes, Grenoble, France","2023 IEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","10 Jul 2023","2023","","","485","497","The cloud is extended towards the edge to form a computing continuum while managing resources' heterogeneity. The serverless technology simplified how to build cloud applications and use resources, becoming a driving force in consolidating the continuum with the deployment of small functions with short execution. However, the adaptation of serverless to the edge-cloud continuum brings new challenges mainly related to resource management and scheduling. Standard cloud scheduling policies are based on greedy algorithms that do not efficiently handle platforms' heterogeneity nor deal with problems such as cold start delays. This work introduces a new scheduling policy that tries to address these issues. It is based on multi-objective optimization for data transfers and makespan while considering heterogeneity. Using simulations that vary workloads, platforms, and heterogeneity levels, we study the system utilization, the trade-offs between the targets, and the impacts of considering platforms' heterogeneity. We perform comparisons with a baseline inspired by a Kubernetes-based policy, representing greedy algorithms. Our experiments show considerable gaps between the efficiency of a greedy-based scheduling policy and a multi-objective-based one. The last outperforms the baseline by reducing makespan, data transfers, and system utilization by up to two orders of magnitudes in relevant cases for the edge-cloud continuum.","","979-8-3503-0119-9","10.1109/CCGrid57682.2023.00052","European Union(grant numbers:GA101017047); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10171469","Scheduling Policies;Serverless Computing;Edge-Cloud Continuum;Heterogeneous Platforms","Greedy algorithms;Cloud computing;Processor scheduling;Force;Data transfer;Scheduling;Delays","","6","","37","IEEE","10 Jul 2023","","","IEEE","IEEE Conferences"
"Tracking Causal Order in AWS Lambda Applications","W. -T. Lin; C. Krintz; R. Wolski; M. Zhang; X. Cai; T. Li; W. Xu","Dept. of Computer Science, UC Santa Barbara Xiaogang Cai Tongjun Li and Weijin Xu Huawei Technologies Inc.; Dept. of Computer Science, UC Santa Barbara Xiaogang Cai Tongjun Li and Weijin Xu Huawei Technologies Inc.; Dept. of Computer Science, UC Santa Barbara Xiaogang Cai Tongjun Li and Weijin Xu Huawei Technologies Inc.; Dept. of Computer Science, UC Santa Barbara Xiaogang Cai Tongjun Li and Weijin Xu Huawei Technologies Inc.; Huawei Technol. Inc., China; Huawei Technol. Inc., China; Huawei Technol. Inc., China",2018 IEEE International Conference on Cloud Engineering (IC2E),"17 May 2018","2018","","","50","60","Serverless computing is a new cloud programming and deployment paradigm that is receiving wide-spread uptake. Serverless offerings such as Amazon Web Services (AWS) Lambda, Google Functions, and Azure Functions automatically execute simple functions uploaded by developers, in response to cloud-based event triggers. The serverless abstraction greatly simplifies integration of concurrency and parallelism into cloud applications, and enables deployment of scalable distributed systems and services at very low cost. Although a significant first step, the serverless abstraction requires tools that software engineers can use to reason about, debug, and optimize their increasingly complex, asynchronous applications. Toward this end, we investigate the design and implementation of GammaRay, a cloud service that extracts causal dependencies across functions and through cloud services, without programmer intervention. We implement GammaRay for AWS Lambda and evaluate the overheads that it introduces for serverless micro-benchmarks and applications written in Python.","","978-1-5386-5008-0","10.1109/IC2E.2018.00027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8360312","serverless;faas;causal dependencies;AWS Lambda;profiling","Cloud computing;Containers;X-ray imaging;Tools;Monitoring;Concurrent computing","","22","","41","IEEE","17 May 2018","","","IEEE","IEEE Conferences"
"BARISTA: Efficient and Scalable Serverless Serving System for Deep Learning Prediction Services","A. Bhattacharjee; A. D. Chhokra; Z. Kang; H. Sun; A. Gokhale; G. Karsai","Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, Tennessee, USA; Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, Tennessee, USA; Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, Tennessee, USA; Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, Tennessee, USA; Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, Tennessee, USA; Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, Tennessee, USA",2019 IEEE International Conference on Cloud Engineering (IC2E),"8 Aug 2019","2019","","","23","33","Pre-trained deep learning models are increasingly being used to offer a variety of compute-intensive predictive analytics services such as fitness tracking, speech, and image recognition. The stateless and highly parallelizable nature of deep learning models makes them well-suited for serverless computing paradigm. However, making effective resource management decisions for these services is a hard problem due to the dynamic workloads and diverse set of available resource configurations that have different deployment and management costs. To address these challenges, we present a distributed and scalable deep-learning prediction serving system called Barista and make the following contributions. First, we present a fast and effective methodology for forecasting workloads by identifying various trends. Second, we formulate an optimization problem to minimize the total cost incurred while ensuring bounded prediction latency with reasonable accuracy. Third, we propose an efficient heuristic to identify suitable compute resource configurations. Fourth, we propose an intelligent agent to allocate and manage the compute resources by horizontal and vertical scaling to maintain the required prediction latency. Finally, using representative real-world workloads for an urban transportation service, we demonstrate and validate the capabilities of Barista.","","978-1-7281-0218-4","10.1109/IC2E.2019.00-10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8790088","Resource Management, Machine Learning Models, Predictive Analytics, Serverless Computing, Containers","Predictive models;Load modeling;Containers;Computational modeling;Deep learning;Forecasting;Analytical models","","47","","45","IEEE","8 Aug 2019","","","IEEE","IEEE Conferences"
"Cherry: A Distributed Task-Aware Shuffle Service for Serverless Analytics","N. Nikitas; I. Konstantinou; V. Kalogeraki; N. Koziris","CSLAB NTUA, Athens, Greece; University of Thessaly, Greece; Athens University of Economics and Business, Athens, Greece; NTUA, Athens, Greece",2021 IEEE International Conference on Big Data (Big Data),"13 Jan 2022","2021","","","120","130","While there has been a lot of effort in recent years in optimising Big Data systems like Apache Spark and Hadoop, the all-to-all transfer of data between a MapReduce computation step, i.e., the shuffle data mechanism between cluster nodes remains always a serious bottleneck. In this work, we present Cherry, an open-source distributed task-aware Caching sHuffle sErvice for seRveRless analYtics. Our thorough experiments on a cloud testbed using realistic and synthetic workloads showcase that Cherry can achieve an almost 23% to 39% reduction in completion of the reduce stage with small shuffle block sizes, a 10% reduction in execution time on real workloads, while it can efficiently handle Spark execution failures with a constant task time re-computation overhead compared to existing approaches.","","978-1-6654-3902-2","10.1109/BigData52589.2021.9671899","Horizon 2020 Framework Programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9671899","Big Data Analytics Frameworks;Distributed Systems;Cloud Computing;Serverless Architecture","Fault tolerance;Conferences;Fault tolerant systems;Computer architecture;Cluster computing;Big Data;Sparks","","4","","32","IEEE","13 Jan 2022","","","IEEE","IEEE Conferences"
"Scaling Anomaly Detection Service Using Serverless Technology","D. Patel; S. Lin; S. Jayaraman; G. Ganapavarapu; A. Bhamidipaty; J. Kalagnanam","IBM Thomas J. Watson Research Center, Yorktown Heights, NY; IBM Thomas J. Watson Research Center, Yorktown Heights, NY; IBM Thomas J. Watson Research Center, Yorktown Heights, NY; IBM Thomas J. Watson Research Center, Yorktown Heights, NY; IBM Thomas J. Watson Research Center, Yorktown Heights, NY; IBM Thomas J. Watson Research Center, Yorktown Heights, NY",2021 IEEE International Conference on Big Data (Big Data),"13 Jan 2022","2021","","","5982","5984","This poster paper presents an efficient design of deploying anomaly detection service using serverless technology. Our design is motivated by the fact that the workload originating from the service calls are adhoc and reserving the infrastructure upfront is not advisable. To address this, we utilized the emerging serverless platform for executing the incoming training request. Our extensive experimental analysis demonstrate the usefulness of the proposed idea.","","978-1-6654-3902-2","10.1109/BigData52589.2021.9671511","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9671511","","Training;Conferences;Big Data;Anomaly detection","","","","1","IEEE","13 Jan 2022","","","IEEE","IEEE Conferences"
"Streaming vs. Functions: A Cost Perspective on Cloud Event Processing","T. Pfandzelter; S. Henning; T. Schirmer; W. Hasselbring; D. Bermbach","TU Berlin & ECDF, Mobile Cloud Computing Research Group; Kiel University, Software Engineering Group; TU Berlin & ECDF, Mobile Cloud Computing Research Group; Kiel University, Software Engineering Group; TU Berlin & ECDF, Mobile Cloud Computing Research Group",2022 IEEE International Conference on Cloud Engineering (IC2E),"16 Nov 2022","2022","","","67","78","In cloud event processing, data generated at the edge is processed in real-time by cloud resources. Both distributed stream processing (DSP) and Function-as-a-Service (FaaS) have been proposed to implement such event processing applications. FaaS emphasizes fast development and easy operation, while DSP emphasizes efficient handling of large data volumes. Despite their architectural differences, both can be used to model and implement loosely-coupled job graphs. In this paper, we consider the selection of FaaS and DSP from a cost perspective. We implement stateless and stateful workflows from the Theodolite benchmarking suite using cloud FaaS and DSP. In an extensive evaluation, we show how application type, cloud service provider, and runtime environment can influence the cost of application deployments and derive decision guidelines for cloud engineers.","","978-1-6654-9115-0","10.1109/IC2E55432.2022.00015","Deutsche Forschungsgemeinschaft(grant numbers:415899119); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9946366","cloud data processing;streaming;FaaS;scalability","Runtime environment;Costs;Theodolites;Benchmark testing;Real-time systems;Guidelines","","3","","56","IEEE","16 Nov 2022","","","IEEE","IEEE Conferences"
"Storm-RTS: Stream Processing with Stable Performance for Multi-Cloud and Cloud-edge","H. D. Nguyen; A. A. Chien",University of Chicago; University of Chicago and Argonne National Laboratory,2023 IEEE 16th International Conference on Cloud Computing (CLOUD),"25 Sep 2023","2023","","","45","57","Stream Processing Engines (SPEs) traditionally de-ploy applications on a set of shared workers (e.g., threads, processes, or containers) requiring complex performance man-agement by SPEs and application developers. We explore a new approach that replaces workers with Rate-based Abstract Ma-chines (RBAMs). This allows SPEs to translate stream operations into FaaS invocations, and exploit guaranteed invocation rates to manage performance. This approach enables SPE applications to achieve transparent and predictable performance. We realize the approach in the Storm-RTS system. Exploring 36 stream processing scenarios over 5 different hardware config-urations, we demonstrate several key advantages. First, Storm-RTS provides stable application performance and can enable flexible reconfiguration across cloud resource configurations. Sec-ond, SPEs built on RBAM can be resource-efficient and scalable. Finally, Storm-RTS allows the stream-processing paradigm to be extended from the cloud to the edge, using its performance stability to hide edge heterogeneity and resource competition. An experiment with 4 cloud and edge sites over 300 cores shows how Storm-RTS can support flexible reconfiguration and simple high-level declarative policies that optimize resource cost or other criteria.","2159-6190","979-8-3503-0481-7","10.1109/CLOUD60044.2023.00015","NSF(grant numbers:CMMI-1832230,OAC-2019506,CNS-1901466); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10254965","Stream Processing;Serverless;FaaS;Real-time;Cloud Computing;Edge Computing","Cloud computing;Costs;Instruction sets;Stability criteria;Containers;Hardware;Engines","","5","","103","IEEE","25 Sep 2023","","","IEEE","IEEE Conferences"
"TableNN: Deep Learning Framework for Learning Domain Specific Tabular Data","P. Sankhe; E. Khabiri; B. Agrawal; Y. Li","University at Buffalo, Buffalo, NY; IBM, NY; IBM, NY; IBM, NY",2021 IEEE International Conference on Big Data (Big Data),"13 Jan 2022","2021","","","4097","4102","Enterprises often have a large number of databases and other sources of tabular data with columns full of domain-specific jargon (e.g. alpha-numeric codings, undeclared abbreviations, etc) which usually require domain experts to decode. Due to the jargon-specific content of the tables, no pre-trained language model such as Wiki2Vec [21] can be applied readily to encode the cell semantics due to absence of unique jorgan words or alpha-numeric codes in the model vocabulary. We propose a deep learning based framework that is ideally suited for serverless computing environment, and that 1) uses a new tokenization method, called Cell-Masking, 2) encodes the semantics of the cells into contextual embedding that exploits the locality features in tabular data, called Cell2Vec, and 3) an attention-based neural network, called TableNN, that provides a supervised learning solution to classify cell entries into predefined column classes. We apply the proposed method on three publicly available datasets of varying data sizes, from different industries. Cell-Masking provides an order of magnitude lower loss value and quickest convergence for cell embedding generation. In Cell2Vec, we demonstrate that the inclusion of row and column context improves the quality of embeddings by better loss curve convergence and improvement in accuracy by 5.4% on the BTS dataset [3].","","978-1-6654-3902-2","10.1109/BigData52589.2021.9671972","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9671972","neural networks;tabular data;semantics;attention;embeddings","Deep learning;Vocabulary;Computational modeling;Semantics;Supervised learning;Neural networks;Serverless computing","","","","22","IEEE","13 Jan 2022","","","IEEE","IEEE Conferences"
"Towards a Holistic Cloud System with End-to-End Performance Guarantees","R. Andreoli; T. Cucinotta","Sant’Anna School of Advanced Studies, Pisa, Italy; Sant’Anna School of Advanced Studies, Pisa, Italy",2023 IEEE International Conference on Cloud Engineering (IC2E),"6 Nov 2023","2023","","","236","238","Computing technologies are undergoing a relentless evolution from both the hardware and software sides, incorporating new mechanisms for low-latency networking, virtualization, operating systems, hardware acceleration, smart services orchestration, serverless computing, hybrid private-public Cloud solutions and others. Therefore, Cloud infrastructures are becoming increasingly attractive for deploying a wider and wider range of applications, including those with more and more stringent timing constraints, like the emerging use case of deploying time-critical applications. However, despite the availability of a number of public Cloud offerings, and of products (or open-source suites) for deploying in-house private Cloud infrastructures, still there are no solutions readily available for managing time-critical software components with predictable end-to-end timing requirements in the range of hundreds or even tens of milliseconds. The goal of this discussion is to present the multi-domain challenges associated with orchestrating a holistic Cloud system with end-to-end guarantees, which is the subject of my current PhD investigations.","2694-0825","979-8-3503-4394-6","10.1109/IC2E59103.2023.00039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10305824","Cloud Orchestration;Cloud System;End-to-End Performance","Computers;Cloud computing;Operating systems;Serverless computing;Software;Timing;Time factors","","","","19","IEEE","6 Nov 2023","","","IEEE","IEEE Conferences"
"MicroBlend: An Automated Service-Blending Framework for Microservice-Based Cloud Applications","M. Son; S. Mohanty; J. R. Gunasekaran; M. Kandemir","The Pennsylvania State University, USA; The Pennsylvania State University, USA; Adobe Research, USA; The Pennsylvania State University, USA",2023 IEEE 16th International Conference on Cloud Computing (CLOUD),"25 Sep 2023","2023","","","460","470","With the increased usage of public clouds for hosting applications, it becomes essential to choose the appropriate services from the public cloud offerings in order to achieve satisfactory performance while minimizing deployment expenses. Prior research has demonstrated that combining different services can be more cost-effective than solutions based on a single service type. However, automating the combination of resources for applications composed of large graphs of loosely-connected microservices has not yet been thoroughly explored, especially in the context of microservice-based cloud applications. Motivated by this, targeting microservice-based applications, we propose MicroBlend, an automated framework that mixes Infrastructure-as-a-Service (IaaS) and Function-as-a-Service (FaaS) cloud services in a way that is both cost-effective and performance-efficient. MicroBlend focuses on: (i) providing an automated approach for blending resources that takes microservice dependencies into account, (ii) generating FaaS-ready code using a compiler-based approach, and (iii) suggesting an optimization plan for combining microservices with user annotation. We implement MicroBlend on Amazon Web Services (AWS) and evaluate its performance using real-world traces from three different applications. Our findings demonstrate that by employing automated microservice-to-cloud service assignment, MicroBlend can significantly reduce Service Level Objective (SLO) violations by 9%, compared to traditional VM-based resource procurement schemes. Additionally, MicroBlend can decrease costs by 11%.","2159-6190","979-8-3503-0481-7","10.1109/CLOUD60044.2023.00062","NSF(grant numbers:2211018,2122155,1931531,1763681,1908793); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10254987","automation;compiler;serverless;microservices;cloud computing;autoscaling","Procurement;Cloud computing;Costs;Codes;Web services;Annotations;Microservice architectures","","1","","53","IEEE","25 Sep 2023","","","IEEE","IEEE Conferences"
"LatEst: Vertical elasticity for millisecond serverless execution","Y. Sfakianakis; M. Marazakis; C. Kozanitis; A. Bilas","Department of Computer Science, University of Crete, Greece; Foundation for Research and Technology - Hellas (FORTH), Institute of Computer Science (ICS), Heraklion, Greece; Foundation for Research and Technology - Hellas (FORTH), Institute of Computer Science (ICS), Heraklion, Greece; Department of Computer Science, University of Crete, Greece","2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","19 Jul 2022","2022","","","879","885","Current state-of-the-art serverless frameworks can-not execute functions within a few milliseconds for bursty work-loads. The reason for that is that typically they rely on horizontal elasticity to cope with the varying demand for resources, which induces high overhead in the event of a cold -start. Recent literature has focused on minimizing the overhead of horizontal elasticity using mechanisms such as snapshots. However, the spawning of new function instances still requires several tens of milliseconds. This paper proposes vertical elasticity to scale resources of serverless functions to cope with bursting workloads. We design LatEst, a controller for serverless frameworks that adapts the allocated resources of active function instances. U sing vertical scaling, LatEst can adjust to bursts of function invocations within a few milliseconds. LatEst implements a feed-back control loop to: (1) predict the required resources during workload changes and (2) react rapidly and accurately to such changes. Moreover, LatEst spawns new instances for functions when the resources of the underlying server are reaching their limit. We evaluate LatEst as an extension of vHive [1] and find that LatEst can improve tail latency of serverless functions up to 25x compared to vHive.","","978-1-6654-9956-9","10.1109/CCGrid54584.2022.00105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9825925","Vertical Scaling;Resource management;Tail latency;Knative;Machine Learning","Cloud computing;Tail;Elasticity;Servers","","2","","18","IEEE","19 Jul 2022","","","IEEE","IEEE Conferences"
"Optimizing Simultaneous Autoscaling for Serverless Cloud Computing","H. Ship; E. Shindin; C. Wang; D. Arroyo; A. Tantawi","IBM Research - Israel, Haifa, Israel; IBM Research - Israel, Haifa, Israel; IBM Research, Yorktown Heights, NY, USA; IBM Research, Almaden, CA, USA; IBM Research, Yorktown Heights, NY, USA",2024 IEEE 17th International Conference on Cloud Computing (CLOUD),"28 Aug 2024","2024","","","105","114","This paper explores resource allocation in server-less cloud computing platforms and proposes an optimization approach for autoscaling systems. Serverless computing relieves users from resource management tasks, enabling focus on application functions. However, dynamic resource allocation and function replication based on changing loads remain crucial. Typically, autoscalers in these platforms utilize threshold-based mechanisms to adjust function replicas independently. We model applications as interconnected graphs of functions, where requests probabilistically traverse the graph, triggering associated function execution. Our objective is to develop a control policy that optimally allocates resources on servers, minimizing failed requests and response time in reaction to load changes. Using a fluid approximation model and Separated Continuous Linear Programming (SCLP), we derive an optimal control policy that determines the number of resources per replica and the required number of replicas over time. We evaluate our approach using a simulation framework built with Python and simpy. Comparing against threshold-based autoscaling, our approach demonstrates significant improvements in average response times and failed requests, ranging from 15% to over 300% in most cases. We also explore the impact of system and workload parameters on performance, providing insights into the behavior of our optimization approach under different conditions. Overall, our study contributes to advancing resource allocation strategies, enhancing efficiency and reliability in serverless cloud computing platforms.","2159-6190","979-8-3503-6853-6","10.1109/CLOUD62652.2024.00022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10643919","","Cloud computing;Costs;Computational modeling;Optimal control;Dynamic scheduling;Time measurement;Resource management","","1","","24","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"Invited Talk 3","P. Fonseca",Purdue University,2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),"29 Jul 2019","2019","","","393","393","Provides an abstract of the invited presentation and may include a brief professional biography of the presenter. The complete presentation was not made available for publication as part of the conference proceedings.","","978-1-7281-3510-6","10.1109/IPDPSW.2019.00075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8778397","","Concurrent computing;Testing;Software systems;Systematics;Software reliability;Serverless computing;Parallel processing","","","","","IEEE","29 Jul 2019","","","IEEE","IEEE Conferences"
"Cocoa: Towards a Scalable Compute Cost-aware Data Analytics System","K. Oh; M. Song","Computer Science University of Nebraska at Omaha, Omaha, US; Computer Science University of Nebraska at Omaha, Omaha, US",2021 IEEE International Conference on Cloud Engineering (IC2E),"22 Nov 2021","2021","","","110","117","Recently, many data analytics systems have focused on adopting a newly emerging compute resource, serverless, which offers scalability and agility to deal with peak workloads in a timely and cost-efficient manner, i.e., serverless data analytics (SDA). Unfortunately, these systems may encounter a cost bottleneck ($) because they have ignored the per unit time cost ($) of serverless, which is more expensive by up to 5.8 times for the same compute capacity than a traditional compute resource such as a virtual machine (VM). In addition, SDA may also encounter a performance bottleneck due to serverless' worse performance than VM. In this paper, we first study and report when serverless is beneficial for data analytics. Then, we present a scalable compute cost-aware data analytics system, Cocoa, that exploits serverless and VM together to achieve composite benefits. A Cocoa prototype was implemented on Spark. Evaluation results show a richer cost-performance tradeoff space opened by exploiting heterogeneous compute resources together, and identify substantial opportunities for future serverless-enabled systems research.","","978-1-6654-4970-0","10.1109/IC2E52221.2021.00025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9610195","serverless;data analytics;scalable;cost performance tradeoff","Cloud computing;Data analysis;Costs;Scalability;Conferences;Prototypes;Virtual machining","","","","50","IEEE","22 Nov 2021","","","IEEE","IEEE Conferences"
"Towards the Modelling of Hybrid Cloud Applications","K. Kritikos; P. Skrzypek; A. Moga; O. Matei","ICS-FORTH, Crete, Greece; 7Bulls, Warsaw, Poland; Holisun, Baia Mare, Romania; Holisun, Baia Mare, Romania",2019 IEEE 12th International Conference on Cloud Computing (CLOUD),"29 Aug 2019","2019","","","291","295","This paper advances the state-of-the-art by enhancing an existing provider-independent modelling language towards the complete specification of both serverless and hybrid multi-cloud applications. This extension has been validated by a use case developed in the context of the Functionizer project.","2159-6190","978-1-7281-2705-7","10.1109/CLOUD.2019.00056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8814556","hybrid application, serverless, cloud, modelling, validation, deployment, requirements","Unified modeling language;Syntactics;Cloud computing;Big Data;Data models;Measurement;Computational modeling","","14","","8","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"Seneca: Fast and Low Cost Hyperparameter Search for Machine Learning Models","M. Zhang; C. Krintz; M. Mock; R. Wolski","Dept. of Computer Science, University of California, Santa Barbara; Dept. of Computer Science, University of California, Santa Barbara; Dept. of Computer Science, University of California, Santa Barbara; Dept. of Computer Science, University of Applied Sciences, Landshut, Germany",2019 IEEE 12th International Conference on Cloud Computing (CLOUD),"29 Aug 2019","2019","","","404","408","The goal of our work is to simplify and expedite the construction and evaluation of machine learning models using autoscaled cloud computing resources. To enable this, we develop an open source system called Seneca, which leverages the serverless programming model and its implementation in Amazon Web Services (AWS) Lambda. Seneca takes a machine learning application, dataset, and a list of possible hyperparameter options as input and automatically constructs an AWS Lambda function. The function ingresses and splits the input dataset into training and testing subsets and constructs, tests, and evaluates (i.e. scores) a machine learning model for a given set of hyperparameter values. Seneca concurrently invokes functions for all combinations of the hyperparameters specified. It then returns the configuration (or model) that results in the best score to the user. In this paper, we overview the design and implementation of Seneca, and empirically evaluate its performance for a popular classification application.","2159-6190","978-1-7281-2705-7","10.1109/CLOUD.2019.00071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8814572","Serverless computing;Machine learning model selection;Hyperparameter tuning","Cloud computing;Machine learning;Computational modeling;Artificial neural networks;Tuning;Testing;Training","","4","","24","IEEE","29 Aug 2019","","","IEEE","IEEE Conferences"
"COVID-19 Multi-Modal Data Analysis with Alexa Voice and Conversational AI Applications : Voice First System Tracking Novel Coronavirus","A. Kaplunovich","Department of Computer Science, University of Maryland, Baltimore, USA",2021 IEEE International Conference on Big Data (Big Data),"13 Jan 2022","2021","","","4514","4517","Novel Coronavirus (COVID-19) has changed the life of the planet. It is extremely important to monitor the situation in real time. Our secure methodology can help people to trace the situation in their country or state without touching a single computer key, just using voice-first computing devices. We have been using the data from multiple sources, created a suite of Alexa skills (Voice-first applications) and observed how the data evolves. The coronavirus data is downloaded automatically to the AWS Cloud and stored securely in the No-SQL DynamoDB database and S3 buckets to help users to monitor up-to-date statistics. Moreover, Alexa Echo devices with screens will display comprehensive graphs containing the most vital numbers – new cases, new deaths, mortality rate and hospitalizations since the pandemic started. Our system is safe, secure, automatic and resilient. It helps users to maintain social distancing and obtain up-to-date information about coronavirus in the location of interest without a single touch, just by using voice. During our journey we have designed and implemented many convenient commands, improving usability and multi-modal user experience. Our innovative approach, serverless architecture and Big Data methodology can help millions of people to stay on top of the coronavirus situation and make day-to-day choices using the information provided. It can also help officials to make educated decisions about opening certain businesses, institutions or activities. Since more and more voice assistants (AI devices) appear in public places – hotels, restaurants, and airports, our approach will help people to stay informed everywhere. Using our touch-free Alexa analytical skills will also promote social distancing.","","978-1-6654-3902-2","10.1109/BigData52589.2021.9671891","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9671891","BigData;Voice Computing;COVID-19;Alexa;Lambda;Cloud;Visualization;Serverless;No-SQL;AI;microservices;AWS","COVID-19;Pandemics;Databases;Virtual assistants;Human factors;Computer architecture;Big Data","","1","","8","IEEE","13 Jan 2022","","","IEEE","IEEE Conferences"
"Virtual Device Model extending NGSI-LD for FaaS at the Edge","F. Martella; G. Parrino; G. Ciulla; R. D. Bernardo; A. Celesti; M. Fazio; M. Villari","Department MIFT, University of Messina, Italy; Engineer; Research & Development Laboratory, Engineering Ingegneria Informatica, Palermo, Italy; Research & Development Laboratory, Engineering Ingegneria Informatica, Palermo, Italy; Department MIFT, University of Messina, Italy; Department MIFT, University of Messina, Italy; Department MIFT, University of Messina, Italy","2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","2 Aug 2021","2021","","","660","667","Smart environments are composed of an ever-increasing number of heterogeneous resources and devices for collecting and processing of a large amount of context data. These activities can be performed at the edge of the smart area, over a distributed and heterogeneous infrastructure, so to be close to the end-user and optimize response time. However, it is hard to define a data model able to support data exchange among different systems and between systems and users. This paper presents the key features of a smart environment and introduces the concept of virtual device, that is an abstracted component characterized by specific high-level functionalities. Then, the paper proposes a data model useful to represent and optimize the adoption of virtual device in smart environments. To better explain the data model features and benefits, we refer to a video surveillance use case, where a smart camera is able to provide the solid angle detection as a service.","","978-1-7281-9586-5","10.1109/CCGrid51090.2021.00079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9499631","Edge computing;NGSI-LD;IoT Cloud;Data Modeling;Virtual Device;Smart environments","Performance evaluation;Cloud computing;Image edge detection;Smart cameras;Video surveillance;Solids;Feature extraction","","3","","13","IEEE","2 Aug 2021","","","IEEE","IEEE Conferences"
"Splice: An Automated Framework for Cost-and Performance-Aware Blending of Cloud Services","M. Son; S. Mohanty; J. R. Gunasekaran; A. Jain; M. T. Kandemir; G. Kesidis; B. Urgaonkar","The Pennsylvania State University, USA; The Pennsylvania State University, USA; Adobe Research, USA; Microsoft Corporation, USA; The Pennsylvania State University, USA; The Pennsylvania State University, USA; The Pennsylvania State University, USA","2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","19 Jul 2022","2022","","","119","128","With the rapid growth of users adopting public clouds to run their applications, the types of resources procured from the different public cloud resource offerings are critical in simultaneously achieving satisfactory performance and reducing deployment costs. Typically, no one resource type can meet all application requirements, and thus combining different resource offerings is known to considerably reduce the performance-cost problem. However, it is non-trivial to use blended resources, due to the manual overhead of designing and implementing such blended approaches. Specifically, it necessitates rewriting the application code to suit a given resource and scaling it on demand. In order to overcome this manual hurdle, we take the first step by proposing Splice, an automated framework for cost-and performance-aware blending of IaaS and FaaS services. The three major goals of Splice are: (1) while cost-saving opportunities exist from blending resources, we aim to largely automate the blending process for public cloud services through a compiler-driven approach; (2) more specifically, we focus on automated blending of VMs and serverless functions; and (3) for serverless applications which contain multiple chained functions, we unearth the potential choices in determining a portion of the services to be blended cost-efficiently. We implement Splice on Amazon Web Services (AWS) using an Abstract Syntax Tree (AST), and extensively evaluate its effectiveness using several ap-plications with real-world traces. Our experiments demonstrate that, through automated blending, Splice is able to reduce SLO violations by 31 % compared to VM - based resource procurement schemes, while simultaneously minimizing costs by up to 32 %.","","978-1-6654-9956-9","10.1109/CCGrid54584.2022.00021","National Science Foundation(grant numbers:2122155,2028929,2008398,1931531,2119236,1908793); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9825969","automation;compiler;serverless;blending","Procurement;Cloud computing;Costs;Codes;Web services;Manuals;FAA","","7","","40","IEEE","19 Jul 2022","","","IEEE","IEEE Conferences"
"Big Data Management System Architectures: From Opportunities to Challenges [Vision]","R. Joshi; C. T. Dani; S. Jahangiri","Department of Computer Science, Santa Clara University, Santa Clara, USA; Department of Computer Science, Santa Clara University, Santa Clara, USA; Department of Computer Science, Santa Clara University, Santa Clara, USA",2023 IEEE International Conference on Big Data (BigData),"22 Jan 2024","2023","","","96","101","Over the years, data management systems have undergone significant transformations adapting to advancements in hardware, diverse data types, and varying workloads. Numerous architectures have been proposed to more effectively address the customer needs, taking into account rapidly evolving factors like emerging data types, shifting workload and access patterns, and modern hardware innovations. With emergence of cloud infrastructures and their improved capability in seamless scaling, the shared-nothing architecture, which dominated the field for about three decades, is now being replaced with disaggregated architectures. The considerable cost of cloud resources has significantly increased the interest in further disaggregating resources, enabling them to scale independently of one another as required. Consequently, cost-sharing via multi-tenancy and the benefits of pay-for-use billing policies in serverless computing have become pivotal in shaping the design of new database architectures. This paper provides a concise overview of database architecture evolution and their triggers for change, the contemporary state-of-the-art structures, and highlights open research problems for future exploration.","","979-8-3503-2445-7","10.1109/BigData59044.2023.10386870","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10386870","Big Data Management Systems;Architectures;Disaggregated Databases;Auto-scaling;Workload Management","Technological innovation;Costs;Databases;Systems architecture;Serverless computing;Computer architecture;Big Data","","","","24","IEEE","22 Jan 2024","","","IEEE","IEEE Conferences"
"OverSketch: Approximate Matrix Multiplication for the Cloud","V. Gupta; S. Wang; T. Courtade; K. Ramchandran","Department of EECS, UC Berkeley; Department of CS, Stevens Institute of Technology; Department of EECS, UC Berkeley; Department of EECS, UC Berkeley",2018 IEEE International Conference on Big Data (Big Data),"24 Jan 2019","2018","","","298","304","We propose OverSketch, an approximate algorithm for distributed matrix multiplication in serverless computing. OverSketch leverages ideas from matrix sketching and high-performance computing to enable cost-efficient multiplication that is resilient to faults and straggling nodes pervasive in low-cost serverless architectures. We establish statistical guarantees on the accuracy of OverSketch and empirically validate our results by solving a large-scale linear program using interior-point methods and demonstrate a 34% reduction in compute time on AWS Lambda.","","978-1-5386-5035-6","10.1109/BigData.2018.8622139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8622139","serverless computing;straggler mitigation;sketched matrix multiplication","Cloud computing;Bandwidth;Distributed algorithms;Memory management;Partitioning algorithms","","32","","20","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"Implementation of Unsupervised k-Means Clustering Algorithm Within Amazon Web Services Lambda","A. Deese","Department of Electrical and Computer Engineering, The College of New Jersey (TCNJ), Ewing, NJ, USA","2018 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)","16 Jul 2018","2018","","","626","632","This work demonstrates how an unsupervised learning algorithm based on k-Means Clustering with Kaufman Initialization may be implemented effectively as an Amazon Web Services Lambda Function, within their serverless cloud computing service. It emphasizes the need to employ a lean and modular design philosophy, transfer data efficiently between Lambda and DynamoDB, as well as employ Lambda Functions within mobile applications seamlessly and with negligible latency. This work presents a novel application of serverless cloud computing and provides specific examples that will allow readers to develop similar algorithms. The author provides compares the computation speed and cost of machine learning implementations on traditional PC and mobile hardware (running locally) as well as implementations that employ Lambda.","","978-1-5386-5815-4","10.1109/CCGRID.2018.00093","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8411080","machine learning;k-Means Clustering;Kaufman;serverless computing;Amazon Web Services Lambda;cloud computing","Cloud computing;Machine learning;Libraries;Training data;Servers;Task analysis;Pipelines","","8","","25","IEEE","16 Jul 2018","","","IEEE","IEEE Conferences"
"Handling heterogeneous workflows in the Cloud while enhancing optimizations and performance","E. Cadorel; H. Coullon; J. -M. Menaud","Spirals, INRIA, Univ. Lille, CRIStAL, Lille, France; IMT Atlantique, INRIA, LS2N, Nantes, France; IMT Atlantique, INRIA, LS2N, Nantes, France",2022 IEEE 15th International Conference on Cloud Computing (CLOUD),"24 Aug 2022","2022","","","49","58","The goal of a workflow engine is to facilitate the writing, the deploying, and the execution of a scientific workflow (i.e., graph of coarse-grain and heterogeneous tasks) on distributed infrastructures. With the democratization of the Cloud paradigm, many workflow engines of the state of the art offer a way to execute workflows on distant data centers by using the Infrastructure-as-a-Service (IaaS) or the Function-as-a-Service (FaaS) services of Cloud providers. Hence, workflow engines can take advantage of the (presumably) infinite resources and the economical model of the Cloud. However, two important limitations lie in this vision of Cloud-oriented workflow engines. First, by using existing services of Cloud providers, and by managing the workflows at the user side, the Cloud providers are unaware of both the workflows and their user needs, and cannot apply specific resource optimizations to their infrastructure. Second, for the same reasons, handling the heterogeneity of tasks (different operating systems) in workflows necessarily degrades either the transparency for the users (who must provision different types of resources), or the completion time performance of the workflows, because of the stacking of virtualization layers. In this paper, we tackle these two limitations by presenting a new Cloud service dedicated to scientific workflows. Unlike existing workflow engines, this service is deployed and managed by the Cloud providers, and enables specific resource optimizations and offers a better control of the heterogeneity of the workflows. We evaluate our new service in comparison to Argo, a well-known workflow engine of the literature based on FaaS services. This evaluation was made on a real distributed experimental platform with a realistic and complex scenario.","2159-6190","978-1-6654-8137-3","10.1109/CLOUD55607.2022.00021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9860563","Scientific workflows;Cloud computing;Separation of concerns;Modularity","Cloud computing;Scheduling algorithms;Heuristic algorithms;FAA;Writing;Dynamic scheduling;Task analysis","","","","23","IEEE","24 Aug 2022","","","IEEE","IEEE Conferences"
"Adaptive Aggregation For Federated Learning","K. R. Jayaram; V. Muthusamy; G. Thomas; A. Verma; M. Purcell","IBM Research, USA; IBM Research, USA; IBM Research, USA; IBM Research, USA; IBM Research, Ireland",2022 IEEE International Conference on Big Data (Big Data),"26 Jan 2023","2022","","","180","185","In this paper, we present a new scalable and adaptive architecture for FL aggregation. First, we demonstrate how traditional tree overlay based aggregation techniques (from P2P, publish-subscribe and stream processing research) can help FL aggregation scale, but are ineffective from a resource utilization and cost standpoint. Next, we present the design and implementation of AdaFed, which uses serverless/cloud functions to adaptively scale aggregation in a resource efficient and fault tolerant manner. We describe how AdaFed enables FL aggregation to be dynamically deployed only when necessary, elastically scaled to handle participant joins/leaves and is fault tolerant with minimal effort required on the (aggregation) programmer side. We also demonstrate that our prototype based on Ray [1] scales to thousands of participants, and is able to achieve a > 90% reduction in resource requirements and cost, with minimal impact on aggregation latency.","","978-1-6654-8045-1","10.1109/BigData55660.2022.10021119","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10021119","federated learning;serverless;adaptive;aggregation","Fault tolerance;Costs;Federated learning;Fault tolerant systems;Prototypes;Publish-subscribe;Big Data","","15","","25","IEEE","26 Jan 2023","","","IEEE","IEEE Conferences"
"Addressing the Fragmentation Problem in Distributed and Decentralized Edge Computing: A Vision","K. Bhardwaj; A. Gavrilovska; V. Kolesnikov; M. Saunders; H. Yoon; M. Bondre; M. Babu; J. Walsh","College of Computing, Georgia Institute of Technology, Atlanta, GA; College of Computing, Georgia Institute of Technology, Atlanta, GA; College of Computing, Georgia Institute of Technology, Atlanta, GA; College of Computing, Georgia Institute of Technology, Atlanta, GA; College of Computing, Georgia Institute of Technology, Atlanta, GA; College of Computing, Georgia Institute of Technology, Atlanta, GA; College of Computing, Georgia Institute of Technology, Atlanta, GA; College of Computing, Georgia Institute of Technology, Atlanta, GA",2019 IEEE International Conference on Cloud Engineering (IC2E),"8 Aug 2019","2019","","","156","167","At the core of the value proposition of edge computing is the ability to put computation close enough to the data sources, on demand. However, the data sources, computational infrastructure and software services needed to come together to power emerging and future edge computing applications are fragmented across different stakeholders, each with their own incentives, policies, and constraints on resources they can afford. This fragmentation limits the ability of edge computing to guarantee to applications and data the edge which will deliver the desired benefit. In this paper, we present our vision for an Edge Exchange, a decentralized directory service for a multi-stakeholder edge, as a path forward to enabling applications to be deployed across the best available edge resources, while still providing each stakeholder with controls regarding their resource use and sharing policies.","","978-1-7281-0218-4","10.1109/IC2E.2019.00030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8790194","edge computing;edge exchange;decentralization;performance;serverless;cloud;orchestration;distributed system","Edge computing;Stakeholders;Poles and towers;Servers;Peer-to-peer computing;Software;Data privacy","","7","","84","IEEE","8 Aug 2019","","","IEEE","IEEE Conferences"
"Fine-Grained Heterogeneous Execution Framework with Energy Aware Scheduling","G. Rattihalli; N. Hogade; A. Dhakal; E. Frachtenberg; R. P. Hong Enriquez; P. Bruel; A. Mishra; D. Milojicic","Hewlett Packard Labs, Milpitas, CA, United States; Hewlett Packard Labs, Milpitas, CA, United States; Hewlett Packard Labs, Milpitas, CA, United States; Hewlett Packard Labs, Portland, OR, United States; Hewlett Packard Labs, Oxford, United Kingdom; Hewlett Packard Labs, Milpitas, CA, United States; Hewlett Packard Labs, New Jersey, NJ, United States; Hewlett Packard Labs, Milpitas, CA, United States",2023 IEEE 16th International Conference on Cloud Computing (CLOUD),"25 Sep 2023","2023","","","35","44","The growing convergence of high-performance, data analytics, and machine-learning applications is increasingly pushing computing systems toward heterogeneous processors and specialized hardware accelerators. Hardware heterogeneity, in turn, leads to finer-grained workflows. State-of-the-art server-less computing resource managers do not currently provide efficient scheduling of such fine-grained tasks on systems with heterogeneous CPUs and specialized hardware accelerators (e.g., GPUs and FPGAs). Working with fine-grained tasks presents an opportunity for more efficient energy use via new scheduling models. Our proposed scheduler enables technologies like Nvidia's Multi-Process Service (MPS) to pack multiple fine-grained tasks on GPUs efficiently. Its advantages include better co-location of jobs and better sharing of hardware resources such as GPUs that were not previously possible on container orchestration systems. We propose a Kubernetes-native energy-aware scheduler that integrates with our heterogeneous framework. Combining fine-grained resource scheduling on heterogeneous hardware and energy-aware scheduling results in up to 17.6% improvement in makespan, up to 20.16% reduction in energy consumption for CPU workloads, and up to 58.15% improvement in makespan, and up to 28.92% reduction in energy consumption for GPU workloads.","2159-6190","979-8-3503-0481-7","10.1109/CLOUD60044.2023.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10254957","Energy-awareness;Heterogeneity;Serverless;Framework;Scheduler;Fine-granularity","Energy consumption;Cloud computing;Schedules;Data analysis;Processor scheduling;Graphics processing units;Machine learning","","5","","45","IEEE","25 Sep 2023","","","IEEE","IEEE Conferences"
"A Cloud-based Architecture using Micro-services for the IoT-based Applications","Y. Jangir; R. Kumar; N. S. U; M. Mahajan; V. Naik","BITS Pilani, Goa, India; BITS Pilani, Goa, India; BITS Pilani, Goa, India; BITS Pilani, Goa, India; BITS Pilani, Goa, India","2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","19 Jul 2022","2022","","","893","898","The IoT solutions are typically customized for particular sensors. Further, these solutions demand compute, storage, and network resources, which have different scales of scalability. An increase in the demand for IoT-based applications makes it difficult for developers to build extensible and scalable IoT applications. The core idea of the paper is to propose an architecture that can be scaled up and down depending on the needs. In our architecture, each service in the cloud can be modified independently and adapted for a different type and number of IoT sensors. We implement the proposed architecture on Amazon Web Services (AWS), a cloud offering from Amazon. We measure the effectiveness of scaling up and down in terms of the number of IoT devices and mobile clients that process data from the IoT devices.","","978-1-6654-9956-9","10.1109/CCGrid54584.2022.00107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9826080","Serverless Infrastructure;Cloud Computing;Scalable Architecture;Internet of Things (IoT)","Performance evaluation;Cloud computing;Costs;Codes;Scalability;Computer architecture;Sensors","","2","","10","IEEE","19 Jul 2022","","","IEEE","IEEE Conferences"
"SAPPARCHI: an Osmotic Platform to Execute Scalable Applications on Smart City Environments","A. Souza; N. Cacho; T. Batista; R. Ranjan","Federal University of Rio Grande do Norte, Natal, Brazil; Federal University of Rio Grande do Norte, Natal, Brazil; Federal University of Rio Grande do Norte, Natal, Brazil; Newcastle University, Newcastle upon Tyne, UK",2022 IEEE 15th International Conference on Cloud Computing (CLOUD),"24 Aug 2022","2022","","","289","298","In the Smart Cities context, a plethora of Middle-ware Platforms had been proposed to support applications execution and data processing. Despite all the progress already made, the vast majority of solutions have not met the requirements of Applications’ Runtime, Development, and Deployment when related to Scalability. Some studies point out that just 1 of 97 (1%) reported platforms reach this all this set of requirements at same time. This small number of platforms may be explained by some reasons: i) Big Data: The huge amount of processed and stored data with various data sources and data types, ii) Multi-domains: many domains involved (Economy, Traffic, Health, Security, Agronomy, etc.), iii) Multiple processing methods like Data Flow, Batch Processing, Services, and Microservices, and 4) High Distributed Degree: The use of multiple IoT and BigData tools combined with execution at various computational levels (Edge, Fog, Cloud) leads applications to present a high level of distribution. Aware of those great challenges, we propose Sapparchi, an integrated architectural model for Smart Cities applications that defines multi-processing levels (Edge, Fog, and Cloud). Also, it presents the Sapparchi middleware platform for developing, deploying, and running applications in the smart city environment with an osmotic multi-processing approach that scales applications from Cloud to Edge. Finally, an experimental evaluation exposes the main advantages of adopting Sapparchi.","2159-6190","978-1-6654-8137-3","10.1109/CLOUD55607.2022.00051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9860412","osmotic computing;serverless;microservice;scalability","Cloud computing;Runtime;Smart cities;Scalability;Computational modeling;Soft sensors;Distributed databases","","2","","31","IEEE","24 Aug 2022","","","IEEE","IEEE Conferences"
"SandPiper: A Cost-Efficient Adaptive Framework for Online Recommender Systems","P. Thinakaran; K. Mahadik; J. Gunasekaran; M. Taylan Kandemir; C. R. Das",The Pennsylvania State University; Adobe Research; Adobe Research; The Pennsylvania State University; The Pennsylvania State University,2022 IEEE International Conference on Big Data (Big Data),"26 Jan 2023","2022","","","423","430","Online recommender systems have proven to have ubiquitous applications in various domains. To provide accurate recommendations in real time it is imperative to constantly train and deploy models with the latest data samples. This retraining involves adjusting the model weights by incorporating newly-arrived streaming data into the model to bridge the accuracy gap. To provision resources for the retraining, typically the compute is hosted on VMs, however, due to the dynamic nature of the data arrival patterns, stateless functions would be an ideal alternative over VMs, as they can instantaneously scale on demand. However, it is non-trivial to statically configure the stateless functions because the model retraining exhibits varying resource needs during different phases of retraining. Therefore, it is crucial to dynamically configure the functions to meet the resource requirements, while bridging the accuracy gap. In this paper, we propose Sandpiper, an adaptive framework that leverages stateless functions to deliver accurate predictions at low cost for online recommender systems. The three main ideas in Sandpiper are (i) we design a data-drift monitor that automatically triggers model retraining at required time intervals to bridge the accuracy gap due to incoming data drifts; (ii) we develop an online configuration model that selects the appropriate function configurations while maintaining the model serving accuracy within the latency and cost budget; and (iii) we propose a dynamic synchronization policy for stateless functions to speed up the distributed model retraining leading to cloud cost minimization. A prototype implementation on AWS shows that Sandpiper maintains the average accuracy above 90%, while 3.8× less expensive than the traditional VM-based schemes.","","978-1-6654-8045-1","10.1109/BigData55660.2022.10020465","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10020465","Serverless;Systems-for-ML;Continual Learning","Bridges;Cloud computing;Adaptation models;Costs;Adaptive systems;Computational modeling;Big Data","","1","","28","IEEE","26 Jan 2023","","","IEEE","IEEE Conferences"
"Egeon: Software-Defined Data Protection for Object Storage","R. Saiz-Laudo; M. Sánchez-Artigas","Computer Science and Maths, Universitat Rovira i Virgili, Tarragona, Catalonia, Spain; Computer Science and Maths, Universitat Rovira i Virgili, Tarragona, Catalonia, Spain","2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","19 Jul 2022","2022","","","99","108","With the growth in popularity of cloud computing, object storage systems (e.g., Amazon S3, OpenStack Swift, Ceph) have gained momentum for their relatively low per-G B costs and high availability. However, as increasingly more sensitive data is being accrued, the need to natively integrate privacy controls into the storage is growing in relevance. Today, due to the poor object storage interface, privacy controls are enforced by data curators with full access to data in the clear. This motivates the need for a new approach to data privacy that can provide strong assurance and control to data owners. To fulfill this need, this paper presents Egeon, a novel software-defined data protection framework for object storage. Egeon enables users to declaratively set privacy policies on how their data can be shared. In the privacy policies, the users can build complex data protection services through the composition of data transformations, which are invoked inline by Egeon upon a read request. As a result, data owners can trivially display multiple views from the same data piece, and modify these views by only updating the policies. And all without restructuring the internals of the underlying object storage system. The Egeon prototype has been built atop OpenStack Swift. Evaluation results shows promise in developing data protection services with little overhead directly into the object store. Further, depending on the amount of data filtered out in the transformed views, end-to-end latency can be low due to the savings in network communication.","","978-1-6654-9956-9","10.1109/CCGrid54584.2022.00019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9825981","object storage;software-defined;serverless;data privacy","Cloud computing;Costs;Data protection;Prototypes","","","","45","IEEE","19 Jul 2022","","","IEEE","IEEE Conferences"
"Distributed On-Demand Deployment for Transparent Access to 5G Edge Computing Services","J. Hammer; H. Hellwagner","Institute of Information Technology, Alpen-Adria-Universität Klagenfurt, Austria; Institute of Information Technology, Alpen-Adria-Universität Klagenfurt, Austria",2023 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),"4 Aug 2023","2023","","","777","784","Multi-access Edge Computing (MEC) is a central piece of 5G telecommunication systems and is essential to satisfy the challenging low-latency demands of future applications. MEC provides a cloud computing platform at the edge of the radio access network. Our previous publications argue that edge computing should be transparent to clients, leveraging Software-Defined Networking (SDN). While we introduced a solution to implement such a transparent approach, one question remained: How to handle user requests to a service that is not yet running in a nearby edge cluster? One advantage of the transparent edge is that one could process the initial request in the cloud. However, this paper argues that on-demand deployment might be fast enough for many services, even for the first request. We present an SDN controller that automatically deploys an application container in a nearby edge cluster if no instance is running yet. In the meantime, the user’s request is forwarded to another (nearby) edge cluster or kept waiting to be forwarded immediately to the newly instantiated instance. Our performance evaluations on a real edge/fog testbed show that the waiting time for the initial request – e.g., for annginx-based service – can be as low as 0.5 seconds – satisfactory for many applications.","","979-8-3503-1199-0","10.1109/IPDPSW59300.2023.00130","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10196625","Multi-Access Edge Computing;MEC;Fog Computing;Software-Defined Networking;SDN;Serverless Computing;Container;Docker;Kubernetes","Performance evaluation;Cloud computing;Multi-access edge computing;5G mobile communication;Conferences;Containers;Software defined networking","","","","30","IEEE","4 Aug 2023","","","IEEE","IEEE Conferences"
"Pilot-Edge: Distributed Resource Management Along the Edge-to-Cloud Continuum","A. Luckow; K. Rattan; S. Jha","RADICAL, ECE, Rutgers University, Piscataway, NJ, USA; RADICAL, ECE, Rutgers University, Piscataway, NJ, USA; Brookhaven National Laboratory, Upton, NY, USA",2021 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),"24 Jun 2021","2021","","","874","878","Many science and industry IoT applications necessitate data processing across the edge-to-cloud continuum to meet performance, security, cost, and privacy requirements. However, diverse abstractions and infrastructures for managing resources and tasks across the edge-to-cloud scenario are required. We propose Pilot-Edge as a common abstraction for resource management across the edge-to-cloud continuum. Pilot-Edge is based on the pilot abstraction, which decouples resource and workload management, and provides a Function-as-a-Service (FaaS) interface for application-level tasks. The abstraction allows applications to encapsulate common functions in high-level tasks that can then be configured and deployed across the continuum. We characterize Pilot-Edge on geographically distributed infrastructures using machine learning workloads (e. g., k-means and auto-encoders). Our experiments demonstrate how Pilot-Edge manages distributed resources and allows applications to evaluate task placement based on multiple factors (e. g., model complexities, throughput, and latency).","","978-1-6654-3577-2","10.1109/IPDPSW52791.2021.00130","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460584","Edge;cloud;IoT;abstractions;machine learning","Industries;Distributed processing;Machine learning;FAA;Throughput;Data processing;Resource management","","14","","27","IEEE","24 Jun 2021","","","IEEE","IEEE Conferences"
"A Distributed Peer to Peer Identity and Access Management for the Osmotic Computing","C. Sicari; A. Catalfamo; A. Galletta; M. Villari","MIFT Department, University of Messina, Messina, Italy; MIFT Department, University of Messina, Messina, Italy; MIFT Department, University of Messina, Messina, Italy; MIFT Department, University of Messina, Messina, Italy","2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","19 Jul 2022","2022","","","775","781","Nowadays Osmotic Computing is emerging as one of the paradigms used to guarantee the Cloud Continuum, and this popularity is strictly related to the capacity to embrace inside it some hot topics like containers, microservices, orchestration and Function as a Service (FaaS). The Osmotic principle is quite simple, it aims to create a federated heterogeneous infrastructure, where an application's components can smoothly move following a concentration rule. In this work, we aim to solve two big constraints of Osmotic Computing related to the incapacity to manage dynamic access rules for accessing the applications inside the Osmotic Infrastructure and the incapacity to keep alive and secure the access to these applications even in presence of network disconnections. For overcoming these limits we designed and implemented a new Osmotic component, that acts as an eventually consistent distributed peer to peer access management system. This new component is used to keep a local Identity and Access Manager (IAM) that permits at any time to access the resource available in an Osmotic node and to update the access rules that allow or deny access to hosted applications. This component has been already integrated inside a Kubernetes based Osmotic Infrastructure and we presented two typical use cases where it can be exploited.","","978-1-6654-9956-9","10.1109/CCGrid54584.2022.00091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9825999","osmotic computing;security;identity and access control;smart city","Cloud computing;Urban areas;Microservice architectures;FAA;Containers;Peer-to-peer computing","","2","","18","IEEE","19 Jul 2022","","","IEEE","IEEE Conferences"
"UniFaaS: Programming across Distributed Cyberinfrastructure with Federated Function Serving","Y. Li; R. Chard; Y. Babuji; K. Chard; I. Foster; Z. Li","Dept. of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China; Data Science and Learning Division, Argonne National Laboratory, Lemont, IL, USA; Dept. of Computer Science, University of Chicago, Chicago, IL, USA; Dept. of Computer Science, University of Chicago, Chicago, IL, USA; Data Science and Learning Division, Argonne National Laboratory, Lemont, IL, USA; Dept. of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China",2024 IEEE International Parallel and Distributed Processing Symposium (IPDPS),"8 Jul 2024","2024","","","217","229","Modern scientific applications are increasingly decomposable into individual functions that may be deployed across distributed and diverse cyberinfrastructure such as supercomputers, clouds, and accelerators. Such applications call for new approaches to programming, distributed execution, and function-level management. We present UniFaaS, a parallel programming framework that relies on a federated function-as-a-service (FaaS) model to enable composition of distributed, scalable, and high-performance scientific workflows, and to support fine-grained function-level management. UniFaaS provides a unified programming interface to compose dynamic task graphs with transparent wide-area data management. UniFaaS exploits an observe-predict-decide approach to efficiently map workflow tasks to target heterogeneous and dynamic resources. We propose a dynamic heterogeneity-aware scheduling algorithm that employs a delay mechanism and a re-scheduling mechanism to accommodate dynamic resource capacity. Our experiments show that UniFaaS can efficiently execute workflows across computing resources with minimal scheduling overhead. We show that UniFaaS can improve the performance of a real-world drug screening workflow by as much as 22.99% when employing an additional 19.48% of resources and a montage workflow by 54.41% when employing an additional 47.83% of resources across multiple distributed clusters, in contrast to using a single cluster.","1530-2075","979-8-3503-8711-7","10.1109/IPDPS57955.2024.00027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10579158","federated cyberinfrastructure;federated function serving","Scheduling algorithms;Parallel programming;Heuristic algorithms;Parallel processing;Elasticity;Dynamic scheduling;Supercomputers","","1","","61","IEEE","8 Jul 2024","","","IEEE","IEEE Conferences"
"OpenDC 2.0: Convenient Modeling and Simulation of Emerging Technologies in Cloud Datacenters","F. Mastenbroek; G. Andreadis; S. Jounaid; W. Lai; J. Burley; J. Bosch; E. van Eyk; L. Versluis; V. van Beek; A. Iosup","TU Delft, the Netherlands; Solvinity, the Netherlands; VU Amsterdam, the Netherlands; VU Amsterdam, the Netherlands; VU Amsterdam, the Netherlands; VU Amsterdam, the Netherlands; VU Amsterdam, the Netherlands; VU Amsterdam, the Netherlands; Solvinity, the Netherlands; VU Amsterdam, the Netherlands","2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","2 Aug 2021","2021","","","455","464","Cloud datacenters are important for the digital society, serving stakeholders across industry, government, and academia. Simulation is a critical part of exploring datacenter technologies, enabling scalable experimentation with millions of jobs and hundreds of thousands of machines, and what-if analysis in a matter of minutes to hours. Although the community has already developed powerful simulators, emerging technologies and applications in modern datacenters require new approaches. Addressing this requirement, in this work we propose OpenDC, a new platform for datacenter simulation. OpenDC includes novel models for emerging cloud-datacenter technologies and applications, such as serverless computing with FaaS deployment and TensorFlow-based machine learning. Our design also focuses on convenience, with a web-based interface for interactive experimentation, support for experiment automation, a library of prefabs for constructing and sharing datacenter designs, and support for diverse input formats and output metrics. We implement, validate, and open-source OpenDC 2.0, a significant redesign and release after a multi-year research and development process. We demonstrate the benefits of OpenDC for the field through a set of representative use-cases: serverless, machine learning, procurement of HPC-as-a-Service infrastructure, educational practices, and reproducibility studies. Overall, OpenDC helps understand how datacenters work, design datacenter infrastructure, and train the next generation of experts.","","978-1-7281-9586-5","10.1109/CCGrid51090.2021.00055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9499454","OpenDC;datacenter;simulation;modeling;usecases;experimentation;performance analysis","Procurement;Cloud computing;Analytical models;Computational modeling;Machine learning;Reproducibility of results;Stakeholders","","20","","55","IEEE","2 Aug 2021","","","IEEE","IEEE Conferences"
"FedSmarteum: Secure Federated Matrix Factorization Using Smart Contracts for Multi-Cloud Supply Chain","S. Bhagavan; M. Gharibi; P. Rao","IBM Corporation, University of Missouri-Kansas, City Kansas City, MO, USA; IBM Corporation, San Jose, CA, USA; University of Missouri-Columbia, Columbia, MO, USA",2021 IEEE International Conference on Big Data (Big Data),"13 Jan 2022","2021","","","4054","4063","With increased awareness comes unprecedented expectations. We live in a digital, cloud era wherein the underlying information architectures are expected to be elastic, secure, resilient, and handle petabyte scaling. The expectation of epic proportions from the next generation of the data frameworks is to not only do all of the above but also build it on a foundation of trust and explainability across multi-organization business networks. From cloud providers to automobile industries or even vaccine manufacturers, components are often sourced by a complex, not full digitized thread of disjoint suppliers. Building Machine Learning and AI-based order fulfillment and predictive models, remediating issues, is a challenge for multi-organization supply chain automation. We posit that Federated Learning in conjunction with blockchain and smart contracts are technologies primed to tackle data privacy and centralization challenges. In this paper, motivated by challenges in the industry, we propose a decentralized distributed system in conjunction with a recommendation system model (Matrix Factorization) that is trained using Federated Learning on an Ethereum blockchain network. We leverage smart contracts that allow decentralized serverless aggregation to update local-ized items vectors. Furthermore, we utilize Homomorphic Encryption (HE) to allow sharing the encrypted gradients over the network while maintaining their privacy. Based on our results, we argue that training a model over a serverless Blockchain network using smart contracts will provide the same accuracy as in a centralized model while maintaining our serverless model privacy and reducing the overhead communication to a central server. Finally, we assert such a system that provides transparency, audit-ready and deep insights into supply chain operations for enterprise cloud customers resulting in cost savings and higher Quality of Service (QoS).","","978-1-6654-3902-2","10.1109/BigData52589.2021.9671789","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9671789","","Industries;Training;Privacy;Smart contracts;Supply chains;Quality of service;Big Data","","4","","29","IEEE","13 Jan 2022","","","IEEE","IEEE Conferences"
"Recycle.io: An IoT-Enabled Framework for Urban Waste Management","E. Al-Masri; I. Diabate; R. Jain; M. H. Lam; S. Reddy Nathala","School of Engineering and Technology, University of Washington, Tacoma, USA; School of Engineering and Technology, University of Washington, Tacoma, USA; School of Engineering and Technology, University of Washington, Tacoma, USA; School of Engineering and Technology, University of Washington, Tacoma, USA; School of Engineering and Technology, University of Washington, Tacoma, USA",2018 IEEE International Conference on Big Data (Big Data),"24 Jan 2019","2018","","","5285","5287","Addressing environmentally safe management of waste is becoming increasingly a challenging task. The predicament of the rate at which waste is generated due to increasing populations is also contributing to this challenge. One possible approach for effectively handling waste can be achieved by source reduction and recycling. The problem, however, improving the collection of waste can be costly particularly during the source separation process after waste is collected. It would be desirable if there exists a mechanism that can help municipalities, local governments or waste management companies to monitor in real-time sources of violations prior to the waste collection process. In this paper, we introduce recycle.io, an Internet of Things (IoT)-enabled waste management system that is based on a serverless architecture that can identify these sources of violations. Using recycle.io, it is then possible to track the violations geographically which can help local governments, for example, to improve or enforce tighter regulations for waste disposal. Our recycle.io system uses Microsoft Azure IoT Hub for device management. Throughout the paper, we demonstrate usefulness of using our approach for urban waste management in smart cities.","","978-1-5386-5035-6","10.1109/BigData.2018.8622117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8622117","IoT devices;IoT gateways;waste management;industrial internet of things;IIoT;garbage collection;smart bin;smart garbage;smart city","Image edge detection;Waste management;Real-time systems;Cloud computing;Sensors;Edge computing;Hardware","","38","","6","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"AutoDECK: Automated Declarative Performance Evaluation and Tuning Framework on Kubernetes","S. Choochotkaew; T. Chiba; S. Trent; T. Yoshimura; M. Amaral","IBM Research, Tokyo, Japan; IBM Research, Tokyo, Japan; IBM Research, Tokyo, Japan; IBM Research, Tokyo, Japan; IBM Research, Tokyo, Japan",2022 IEEE 15th International Conference on Cloud Computing (CLOUD),"24 Aug 2022","2022","","","309","314","Containerization and application variety bring many challenges in automating evaluations for performance tuning and comparison among infrastructure choices. Due to the tightly-coupled design of benchmarks and evaluation tools, the present automated tools on Kubernetes are limited to trivial microbenchmarks and cannot be extended to complex cloudnative architectures such as microservices and serverless, which are usually managed by customized operators for setting up workload dependencies. In this paper, we propose AutoDECK, a performance evaluation framework with a fully declarative manner. The proposed framework automates configuring, deploying, evaluating, summarizing, and visualizing the benchmarking workload. It seamlessly integrates mature Kubernetes-native systems and extends multiple functionalities such as tracking the image-build pipeline, and auto-tuning. We present five use cases of evaluations and analysis through various kinds of bench-marks including microbenchmarks and HPC/AI benchmarks. The evaluation results can also differentiate characteristics such as resource usage behavior and parallelism effectiveness between different clusters. Furthermore, the results demonstrate the benefit of integrating an auto-tuning feature in the proposed framework, as shown by the 10% transferred memory bytes in the Sysbench benchmark.","2159-6190","978-1-6654-8137-3","10.1109/CLOUD55607.2022.00053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9860408","","Performance evaluation;Visualization;Cloud computing;Pipelines;Benchmark testing;Parallel processing;Artificial intelligence","","1","","19","IEEE","24 Aug 2022","","","IEEE","IEEE Conferences"
"ScaDL 2022 Invited Talk 2: AI/ML Pipelines using CodeFlare","M. Srivatsa","IBM Research, USA",2022 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),"1 Aug 2022","2022","","","1040","1040","Pipelines have become a ubiquitous construct in machine learning spanning tasks ranging from data cleaning and preprocessing, training foundational models, model optimization and transfer learning and low latency inferencing. While the many pipeline construct has existed for many years (e.g., SciKit learn pipelines, Spark pipelines), this talk will focus on a process calculus style definition of pipeline - called CodeFlare pipelines - that makes it readily amenable to scaling complex AI/ML workflows on a commodity cluster. CodeFlare pipelines not only enable data scientists to introduce compute, data and multi-stage parallelism using simple annotations on the pipeline graph, but also operationalize them on a hybrid cloud platform (Red Hat OpenShift), thereby making the solution deployable just about anywhere and leverage the benefits of serverless computing. This talk will cover a basic realization of CodeFlare pipelines on the Ray platform (1.7.0 release) that has shown near linear scalability for transfer learning and inferencing on foundational models.","","978-1-6654-9747-3","10.1109/IPDPSW55747.2022.00167","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9835358","","Pipelines;Transfer learning;Distributed processing;Data models;Conferences;Urban areas;Training","","","","","IEEE","1 Aug 2022","","","IEEE","IEEE Conferences"
"TurboHE: Accelerating Fully Homomorphic Encryption Using FPGA Clusters","H. Liao; M. A. Elmohr; X. Dong; Y. Qian; W. Yang; Z. Shang; Y. Tan",Huawei Technologies Canada; Huawei Technologies Canada; Huawei Technologies Canada; Huawei Technologies Canada; Huawei Technologies Canada; Huawei Technologies Canada; Huawei Technologies Canada,2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS),"18 Jul 2023","2023","","","788","797","With the burgeoning demands for cloud computing in various fields followed by the rising attention to sensitive data exposure, Fully Homomorphic Encryption (FHE) is gaining popularity as a potential solution to privacy protection. By performing computations directly on the ciphertext (encrypted data) without decrypting it, FHE can guarantee the security of data throughout its lifecycle without compromising the privacy. However, the excruciatingly slow speed of FHE scheme makes adopting it impractical in real life applications. Therefore, hardware accelerators come to the rescue to mitigate the problem. Among various hardware platforms, FPGA clusters are particularly promising because of their flexibility and ready availability at many cloud providers such as FPGA-as-a-Service (FaaS). Hence, reusing the existing infrastructure can greatly facilitate the implementation of FHE on the cloud.In this paper, we present TurboHE, the first hardware accelerator for FHE operations based on an FPGA cluster. TurboHE aims to boost the performance of CKKS, one of the fastest FHE schemes which is most suitable to machine learning applications, by accelerating its computationally intensive and frequently used operation: relinearization. The proposed scalable architecture based on hardware partitioning can be easily configured to accommodate high acceleration requirements for relinearization with very large CKKS parameters. As a demonstration, an implementation, which supports 32,768 polynomial coefficients and a coefficient bitwidth of 594 decomposed into 11 Residue Number System (RNS) components, was deployed on a cluster consisting of 9 Xilinx VU13P FPGAs. The cluster operated at 200 MHz and achieved 1096 times throughput compared with a single threaded CPU implementation. Moreover, the low level hardware components implemented in this work such as the NTT module can also be applied to accelerate other lattice-based cryptography schemes.","1530-2075","979-8-3503-3766-2","10.1109/IPDPS54959.2023.00084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10177451","Fully homomorphic encryption;CKKS;NTT;FPGA Cluster;Hardware acceleration","Cloud computing;Ion radiation effects;Protocols;Computer architecture;Throughput;Cryptography;Homomorphic encryption","","3","","24","IEEE","18 Jul 2023","","","IEEE","IEEE Conferences"
"KalpaVriksh: Efficient and Cost-effective GUI Application Hosting using Singleton Snapshots","S. Shaikh; S. Kumar; D. Mishra","Indian Institute of Technology, Kanpur, India; Indian Institute of Technology, Kanpur, India; Indian Institute of Technology, Kanpur, India","2023 IEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","10 Jul 2023","2023","","","180","190","Hosting popular GUI applications in different virtual machines (VMs) in a cloud can provide strong intra- application isolation and enhance the security of end-user devices. In this context, micro-VMs can be a very good fit where different applications are hosted in different micro-VMs hosted in the cloud. However, one of the challenges for the cloud service provider is to launch the application quickly when requested by any client. Techniques like VM snapshots can be used to improve the application launch time as shown in many existing research works. In this paper, we argue that GUI applications are different from snapshot-optimized cloud services like FaaS because the GUI applications are stateful and require specialized techniques for snapshot management. To manage application snapshots in a memory-efficient manner, the proposed KalpaVriksh framework maintains a single snapshot to launch multiple GUI applications from different end users. Furthermore, the unified snapshot framework does not impact the application launch time by using intelligent snapshot creation procedures. The experimental analysis shows that KalpaVriksh snapshot techniques apart from being memory- efficient, reach the farthest feasible point of snapshot capture (i.e., first external communication) during application execution, faster than a normal application launch (by 4.9x).","","979-8-3503-0119-9","10.1109/CCGrid57682.2023.00026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10171579","Virtual Machines;microVM;snapshots;cloud","Cloud computing;Containers;Virtual machining;Security;Graphical user interfaces","","","","38","IEEE","10 Jul 2023","","","IEEE","IEEE Conferences"
"An Empirical Study of Container Image Configurations and Their Impact on Start Times","M. Straesser; A. Bauer; R. Leppich; N. Herbst; K. Chard; I. Foster; S. Kounev","University of Würzburg, Würzburg, Germany; University of Chicago, Chicago, IL, USA; University of Würzburg, Würzburg, Germany; University of Würzburg, Würzburg, Germany; University of Chicago, Chicago, IL, USA; University of Chicago, Chicago, IL, USA; University of Würzburg, Würzburg, Germany","2023 IEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","10 Jul 2023","2023","","","94","105","A core selling point of application containers is their fast start times compared to other virtualization approaches like virtual machines. Predictable and fast container start times are crucial for improving and guaranteeing the performance of containerized cloud, serverless, and edge applications. While previous work has investigated container starts, there remains a lack of understanding of how start times may vary across container configurations. We address this shortcoming by presenting and analyzing a dataset of approximately 200,000 open-source Docker Hub images featuring different image configurations (e.g., image size and exposed ports). Leveraging this dataset, we investigate the start times of containers in two environments and identify the most influential features. Our experiments show that container start times can vary between hundreds of milliseconds and tens of seconds in the same environment. Moreover, we conclude that no single dominant configuration feature determines a container's start time, and hardware and software parameters must be considered together for an accurate assessment.","","979-8-3503-0119-9","10.1109/CCGrid57682.2023.00019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10171550","container;start time;docker;empirical study","Cloud computing;Computational modeling;Image edge detection;Scalability;Containers;Predictive models;Hardware","","10","","42","IEEE","10 Jul 2023","","","IEEE","IEEE Conferences"
"Quantifying and Improving Performance of Distributed Deep Learning with Cloud Storage","N. Krichevsky; R. St Louis; T. Guo","Computer Science Department, Worcester Polytechnic Institute, Worcester, MA, USA; Computer Science Department, Worcester Polytechnic Institute, Worcester, MA, USA; Computer Science Department, Worcester Polytechnic Institute, Worcester, MA, USA",2021 IEEE International Conference on Cloud Engineering (IC2E),"22 Nov 2021","2021","","","99","109","Cloud computing provides a powerful yet low-cost environment for distributed deep learning workloads. However, training complex deep learning models often requires accessing large amounts of data, which can easily exceed the capacity of local disks. Prior research often overlooks this training data problem by implicitly assuming that data is available locally or via low latency network-based data storage. Such implicit assumptions often do not hold in a cloud-based training environment, where deep learning practitioners create and tear down dedicated GPU clusters on demand, or do not have the luxury of local storage, such as in serverless workloads. In this work, we investigate the performance of distributed training that leverages training data residing entirely inside cloud storage buckets. These buckets promise low storage costs, but come with inherent bandwidth limitations that make them seem unsuitable for an efficient training solution. To account for these bandwidth limitations, we propose the use of two classical techniques, namely caching and pre-fetching, to mitigate the training performance degradation. We implement a prototype, DELI, based on the popular deep learning framework PyTorch by building on its data loading abstractions. We then evaluate the training performance of two deep learning workloads using Google Cloud's NVIDIA K80 GPU servers and show that we can reduce the time that the training loop is waiting for data by 85.6%-93.5% compared to loading directly from a storage bucket-thus achieving comparable performance to loading data directly from disk-while only storing a fraction of the data locally at a time. In addition, DELI has the potential of lowering the cost of running a training workload, especially on models with long per-epoch training times.","","978-1-6654-4970-0","10.1109/IC2E52221.2021.00024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9610452","distributed deep learning;cloud-based performance;training data loading","Training;Deep learning;Cloud computing;Loading;Training data;Prototypes;Graphics processing units","","5","","38","IEEE","22 Nov 2021","","","IEEE","IEEE Conferences"
"Consolidating billions of Taxi rides with AWS EMR and Spark in the Cloud : Tuning, Analytics and Best Practices","A. Kaplunovich; Y. Yesha","Computer Science Department, University of Maryland Baltimore County (UMBC), Catonsville, Maryland, USA; Computer Science Department, University of Maryland Baltimore County (UMBC), Catonsville, Maryland, USA",2018 IEEE International Conference on Big Data (Big Data),"24 Jan 2019","2018","","","4501","4507","Saving nature using Big Data Analytics is a very noble goal. Using New York taxi rides data, we decided to learn how many rides could be consolidated. It was a journey we would like to share. First, we had to choose the platform for calculation between Amazon Athena, Serverless Microservices, SQL or NoSql databases, Hadoop and Spark. Then, we had to find an optimal solution for the platform using assorted tuning and optimization techniques. Although the problem seems to be straight forward, it turned out that the solution is quite challenging because of the input size, data quality, calculation complexities and numerous EMR/Spark tuning options. We have been using New York taxi data from 2009 to 2017 to quantify the rides that can be joined together. The taxi rides were consolidated based on pickup location, pickup time and drop-off location. We have been calculating the percentage of taxi rides that can be joined. The benchmark originally set was rides within five minutes with a pickup and drop-off locations within half a kilometer. Then we started experimenting with different times and locations. We have been using parquet format, parallel Scala collections, compression, filtering, new column introduction, tuning parameters, I/O overhead tuning, bucketing, timeouts and partitioning. Over 1.2 billion rides were processed using Amazon EMR with Spark. We have been optimizing calculation time and processing price. Spark has hundreds of parameters, EMR has over fifty instances to choose from. It was challenging to process our data within reasonable time. We were able to find the optimal Spark queries (plans), tested different types of joins and compared their performances. Also, we were able to compare I/O and in-memory operations during partitioning and large files manipulation (the input file sizes were hundreds of Gigabytes). The results were amazing - we could consolidate around thirty five percent of total rides, saving tons of gas and improving environment and traffic in New York City.","","978-1-5386-5035-6","10.1109/BigData.2018.8622378","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8622378","Analytics;Spark;EMR;Cloud;BigData;Best Practices;Parquet;AWS;Optimization;Tuning","Sparks;Public transportation;Servers;Tuning;Big Data;Structured Query Language;Tools","","2","","10","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"CNN Training Latency Prediction Using Hardware Metrics on Cloud GPUs","Y. Hur; K. Lee","Computer Science, Kookmin Univ., Seoul, South Korea; Computer Science, Kookmin Univ., Seoul, South Korea","2024 IEEE 24th International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","8 Oct 2024","2024","","","216","226","Convolutional neural network (CNN) models are becoming larger and more sophisticated over time, and training requires a significant amount of time and computing resources. To meet computing demand, graphics processing units (GPUs) are widely used for training. Due to the excessive cost and overhead of maintaining a GPU cluster, users may prefer to use GPUs provided by a public cloud vendor rather than creating their own servers. The initial cloud service is offered in the Infrastructure-as-a-Service (IaaS) model. As the cloud evolves, the abstraction level of the public cloud service becomes higher, and serverless computing is considered the next-generation cloud service. In the new way of offering cloud services, vendors are required to provide an efficient environment for diverse workloads. To meet the new requirement of the evolving cloud service, this paper proposes heuristics to predict the training latency on various GPU devices without using model information to help cloud-service vendors prepare an efficient training environment with minimal exposure to users’ model architectures. Unlike previous work that relies on internal model details for latency prediction, the proposed system uses only the hardware metrics that are extracted during training. Using the information, we first propose an algorithm to detect an epoch period that we aim to predict. The detected epoch period becomes the target latency to predict, for which we apply a stacked regressor to achieve superior prediction accuracy. Detailed experiments revealed that the average prediction accuracy of the proposed training latency prediction model is 11.17%, which is similar to the state-of-the-art approach that references the internal architecture of the model. Unlike previous work, the proposed work does not reference the model’s internal architecture, which proves the applicability of this proposed work in the next-generation cloud service.","2993-2114","979-8-3503-9566-2","10.1109/CCGrid59990.2024.00033","National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10701385","CNN training;performance model;GPU;cloud computing","Training;Measurement;Cloud computing;Accuracy;Computational modeling;Graphics processing units;Computer architecture;Predictive models;Hardware;Data models","","","","36","IEEE","8 Oct 2024","","","IEEE","IEEE Conferences"
"Learning to Score: Tuning Cluster Schedulers through Reinforcement Learning","M. Asenov; Q. Deng; G. Yeung; A. Barker","Edinburgh Research Centre, Central Software Institute, Huawei; Edinburgh Research Centre, Central Software Institute, Huawei; Edinburgh Research Centre, Central Software Institute, Huawei; Edinburgh Research Centre, Central Software Institute, Huawei",2023 IEEE International Conference on Cloud Engineering (IC2E),"6 Nov 2023","2023","","","113","120","Efficiently allocating incoming jobs to nodes in large-scale clusters can lead to substantial improvements in both cluster utilization and job performance. In order to allocate incoming jobs, cluster schedulers usually rely on a set of scoring functions to rank feasible nodes. Results from individual scoring functions are usually weighted equally, which could lead to suboptimal deployments as the one-size-fits-all solution does not take into account the characteristics of each workload. Tuning the weights of scoring functions, however, requires expert knowledge and is computationally expensive.This paper proposes a reinforcement learning approach for learning the weights in scheduler scoring algorithms with the overall objective of improving the end-to-end performance of jobs for a given cluster. Our approach is based on percentage improvement reward, frame-stacking, and limiting domain information. We propose a percentage improvement reward to address the objective of multi-step parameter tuning. The inclusion of frame-stacking allows for carrying information across an optimization experiment. Limiting domain information prevents overfitting and improves performance in unseen clusters and workloads. The policy is trained on different combinations of workloads and cluster setups. We demonstrate the proposed approach improves performance on average by 33% compared to fixed weights and 12% compared to the best-performing baseline in a lab-based serverless scenario.","2694-0825","979-8-3503-4394-6","10.1109/IC2E59103.2023.00021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10305841","scheduling;scoring functions;reinforcement learning;tuning","Limiting;Clustering algorithms;Reinforcement learning;Tuning;Optimization","","","","40","IEEE","6 Nov 2023","","","IEEE","IEEE Conferences"
"A Parallel Chain Mail Approach for Scalable Spatial Data Interpolation","A. Asratyan; S. Sheikholeslami; V. Vlassov","Department of Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; Department of Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; Department of Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden",2021 IEEE International Conference on Big Data (Big Data),"13 Jan 2022","2021","","","306","314","Deteriorating air quality is a growing concern that has been linked to many health-related issues. Its monitoring is a good first step to understanding the problem. However, it is not always possible to collect air quality data from every location. Various data interpolation techniques are used to assist with populating sparse maps with more context, but many of these algorithms are computationally expensive. This work introduces a three-step Chain Mail algorithm that uses kriging (without any modifications to the base algorithm) and achieves up to ×100 execution time improvement with minimal accuracy loss (relative RMSE of 3%) by running concurrent interpolation executions. This approach can be described as a multiple-step parallel interpolation algorithm that includes specific regional border data manipulation for achieving greater accuracy. It does so by interpolating geographically defined data chunks in parallel and sharing the results with their neighboring nodes to provide context and compensate for lack of knowledge of the surrounding areas. Combined with a serverless cloud architecture, this approach opens doors to interpolating large data sets in a matter of minutes while remaining cost-efficient. The effectiveness of the three-step Chain Mail approach depends on the equal point distribution among all nodes and the resolution of the parallel configuration. In general, it offers a good balance between execution speed and accuracy.","","978-1-6654-3902-2","10.1109/BigData52589.2021.9671487","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9671487","Distributed Computing;Parallel Execution;Data Interpolation;Kriging;Geostatistics;Air Quality","Interpolation;Conferences;Distributed databases;Computer architecture;Big Data;Air quality;Spatial databases","","","","11","IEEE","13 Jan 2022","","","IEEE","IEEE Conferences"
"Deployment and Management of Time Series Forecasts in Ocean Industry","F. O’Donncha; A. Akhriev; B. Eck; M. Burke; R. Filgueira; J. Grant","AI 4 Digital Twins, IBM Research Europe, Dublin, Ireland; AI & Quantum, IBM Research Europe, Dublin, Ireland; AI 4 Digital Twins, IBM Research Europe, Dublin, Ireland; Department of Oceanography, Dalhousie University, Halifax, Canada; Marine Affairs Program, Dalhousie University, Halifax, Canada; Department of Oceanography, Dalhousie University, Halifax, Canada",2021 IEEE International Conference on Big Data (Big Data),"13 Jan 2022","2021","","","4091","4096","Machine learning has not achieved the same degree of success in environmental applications as in other industries. Challenges around data sparsity, quality, and consistency have limited the impact of deep neural network approaches and restricted the focus to research applications. An alternative approach – that is more amenable to the characteristics of data coming from disparate IoT devices deployed at different times and locations in the ocean – is to develop many lightweight models that can be readily scaled up or down based on the number of devices available at any time. This paper presents a serverless framework that naturally marries a single IoT sensor device with a forecasting model. Aspects related to data ingestion, data processing, model training and deployment are described. The framework is applied to a fish farm site in Atlantic Canada.","","978-1-6654-3902-2","10.1109/BigData52589.2021.9671877","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9671877","time series;imputation;ocean;environment;model management","Industries;Training;Computational modeling;Oceans;Time series analysis;Predictive models;Big Data","","","","34","IEEE","13 Jan 2022","","","IEEE","IEEE Conferences"
"Experimentally Evaluating the Resource Efficiency of Big Data Autoscaling","J. Will; N. Treide; L. Thamsen; O. Kao","Technische Universität, Berlin, Germany; Technische Universität, Berlin, Germany; University of Glasgow, United Kingdom; Technische Universität, Berlin, Germany",2024 IEEE International Conference on Big Data (BigData),"16 Jan 2025","2024","","","3825","3830","Distributed dataflow systems like Spark and Flink enable data-parallel processing of large datasets on clusters. Yet, selecting appropriate computational resources for dataflow jobs is often challenging. For efficient execution, individual resource allocations, such as memory and CPU cores, must meet the specific resource requirements of the job. An alternative to selecting a static resource allocation for a job execution is autoscaling as implemented for example by Spark.In this paper, we evaluate the resource efficiency of autoscaling batch data processing jobs based on resource demand both conceptually and experimentally by analyzing a new dataset of Spark job executions on Google Dataproc Serverless. In our experimental evaluation, we show that there is no significant resource efficiency gain over static resource allocations. We found that the inherent conceptual limitations of such autoscaling approaches are the inelasticity of node size as well as the inelasticity of the ratio of memory to CPU cores.","2573-2978","979-8-3503-6248-0","10.1109/BigData62323.2024.10825367","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10825367","Scalable Data Analytics;Distributed Dataflows;Resource Allocation;Autoscaling;Cluster Management","Costs;Memory management;Distributed databases;Big Data;Data processing;Internet;Resource management;Sparks;Optimization","","","","17","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
